\documentclass[twoside,11pt]{article}
\usepackage{jmlr2e}            % <- uncomment for JMLR submission
%\usepackage{jmlr2e-stripped}    % <- uncomment for arXiv version

\usepackage{amsmath,amsfonts}
\usepackage{url}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}

\input{latex_front}
\input{latex_macros}

\usepackage{caption}
\captionsetup{format=hang}

\includecomment{arxiv}\excludecomment{kdd}

% See http://www.jmlr.org/format/sample.tex

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}
\jmlrheading{19}{2018}{1-78}{11/17}{5/18}{17-716}{Elaine Angelino, Nicholas Larus-Stone, Daniel Alabi, Margo Seltzer, and Cynthia Rudin}

% Short headings should be running head and authors last names
\ShortHeadings{Learning Certifiably Optimal Rule Lists}{Angelino, Larus-Stone, Alabi, Seltzer, and Rudin}
\firstpageno{1}

\begin{document}

\title{Learning Certifiably Optimal Rule Lists for Categorical Data}

\author{\name Elaine Angelino \email elaine@eecs.berkeley.edu \\
        \addr Department of Electrical Engineering and Computer Sciences\\
        University of California, Berkeley,
        Berkeley, CA 94720
        \AND
        \name Nicholas Larus-Stone \email nlarusstone@alumni.harvard.edu \\
        \name Daniel Alabi \email alabid@g.harvard.edu \\
        \name Margo Seltzer \email margo@eecs.harvard.edu \\
        \addr School of Engineering and Applied Sciences\\
        Harvard University,
        Cambridge, MA 02138
        \AND
        \name Cynthia Rudin$^*$ \email cynthia@cs.duke.edu \\
        \addr Department of Computer Science and
        Department of Electrical and Computer Engineering\\
        Duke University,
        Durham, NC 27708}

\editor{Maya Gupta\\ $^*$To whom correspondence should be addressed.}
%\footnote{}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
\input{sections/abstract}
\end{abstract}

\begin{keywords}
    rule lists, decision trees, optimization, interpretable models, criminal justice applications
\end{keywords}

\section{Introduction}

As machine learning continues to gain prominence in socially-important decision-making,
the interpretability of predictive models remains a crucial problem.
%
Our goal is to build models that are highly predictive, transparent, and easily understood by humans.
%
We use rule lists, also known as decision lists, to achieve this goal.
%
Rule lists are predictive models composed of if-then statements;
these models are interpretable because the rules provide a reason for each prediction~(Figure~\ref{fig:rule-list}).

Constructing rule lists, or more generally, decision trees, has been a challenge for more than
30 years; most approaches use greedy splitting techniques~\citep{Rivest87,Breiman84,Quinlan93}.
%
Recent approaches use Bayesian analysis, either to find a locally optimal solution~\citep{Chipman:1998jh} or to explore the search space~\citep{LethamRuMcMa15, YangRuSe16}.
%
These approaches achieve high accuracy while also managing to run reasonably quickly.
%
However, despite the apparent accuracy of the rule lists generated by these algorithms,
there is no way to determine either if the generated rule list is optimal or how close it is to optimal,
where optimality is defined with respect to minimization of a regularized loss function.

\begin{arxiv}
\begin{figure}[t!]
\begin{algorithmic}
\State \bif $(age=18-20) \band (sex=male)$ \bthen $yes$
\State \belif $(age=21-23)	 \band (priors=2-3)$ \bthen $yes$
\State \belif $(priors>3)$ \bthen $yes$
\State \belse $no$
\end{algorithmic}
\caption{An example rule list that predicts two-year recidivism
for the ProPublica data set, found by CORELS.
}
\label{fig:rule-list}
\end{figure}
\end{arxiv}

Optimality is important, because there are societal implications for a lack of optimality.
%
Consider the ProPublica article on the Correctional Offender Management Profiling for Alternative Sanctions
(COMPAS) recidivism prediction tool~\citep{LarsonMaKiAn16}.
%
It highlights a case where a black box, proprietary predictive model is being used for recidivism prediction.
%
The authors hypothesize that the COMPAS scores are racially biased,
but since the model is not transparent, no one (outside of the creators of COMPAS)
can determine the reason or extent of the bias~\citep{LarsonMaKiAn16},
nor can anyone determine the reason for any particular prediction.
%
By using COMPAS, users implicitly assumed that a transparent model
would not be sufficiently accurate for recidivism prediction,
\ie they assumed that a black box model would provide better accuracy.
%
We wondered whether there was indeed no transparent and sufficiently accurate model.
%
Answering this question requires solving a computationally hard problem.
%
Namely, we would like to both find a transparent model that is optimal
within a particular pre-determined class of models
and produce a certificate of its optimality, with respect to the regularized empirical risk.
%
This would enable one to say, for this problem and model class,
with certainty and before resorting to black box methods,
whether there exists a transparent~model.
%
While there may be differences between training and test performance,
finding the simplest model with optimal training performance is prescribed by
statistical learning theory.

To that end, we consider the class of rule lists assembled from pre-mined frequent itemsets
and search for an optimal rule list that minimizes a regularized risk function,~$R$.
%
This is a hard discrete optimization problem.
%
Brute force solutions that minimize~$R$ are computationally prohibitive
due to the exponential number of possible rule lists.
%
However, this is a worst case bound that is not realized in practical settings.
%
For realistic cases, it is possible to solve fairly large cases of this problem to optimality,
with the careful use of algorithms, data structures, and implementation techniques.

\begin{kdd}
\begin{figure}[b!]
\vspace{-3mm}
\begin{algorithmic}
\normalsize
\State \bif $(age=23-25) \band (priors=2-3)$ \bthen $yes$
\State \belif $(age=18-20)$ \bthen $yes$
\State \belif $(sex=male) \band (age=21-22)$ \bthen $yes$
\State \belif $(priors>3)$ \bthen $yes$
\State \belse $no$
\end{algorithmic}
\vspace{-3mm}
\caption{An example rule list that predicts two-year recidivism
for the ProPublica data set, found by CORELS.
}
\label{fig:rule-list}
\end{figure}
\end{kdd}

We develop specialized tools from the fields of discrete optimization and artificial intelligence.
%
Specifically, we introduce a special branch-and bound algorithm, called
Certifiably Optimal RulE ListS (CORELS), that provides the optimal solution
according to the training objective, along with a certificate of optimality.
%
The certificate of optimality means that we can investigate how close other models
(\eg models provided by greedy algorithms) are to optimal.

\begin{arxiv}
Within its branch-and-bound procedure, CORELS maintains a lower bound on the
minimum value of~$R$ that each incomplete rule list can achieve.
%
This allows CORELS to prune an incomplete rule list (and every possible extension)
if the bound is larger than the error of the best rule list that it has already evaluated.
%
The use of careful bounding techniques leads to massive pruning of
the search space of potential rule lists.
%
The algorithm continues to consider incomplete and complete rule lists until it has either
examined or eliminated every rule list from consideration.
%
Thus, CORELS terminates with the optimal rule list and a certificate of optimality.
\end{arxiv}

The efficiency of CORELS depends on how much of the search space our bounds
allow us to prune; we seek a tight lower bound on~$R$.
%
The bound we maintain throughout execution is a maximum of several bounds
that come in three categories.
%
The first category of bounds are those intrinsic to the rules themselves.
%
This category includes bounds stating that each rule must capture sufficient data;
if not, the rule list is provably non-optimal.
%
The second type of bound compares a lower bound on the value of~$R$
to that of the current best solution.
%
This allows us to exclude parts of the search space that could never be better
than our current solution.
%
Finally, our last type of bound is based on comparing incomplete rule lists that
capture the same data and allows us to pursue only the most accurate option.
%
This last class of bounds is especially important---without our use of a novel
\textit{symmetry-aware map}, we are unable to solve most problems of reasonable scale.
%
This symmetry-aware map keeps track of the best accuracy
over all observed permutations of a given incomplete rule list.

We keep track of these bounds using a modified \emph{prefix tree},
a data structure also known as a trie.
%
Each node in the prefix tree represents an individual rule;
thus, each path in the tree represents a rule list such that
the final node in the path contains metrics about that rule list.
%
This tree structure, together with a search policy and sometimes a queue,
enables a variety of strategies, including breadth-first,
best-first, and stochastic search.
%
In particular, we can design different best-first strategies
by customizing how we order elements in a priority queue.
%
In addition, we are able to limit the number of nodes in the trie
and thereby enable tuning of space-time tradeoffs in a robust manner.
%
This trie structure is a useful way of organizing the generation
and evaluation of rule lists.

\begin{arxiv}
We evaluated CORELS on a number of publicly available data sets.
%
Our metric of success was 10-fold cross-validated prediction accuracy on a subset of the data.
%
These data sets involve hundreds of rules and thousands of observations.
%
CORELS is generally able to find an optimal rule list in a matter of seconds
and certify its optimality within about 10 minutes.
%
We show that we are able to achieve better or similar out-of-sample accuracy on these
data sets compared to the popular greedy algorithms, CART and C4.5.
\end{arxiv}

CORELS targets large (not massive) problems,
where interpretability and certifiable optimality are important.
%
We illustrate the efficacy of our approach using (1)~the ProPublica COMPAS data set~\citep{LarsonMaKiAn16}, for the problem of two-year recidivism prediction,
and (2)~stop-and-frisk data sets from the
\begin{kdd}
New York Civil Liberties Union (NYCLU)~\citep{nyclu:2014},
\end{kdd}
\begin{arxiv}
NYPD~\citep{nypd} and the NYCLU~\citep{nyclu:2014},
\end{arxiv}
to predict whether a weapon will be found
on a stopped individual who is frisked or searched.
%
On these data, we produce certifiably optimal, interpretable rule lists that achieve
the same accuracy as approaches such as random forests.
%
This calls into question the need for use of a proprietary,
black box algorithm for recidivism prediction.

Our work overlaps with the thesis of~\citet{Larus-Stone17}.
%
We have also written a
\begin{kdd}
long version of this report that includes proofs to all
bounds in~\S\ref{sec:framework}, additional bounds and empirical results,
and further implementation and data processing details~\citep{AngelinoLaAlSeRu17}. \\

Our code is at \textbf{\url{https://github.com/nlarusstone/corels}}.
\end{kdd}
\begin{arxiv}
preliminary conference version of this article~\citep{AngelinoLaAlSeRu17-kdd}, and a report
highlighting systems optimizations of our implementation~\citep{Larus-Stone18-sysml}; the latter includes
additional empirical measurements not presented here. \\

Our code is at \textbf{\url{https://github.com/nlarusstone/corels}},
where we provide the C++ implementation we used in our experiments~(\S\ref{sec:experiments}).
% Python and R wrappers?
%
\citet{corels-website} have also created an interactive web interface at
\textbf{\url{https://corels.eecs.harvard.edu}}, where a user can upload data and
run CORELS from a browser.
\end{arxiv}

\section{Related Work}

Since every rule list is a decision tree and every decision tree can be expressed as an equivalent rule list, the problem we are solving is a version of the ``optimal decision tree'' problem, though regularization changes the nature of the problem (as shown through our bounds). The optimal decision tree problem is computationally hard, though since the late 1990's, there has been research on building optimal decision trees using optimization techniques~\citep{Bennett96optimaldecision,dobkininduction,FarhangfarGZ08}. A particularly interesting paper along these lines is that of \citet{NijssenFromont2010}, who created a ``bottom-up'' way to form optimal decision trees. Their method performs an expensive search step, mining all possible leaves (rather than all possible rules), and uses those leaves to form trees. Their method can lead to memory problems, but it is possible that these memory issues can be mitigated using the theorems in this paper.\footnote{There is no public version of their code for distribution as of this writing.} None of these methods used the tight bounds and data structures of CORELS.

Because the optimal decision tree problem is hard, there are a huge number of algorithms such as CART \citep{Breiman84} and C4.5 \citep{Quinlan93} that do not perform exploration of the search space beyond greedy splitting. Similarly, there are decision list and associative classification methods that construct rule lists iteratively in a greedy way
\citep{Rivest87,Liu98,Li01,Yin03,Sokolova03,Marchand05,Vanhoof10,RudinLeMa13}.
Some exploration of the search space is done by Bayesian decision tree methods~\citep{Dension:1998hl,Chipman:2002hc,Chipman10} and Bayesian rule-based methods \citep{LethamRuMcMa15,YangRuSe16}. The space of trees of a given depth is much larger than the space of
rule lists of that same depth, and the trees within the Bayesian tree algorithms
are grown in a top-down greedy way. Because of this, authors of Bayesian tree algorithms have noted that their MCMC chains tend to reach only locally optimal solutions.
The RIPPER algorithm \citep{ripper} is similar to the Bayesian tree methods in that it grows, prunes, and then locally optimizes.
%
The space of rule lists is smaller than that of trees, and has simpler structure.
%
Consequently, Bayesian rule list algorithms tend to be more successful at
escaping local minima and can introduce methods of exploring the search space
that exploit this structure---these properties motivate our focus on lists.
%
That said, the tightest bounds for the Bayesian lists \citep[namely, those of][upon whose work we build]{YangRuSe16},
are not nearly as tight as those of CORELS.

Tight bounds, on the other hand, have been developed for the (immense) literature on building disjunctive normal form (DNF) models; a good example of this is the work of \citet{Rijnbeek10}.
%
For models of a given size, since the class of DNF's is a proper subset of decision lists, our framework can be restricted to learn optimal DNF's.
The field of DNF learning includes work from the fields of rule learning/induction \citep[\eg early algorithms by][]{Michalski1969,ClarkNiblett1989,Frank1998} and associative classification \citep{Vanhoof10}.
Most papers in these fields aim to carefully guide the search through the space of models. If we were to place a restriction on our code to learn DNF's, which would require restricting predictions within the list to the positive class only, we could potentially use methods from rule learning and associative classification to help order CORELS' queue, which would in turn help us eliminate parts of the search space more quickly.

Some of our bounds, including the minimum support bound
(\S\ref{sec:lb-support}, Theorem~\ref{thm:min-capture}),
come from \citet{RudinEr16}, who provide flexible mixed-integer programming (MIP)
formulations using the same objective as we use here;
MIP solvers in general cannot compete with the speed of CORELS.

CORELS depends on pre-mined rules, which we obtain here via enumeration.
The literature on association rule mining is huge, and any method for rule mining could be reasonably substituted.

CORELS' main use is for producing interpretable predictive models. There is a growing interest in interpretable (transparent, comprehensible) models because of their societal importance \citep[see][]{ruping2006learning,bratko1997machine,dawes1979robust,VellidoEtAl12,Giraud98,Holte93,Schmueli10,Huysmans11,Freitas14}. There are now regulations on algorithmic decision-making in the European Union on the ``right to an explanation'' \citep{Goodman2016EU} that would legally require interpretability of predictions. There is work in both the DNF literature \citep{Ruckert2008} and decision tree literature \citep{GarofalakisHyRaSh00} on building interpretable models. Interpretable models must be so sparse that they need to be heavily optimized; heuristics tend to produce either inaccurate or non-sparse models.

Interpretability has many meanings, and it is possible to extend the ideas in this work to other definitions of interpretability; these rule lists may have exotic constraints that help with ease-of-use. For example, Falling Rule Lists \citep{WangRu15} are constrained to have decreasing probabilities down the list, which makes it easier to assess whether an observation is in a high risk subgroup. In parallel to this paper, we have been working on an algorithm for Falling Rule Lists \citep{ChenRu18} with bounds similar to those presented here, but even CORELS' basic support bounds do not hold for the falling case, which is much more complicated. One advantage of the approach taken by \citet{ChenRu18} is that it can handle class imbalance by weighting the positive and negative classes differently; this extension is possible in CORELS but not addressed here.

The models produced by CORELS are predictive only; they cannot be used for policy-making because they are not causal models, they do not include the costs of true and false positives, nor the cost of gathering information. It is possible to adapt CORELS' framework for causal inference \citep{WangRu15CFRL}, dynamic treatment regimes \citep{ZhangEtAl15}, or cost-sensitive dynamic treatment regimes \citep{LakkarajuRu17} to help with policy design.  CORELS could potentially be adapted to handle these kinds of interesting problems.

\section{Learning Optimal Rule Lists}
\label{sec:framework}

\begin{arxiv}
In this section, we present our framework for learning certifiably optimal rule lists.
%
First, we define our setting and useful notation~(\S\ref{sec:setup})
and then the objective function we seek to minimize~(\S\ref{sec:objective}).
%
Next, we describe the principal structure of our optimization algorithm~(\S\ref{sec:optimization}), which depends on a hierarchically
structured objective lower bound~(\S\ref{sec:hierarchical}).
%
We then derive a series of additional bounds that we incorporate into our
algorithm, because they enable aggressive pruning of our state space.
\end{arxiv}

\subsection{Notation}
\label{sec:setup}

\begin{arxiv}
We restrict our setting to binary classification,
where rule lists are Boolean functions;
this framework is straightforward to generalize to multi-class classification.
\end{arxiv}
\begin{kdd}
We restrict our setting to binary classification.
\end{kdd}
%
Let~${\{(x_n, y_n)\}_{n=1}^N}$ denote training data,
where ${x_n \in \{0, 1\}^J}$ are binary features and ${y_n \in \{0, 1\}}$ are labels.
%
Let~${\x = \{x_n\}_{n=1}^N}$ and~${\y = \{y_n\}_{n=1}^N}$,
and let~${x_{n,j}}$ denote the $j$-th feature of~$x_n$.

A rule list ${\RL = (r_1, r_2, \dots, r_K, r_0)}$ of length~${K \ge 0}$
is a ${(K+1)}$-tuple consisting of~$K$ distinct association rules,
${r_k = p_k \rightarrow q_k}$, for ${k = 1, \dots, K}$,
followed by a default rule~$r_0$.
%
\begin{arxiv}
Figure~\ref{fig:rule-list-symbols} illustrates
a rule list, ${\RL =}$ ${(r_1, r_2, r_3, r_0)}$,
which for clarity, we sometimes call a $K$-rule list.
\end{arxiv}
\begin{kdd}
Figure~\ref{fig:rule-list} illustrates a 3-rule list,
${\RL =}$ ${(r_1, r_2, r_3, r_0)}$.
\end{kdd}
%
An association rule~${r = p \rightarrow q}$ is an implication
corresponding to the conditional statement, ``if~$p$, then~$q$.''
%
In our setting, an antecedent~$p$ is a Boolean assertion that
evaluates to either true or false for each datum~$x_n$,
and a consequent~$q$ is a label prediction.
%
For example, ${(x_{n, 1} = 0) \wedge (x_{n, 3} = 1) \rightarrow}$ ${(y_n = 1)}$
is an association rule.
%
%The number of conditions in an antecedent is its cardinality;
%the antecedent in the previous example has a cardinality of two.
%
The final default rule~$r_0$ in a rule list can be thought of
as a
\begin{arxiv}
special
\end{arxiv}
association rule~${p_0 \rightarrow q_0}$
whose antecedent~$p_0$ simply asserts true.

\begin{arxiv}
\begin{figure}[t!]
\small
\begin{subfigure}{0.67\textwidth}
\begin{algorithmic}
\State \bif $(age=18-20) \band (sex=male)$ \bthen $yes$
\State \belif $(age=21-23)	 \band (priors=2-3)$ \bthen $yes$
\State \belif $(priors>3)$ \bthen $yes$
\State \belse $no$
\end{algorithmic}
\end{subfigure}
\hfill
\begin{subfigure}{0.32\textwidth}
\begin{algorithmic}
\State \bif $p_1$ \bthen $q_1$
\State \belif $p_2$ \bthen $q_2$
\State \belif $p_3$ \bthen $q_3$
\State \belse $q_0$
\end{algorithmic}
\end{subfigure}
\caption{The rule list ${\RL = (r_1, r_2, r_3, r_0)}$.
Each rule is of the form ${r_k = p_k \rightarrow q_k}$,
for all ${k = 0, \dots, 3}$.
We can also express this rule list as ${\RL = (\Prefix, \Labels, \Default, K)}$,
where ${\Prefix = (p_1, p_2, p_3)}$, ${\Labels = (1, 1, 1, 1)}$,
${\Default = 0}$, and ${K=3}$.
This is the same 3-rule list as in Figure~\ref{fig:rule-list},
that predicts two-year recidivism for the ProPublica data set.
}
\label{fig:rule-list-symbols}
\end{figure}
\end{arxiv}

Let ${\RL = (r_1, r_2, \dots, r_K, r_0)}$ be a
\begin{arxiv}
$K$-rule list,
\end{arxiv}
\begin{kdd}
rule list,
\end{kdd}
where ${r_k = p_k \rightarrow q_k}$ for each ${k = 0, \dots, K}$.
%
We introduce a useful alternate rule list representation:
${\RL = (\Prefix, \Labels, \Default, K)}$,
where we define ${\Prefix = (p_1, \dots, p_K)}$ to be $\RL$'s prefix,
${\Labels = (q_1, \dots, q_K) \in \{0, 1\}^K}$~gives
the label predictions associated with~$\Prefix$,
and ${\Default \in \{0, 1\}}$ is the default label prediction.
%
For
\begin{arxiv}
example, for
\end{arxiv}
the rule list in Figure~\ref{fig:rule-list},
we would write ${\RL = (\Prefix, \Labels, \Default, K)}$,
where ${\Prefix = (p_1, p_2, p_3)}$, ${\Labels = (1, 1, 1)}$,
${\Default = 0}$, and ${K=3}$.
%
Note that ${((), (), q_0, 0)}$ is a well-defined rule list with an empty prefix;
it is completely defined by a single default rule.

Let ${\Prefix = (p_1, \dots, p_k, \dots, p_K)}$ be an antecedent list,
then for any ${k \le K}$, we define ${\Prefix^k =}$ ${(p_1, \dots, p_k)}$
to be the $k$-prefix of~$\Prefix$.
%
For any such $k$-prefix~$\Prefix^k$,
we say that~$\Prefix$ starts with~$\Prefix^k$.
%
For any given space of rule lists,
we define~$\StartsWith(\Prefix)$ to be the set of
all rule lists whose prefixes start with~$\Prefix$:
\begin{align}
\StartsWith(\Prefix) =
\{(\Prefix', \Labels', \Default', K') : \Prefix' \textnormal{ starts with } \Prefix \}.
\label{eq:starts-with}
\end{align}
%We also say that an antecedent list~$\Prefix$ contains another
%antecedent list~$\Prefix'$ if the antecedents in~$\Prefix'$ correspond to
% a contiguous subsequence of antecedents anywhere in~$\Prefix$.
%
If ${\Prefix = (p_1, \dots, p_K)}$ and ${\Prefix' = (p_1, \dots, p_K, p_{K+1})}$
are two prefixes such that~$\Prefix'$ starts with~$\Prefix$ and extends it by
a single antecedent, we say that~$\Prefix$ is the parent of~$\Prefix'$
and that~$\Prefix'$ is a child of~$\Prefix$.

A rule list~$\RL$ classifies datum~$x_n$ by providing the label prediction~$q_k$
of the first rule~$r_k$ whose antecedent~$p_k$ is true for~$x_n$.
%
We say that an antecedent~$p_k$ of antecedent list~$\Prefix$ captures~$x_n$
in the context of~$\Prefix$ if~$p_k$ is the first antecedent in~$\Prefix$ that
evaluates to true for~$x_n$.
%
\begin{arxiv}
We also say that a
\end{arxiv}
\begin{kdd}
A
\end{kdd}
prefix captures those data captured by its antecedents;
for a rule list~${\RL = (\Prefix, \Labels, \Default, K)}$,
data not captured by the prefix~$\Prefix$
are classified according to the default label prediction~$\Default$.

Let~$\beta$ be a set of antecedents.
%
We define~${\Cap(x_n, \beta) = 1}$ if an antecedent in~$\beta$
captures datum~$x_n$, and~0 otherwise.
%
For example, let~$\Prefix$ and~$\Prefix'$ be prefixes such that~$\Prefix'$ starts
with~$\Prefix$, then~$\Prefix'$ captures all the data that~$\Prefix$ captures:
\begin{arxiv}
\begin{align*}
\{x_n: \Cap(x_n, \Prefix)\} \subseteq \{x_n: \Cap(x_n, \Prefix')\}.
%\label{eq:cap-subset}
\end{align*}
\end{arxiv}
\begin{kdd}
${\{x_n: \Cap(x_n, \Prefix)\} \subseteq \{x_n: \Cap(x_n, \Prefix')\}}$.
\end{kdd}
%We also define ${\Cap(\x, \beta) = \{\Cap(x_n, \beta)\}_{n=1}^N} \in \{0, 1\}^N$
%to be~$\beta$'s captures vector.

Now let~$\Prefix$ be an ordered list of antecedents,
and let~$\beta$ be a subset of antecedents in~$\Prefix$.
%
Let us define~${\Cap(x_n, \beta \given \Prefix) = 1}$ if~$\beta$
captures datum~$x_n$ in the context of~$\Prefix$,
\ie if the first antecedent in~$\Prefix$ that evaluates to true for~$x_n$
is an antecedent in~$\beta$, and~0 otherwise.
%
Thus, ${\Cap(x_n, \beta \given \Prefix) = 1}$ only if ${\Cap(x_n, \beta) = 1}$;
${\Cap(x_n, \beta \given \Prefix) = 0}$ either if ${\Cap(x_n, \beta) = 0}$,
or if ${\Cap(x_n, \beta) = 1}$ but there is an antecedent~$\alpha$ in~$\Prefix$,
preceding all antecedents in~$\beta$, such that ${\Cap(x_n, \alpha) = 1}$.
%
For example, if ${\Prefix = (p_1, \dots, p_k, \dots, p_K)}$ is a prefix, then
\begin{align*}
\Cap(x_n, p_k \given \Prefix) =
  \left(\bigwedge_{k'=1}^{k - 1} \neg\, \Cap(x_n, p_{k'}) \right)
  \wedge \Cap(x_n, p_k)
\end{align*}
indicates whether antecedent~$p_k$ captures datum~$x_n$ in the context of~$\Prefix$.
%
Now, define ${\Supp(\beta, \x)}$ to be the normalized support of~$\beta$,
\begin{align}
\Supp(\beta, \x) = \frac{1}{N} \sum_{n=1}^N \Cap(x_n, \beta),
\label{eq:support}
\end{align}
and similarly define~${\Supp(\beta, \x \given \Prefix)}$
to be the normalized support of~$\beta$ in the context of~$\Prefix$,
\begin{align}
\Supp(\beta, \x \given \Prefix) = \frac{1}{N} \sum_{n=1}^N \Cap(x_n, \beta \given \Prefix),
\label{eq:support-context}
\end{align}

Next, we address how empirical data constrains rule lists.
%
Given training data~${(\x, \y)}$,
an antecedent list ${\Prefix = (p_1, \dots, p_K)}$
implies a rule list ${\RL = (\Prefix, \Labels, \Default, K)}$
with prefix~$\Prefix$, where the label predictions
${\Labels = (q_1, \dots, q_K)}$ and~$\Default$ are empirically set
to minimize the number of misclassification errors made by
the rule list on the training data.
%
Thus for~${1 \le k \le K}$, label prediction~$q_k$ corresponds to the
majority label of data captured by antecedent~$p_k$ in the context of~$\Prefix$,
and the default~$\Default$ corresponds to the majority label of data
not captured by~$\Prefix$.
%
In the remainder of our presentation, whenever we refer to a rule list with a
particular prefix, we implicitly assume these empirically determined label predictions.

Our method is technically an associative classification method since it
leverages pre-mined rules.

\subsection{Objective Function}
\label{sec:objective}

\begin{arxiv}
We define
\end{arxiv}
\begin{kdd}
Define
\end{kdd}
a simple objective function for a rule list ${\RL = (\Prefix, \Labels, \Default, K)}$:
\begin{align}
\Obj(\RL, \x, \y) = \Loss(\RL, \x, \y) + \Reg K.
\label{eq:objective}
\end{align}
This objective function is a regularized empirical risk;
it consists of a loss~$\Loss(\RL, \x, \y)$, measuring misclassification error,
and a regularization term that penalizes longer rule lists.
%
$\Loss(\RL, \x, \y)$~is the fraction of training data whose labels are
incorrectly predicted by~$\RL$.
%
In our setting, the regularization parameter~${\Reg \ge 0}$ is a small constant;
\eg ${\Reg = 0.01}$ can be thought of as adding a penalty equivalent to misclassifying~$1\%$
of data when increasing a rule list's length
\begin{arxiv}
by one association rule.
\end{arxiv}
\begin{kdd}
by~one.
\end{kdd}
%
%As noted in~\S\ref{sec:setup}, a prefix~$\Prefix$ and training data together
%fully specify a rule list~${\RL = (\Prefix, \Labels, \Default, K)}$,
%thus let us define~${\Obj(\Prefix, \x, \y) \equiv \Obj(\RL, \x, \y)}$.

\subsection{Optimization Framework}
\label{sec:optimization}

Our objective has structure amenable to global optimization via a branch-and-bound framework.
%
In particular, we make a series of important observations, each of which translates into
a useful bound, and that together interact to eliminate large parts of the search~space.
%
We discuss these in depth in what follows:
%
\begin{itemize}
\item Lower bounds on a prefix also hold for every extension of that prefix.
(\S\ref{sec:hierarchical}, Theorem~\ref{thm:bound})

\item If a rule list is not accurate enough with respect to its length,
we can prune all extensions of it.
(\S\ref{sec:hierarchical}, Lemma~\ref{lemma:lookahead})

\item We can calculate \emph{a priori} an upper bound on the maximum length
of an optimal rule list.
(\S\ref{sec:ub-prefix-length}, Theorem~\ref{thm:ub-prefix-specific})

\item Each rule in an optimal rule list must have support that is sufficiently large.
%(Otherwise it would not be in an optimal rule list.)
%
This allows us to construct rule lists from frequent itemsets,
while preserving the guarantee that we can find a globally optimal
rule list from pre-mined rules.
(\S\ref{sec:lb-support}, Theorem~\ref{thm:min-capture})

\item Each rule in an optimal rule list must predict accurately.
%
In particular, the number of observations predicted correctly
by each rule in an optimal rule list must be above a threshold.
(\S\ref{sec:lb-support}, Theorem~\ref{thm:min-capture-correct})

\item We need only consider the optimal permutation of antecedents in a prefix;
we can omit all other permutations.
(\S\ref{sec:equivalent}, Theorem~\ref{thm:equivalent} and Corollary~\ref{thm:permutation})

\item  If multiple observations have identical features and opposite labels,
we know that any model will make mistakes.
%
In particular, the number of mistakes on these observations will be at least
the number of observations with the minority label.
(\S\ref{sec:identical}, Theorem~\ref{thm:identical})
\end{itemize}
\begin{kdd}
We present additional theorems and all proofs in~\citep{AngelinoLaAlSeRu17}.
\end{kdd}

\subsection{Hierarchical Objective Lower Bound}
\label{sec:hierarchical}

We can decompose the misclassification error
\begin{arxiv}
in~\eqref{eq:objective}
\end{arxiv}
into two contributions corresponding to the prefix and the default rule:
\begin{align*}
\Loss(\RL, \x, \y) %= \Loss(\Prefix, r_q, \Default, \x, \y)
\equiv \Loss_p(\Prefix, \Labels, \x, \y) + \Loss_0(\Prefix, \Default, \x, \y),
\end{align*}
where ${\Prefix = (p_1, \dots, p_K)}$ and ${\Labels = (q_1, \dots, q_K)}$;
\begin{align*}
\Loss_p(\Prefix, \Labels, \x, \y) =
\frac{1}{N} \sum_{n=1}^N \sum_{k=1}^K \Cap(x_n, p_k \given \Prefix) \wedge \one [ q_k \neq y_n ]
%\label{eq:loss}
\end{align*}
is the fraction of data captured and misclassified by the prefix, and
\begin{align*}
\Loss_0(\Prefix, \Default, \x, \y) =
\frac{1}{N} \sum_{n=1}^N \neg\, \Cap(x_n, \Prefix) \wedge \one [ \Default \neq y_n ]
\end{align*}
is the fraction of data not captured by the prefix and misclassified by the default rule.
%
Eliminating the latter error term gives a lower bound~$b(\Prefix, \x, \y)$ on the objective,
\begin{align}
b(\Prefix, \x, \y) \equiv \Loss_p(\Prefix, \Labels, \x, \y) + \Reg K \le \Obj(\RL, \x, \y),
\label{eq:lower-bound}
\end{align}
where we have suppressed the lower bound's dependence on label predictions~$\Labels$
because they are fully determined, given~${(\Prefix, \x, \y)}$.
%
Furthermore,
\begin{arxiv}
as we state next in Theorem~\ref{thm:bound},
\end{arxiv}
$b(\Prefix, \x, \y)$ gives a lower bound on the objective of
\emph{any} rule list whose prefix starts with~$\Prefix$.

\begin{theorem}[Hierarchical objective lower bound]
\begin{arxiv}
Define~${b(\Prefix, \x, \y)}$
\end{arxiv}
\begin{kdd}
Define~${b(\Prefix, \x, \y) = \Loss_p(\Prefix, \Labels, \x, \y) + \Reg K}$,
\end{kdd}
as in~\eqref{eq:lower-bound}.
%
Also, define $\StartsWith(\Prefix)$ to be the set of all rule lists
whose prefixes starts with~$\Prefix$, as in~\eqref{eq:starts-with}.
%
Let ${\RL = }$ ${(\Prefix, \Labels, \Default, K)}$ be a rule list
with prefix~$\Prefix$, and let
${\RL' = (\Prefix', \Labels', \Default', K')}$ $\in \StartsWith(\Prefix)$
be any rule list such that its prefix~$\Prefix'$ starts with~$\Prefix$
and ${K' \ge K}$, then ${b(\Prefix, \x, \y) \le}$ ${\Obj(\RL', \x, \y)}$.
\label{thm:bound}
\end{theorem}

\begin{arxiv}
\begin{proof}
Let ${\Prefix = (p_1, \dots, p_K)}$ and ${\Labels = (q_1, \dots, q_K)}$;
let ${\Prefix' = (p_1, \dots, p_K, p_{K+1}, \dots, p_{K'})}$
and ${\Labels' = (q_1, \dots, q_K, q_{K+1}, \dots, q_{K'})}$.
%
Notice that~$\Prefix'$ yields the same mistakes as~$\Prefix$,
and possibly additional mistakes:
\begin{align}
&\Loss_p(\Prefix', \Labels', \x, \y)
= \frac{1}{N} \sum_{n=1}^N  \sum_{k=1}^{K'} \Cap(x_n, p_k \given \Prefix') \wedge \one [ q_k \neq y_n ] \nn \\
&= \frac{1}{N} \sum_{n=1}^N \left( \sum_{k=1}^K \Cap(x_n, p_k \given \Prefix) \wedge \one [ q_k \neq y_n ]
+ \sum_{k=K+1}^{K'} \Cap(x_n, p_k \given \Prefix') \wedge \one [ q_k \neq y_n ] \right) \nn \\
&=\Loss_p(\Prefix, \Labels, \x, \y)
+ \frac{1}{N} \sum_{n=1}^N \sum_{k=K+1}^{K'} \Cap(x_n, p_k \given \Prefix') \wedge \one [ q_k \neq y_n ]
\ge \Loss_p(\Prefix, \Labels, \x, \y),
\label{eq:prefix-loss}
\end{align}
where in the second equality we have used the fact that
${\Cap(x_n, p_k \given \Prefix') = \Cap(x_n, p_k \given \Prefix)}$
for~${1 \le k \le K}$.
%
It follows that
\begin{align}
b(\Prefix, \x, \y) &= \Loss_p(\Prefix, \Labels, \x, \y) + \Reg K \nn \\
&\le  \Loss_p(\Prefix', \Labels', \x, \y) + \Reg K' = b(\Prefix', \x, \y)
\le \Obj(\RL', \x, \y).
\label{eq:prefix-lb}
\end{align}
\end{proof}
\end{arxiv}

To generalize, consider a sequence of prefixes such that each prefix
starts with all previous prefixes in the sequence.
%
It follows that the corresponding sequence of objective lower bounds
increases monotonically.
%
This is precisely the structure required and exploited by branch-and-bound,
illustrated in Algorithm~\ref{alg:branch-and-bound}.

\begin{algorithm}[t!]
\caption{Branch-and-bound for learning rule lists.}
\label{alg:branch-and-bound}
\begin{algorithmic}
\normalsize
\State \textbf{Input:} Objective function $\Obj(\RL, \x, \y)$,
objective lower bound ${b(\Prefix, \x, \y)}$,
set of antecedents ${\RuleSet = \{s_m\}_{m=1}^M}$,
training data $(\x, \y) = {\{(x_n, y_n)\}_{n=1}^N}$,
initial best known rule list~$\InitialRL$ with objective
${\InitialObj = \Obj(\InitialRL, \x, \y)}$;
$\InitialRL$ could be obtained as output from another (approximate) algorithm,
otherwise, $(\InitialRL, \InitialObj) = (\text{null}, 1)$ provide reasonable default values
\State \textbf{Output:} Provably optimal rule list~$\OptimalRL$ with minimum objective~$\OptimalObj$ \\

\State $(\CurrentRL, \CurrentObj) \gets (\InitialRL, \InitialObj)$ \Comment{Initialize best rule list and objective}
\State $Q \gets $ queue$(\,[\,(\,)\,]\,)$ \Comment{Initialize queue with empty prefix}
\While {$Q$ not empty} \Comment{Stop when queue is empty}
	\State $\Prefix \gets Q$.pop(\,) \Comment{Remove prefix~$\Prefix$ from the queue}
	\begin{arxiv}
	\State $\RL \gets (\Prefix, \Labels, \Default, K)$ \Comment{Set label predictions~$\Labels$ and~$\Default$ to minimize training error}
	\end{arxiv}
	\If {$b(\Prefix, \x, \y) < \CurrentObj$} \Comment{\textbf{Bound}: Apply Theorem~\ref{thm:bound}}
        \State $\Obj \gets \Obj(\RL, \x, \y)$ \Comment{Compute objective of~$\Prefix$'s rule list~$\RL$}
        \If {$\Obj < \CurrentObj$} \Comment{Update best rule list and objective}
            \State $(\CurrentRL, \CurrentObj) \gets (\RL, \Obj)$
        \EndIf
        \For {$s$ in $\RuleSet$} \Comment{\textbf{Branch}: Enqueue~$\Prefix$'s children}
            \If {$s$ not in $\Prefix$}
                \State $Q$.push$(\,(\Prefix, s)\,)$
            \EndIf
        \EndFor
    \EndIf
\EndWhile
\State $(\OptimalRL, \OptimalObj) \gets (\CurrentRL, \CurrentObj)$ \Comment{Identify provably optimal solution}
\end{algorithmic}
\end{algorithm}

Specifically, the objective lower bound in Theorem~\ref{thm:bound}
enables us to prune the state space hierarchically.
%
While executing branch-and-bound, we keep track of the current best (smallest)
objective~$\CurrentObj$, thus it is a dynamic, monotonically decreasing quantity.
%
If we encounter a prefix~$\Prefix$ with lower bound
${b(\Prefix, \x, \y) \ge \CurrentObj}$,
then by Theorem~\ref{thm:bound}, we do not need to consider \emph{any}
rule list~${\RL' \in \StartsWith(\Prefix)}$ whose prefix~$\Prefix'$ starts with~$\Prefix$.
%because ${b(\Prefix', \x, \y) \ge b(\Prefix, \x, \y)}$. \\
%
For the objective of such a rule list, the current best objective
provides a lower bound, \ie
${\Obj(\RL', \x, \y) \ge b(\Prefix', \x, \y) \ge}$ ${b(\Prefix, \x, \y) \ge \CurrentObj}$,
and thus~$\RL'$ cannot be optimal.

Next, we state an immediate consequence of Theorem~\ref{thm:bound}.

\begin{lemma}[Objective lower bound with one-step lookahead]
\label{lemma:lookahead}
Let~$\Prefix$ be a $K$-prefix
and let~$\CurrentObj$ be the current best objective.
%
If ${b(\Prefix, \x, \y) + \Reg \ge \CurrentObj}$,
then for any $K'$-rule list ${\RL' \in \StartsWith(\Prefix)}$
whose prefix~$\Prefix'$ starts with~$\Prefix$ and~${K' > K}$,
it follows that ${\Obj(\RL', \x, \y) \ge \CurrentObj}$.
\end{lemma}

\begin{arxiv}
\begin{proof}
By the definition of the lower bound~\eqref{eq:lower-bound},
which includes the penalty for longer prefixes,
\begin{align}
\Obj(\Prefix', \x, y) \ge b(\Prefix', \x, \y) &= \Loss_p(\Prefix', \Labels', \x, \y) + \Reg K' \nn \\
&= \Loss_p(\Prefix', \Labels', \x, \y) + \Reg K + \Reg (K' - K) \nn \\
&= b(\Prefix, \x, \y) + \Reg (K' - K)
\ge b(\Prefix, \x, \y) + \Reg \ge \CurrentObj.
%\label{eq:lookahead}
\end{align}
\end{proof}
\end{arxiv}

Therefore, even if we encounter a prefix~$\Prefix$
with lower bound ${b(\Prefix, \x, \y) \le \CurrentObj}$,
\begin{kdd}
if
\end{kdd}
\begin{arxiv}
as long as
\end{arxiv}
${b(\Prefix, \x, \y) + \Reg \ge \CurrentObj}$, then we can prune
all prefixes~$\Prefix'$ that start with and are longer than~$\Prefix$.

\subsection{Upper Bounds on Prefix Length}
\label{sec:ub-prefix-length}

\begin{arxiv}
In this section, we derive several upper bounds on prefix length:
\begin{itemize}
\item The simplest upper bound on prefix length is given by the total
number of available antecedents. (Proposition~\ref{prop:trivial-length})
%
\item The current best objective~$\CurrentObj$ implies an upper bound
on prefix length. (Theorem~\ref{thm:ub-prefix-length})
%
\item For intuition, we state a version of the above bound that is valid
at the start of execution. (Corollary~\ref{cor:ub-prefix-length})
%
\item By considering specific families of prefixes,
we can obtain tighter bounds on prefix length. (Theorem~\ref{thm:ub-prefix-specific})
\end{itemize}
In the next section (\S\ref{sec:ub-size}), we use these results
to derive corresponding upper bounds on the number of
prefix evaluations made by Algorithm~\ref{alg:branch-and-bound}.

\begin{proposition}[Trivial upper bound on prefix length]
\label{prop:trivial-length}
Consider a state space of all rule lists formed from
a set of~$M$ antecedents,
and let~$L(\RL)$ be the length of rule list~$\RL$.
%
$M$ provides an upper bound on the length of
any optimal rule list
${\OptimalRL \in \argmin_\RL \Obj(\RL, \x, \y)}$,
\ie ${L(\RL) \le M}$.
\end{proposition}

\begin{proof}
Rule lists consist of distinct rules by definition.
\end{proof}
\end{arxiv}

At any point during branch-and-bound execution, the current best objective~$\CurrentObj$
implies an upper bound on the maximum prefix length we might still have to consider.
%
\begin{theorem}[Upper bound on prefix length]
\label{thm:ub-prefix-length}
Consider a state space of all rule lists formed from a set of~$M$ antecedents.
%
Let~$L(\RL)$ be the length of rule list~$\RL$
and let~$\CurrentObj$ be the current best objective.
%
For all optimal rule lists ${\OptimalRL \in \argmin_\RL \Obj(\RL, \x, \y)}$
\begin{arxiv}
\begin{align}
L(\OptimalRL) \le \min \left(\left\lfloor \frac{\CurrentObj}{\Reg} \right\rfloor, M \right),
\label{eq:max-length}
\end{align}
\end{arxiv}
\begin{kdd}
\begin{align}
L(\OptimalRL) \le \min \left(\left\lfloor \CurrentObj / \Reg \right\rfloor, M \right),
\label{eq:max-length}
\end{align}
\end{kdd}
where~$\Reg$ is the regularization parameter.
%
\begin{arxiv}
Furthermore, if~$\CurrentRL$ is a rule list with
objective ${\Obj(\CurrentRL, \x, \y) = \CurrentObj}$,
length~$K$, and zero misclassification error,
then for every optimal rule list
${\OptimalRL \in}$ ${\argmin_\RL \Obj(\RL, \x, \y)}$,
if ${\CurrentRL \in \argmin_d \Obj(\RL, \x, \y)}$,
then ${L(\OptimalRL) \le K}$,
or otherwise if ${\CurrentRL \notin \argmin_d \Obj(\RL, \x, \y)}$,
then ${L(\OptimalRL) \le K - 1}$.
\end{arxiv}
\end{theorem}

\begin{arxiv}
\begin{proof}
For an optimal rule list~$\OptimalRL$ with objective~$\OptimalObj$,
\begin{align}
\Reg L(\OptimalRL) \le \OptimalObj = \Obj(\OptimalRL, \x, \y)
= \Loss(\OptimalRL, \x, \y) + \Reg L(\OptimalRL)
\le \CurrentObj. \nn
\end{align}
The maximum possible length for~$\OptimalRL$ occurs
when~$\Loss(\OptimalRL, \x, \y)$ is minimized;
combining with Proposition~\ref{prop:trivial-length}
gives bound~\eqref{eq:max-length}.

For the rest of the proof,
let~${K^* = L(\OptimalRL)}$ be the length of~$\OptimalRL$.
%
If the current best rule list~$\CurrentRL$ has zero
misclassification error, then
\begin{align}
\Reg K^* \leq \Loss(\OptimalRL, \x, \y) + \Reg K^* = \Obj(\OptimalRL, \x, \y)
\le \CurrentObj = \Obj(\CurrentRL, \x, \y) = \Reg K, \nn
\end{align}
and thus ${K^* \leq K}$.
%
If the current best rule list is suboptimal,
\ie ${\CurrentRL \notin \argmin_\RL \Obj(\RL, \x, \y)}$, then
%
\begin{align}
\Reg K^* \leq \Loss(\OptimalRL, \x, \y) + \Reg K^* = \Obj(\OptimalRL, \x, \y)
< \CurrentObj = \Obj(\CurrentRL, \x, \y) = \Reg K, \nn
\end{align}
in which case ${K^* < K}$, \ie ${K^* \leq K-1}$, since $K$ is an integer.
\end{proof}

The latter part of Theorem~\ref{thm:ub-prefix-length} tells us that
if we only need to identify a single instance of an optimal rule list
${\OptimalRL \in \argmin_\RL \Obj(\RL, \x, \y)}$, and we encounter a perfect
$K$-rule list with zero misclassification error, then we can prune all
prefixes of length~$K$ or greater.

\end{arxiv}

\begin{corollary}[Simple upper bound on prefix length]
\label{cor:ub-prefix-length}
\begin{arxiv}
Let~$L(\RL)$ be the length of rule list~$\RL$.
\end{arxiv}
%
For all optimal rule lists ${\OptimalRL \in \argmin_\RL \Obj(\RL, \x, \y)}$,
\begin{arxiv}
\begin{align}
L(\OptimalRL) \le \min \left( \left\lfloor \frac{1}{2\Reg} \right\rfloor, M \right).
\label{eq:max-length-trivial}
\end{align}
\end{arxiv}
\begin{kdd}
\begin{align}
L(\OptimalRL) \le \min \left( \left\lfloor 1 / 2\Reg \right\rfloor, M \right).
\label{eq:max-length-trivial}
\end{align}
\end{kdd}
\end{corollary}

\begin{arxiv}
\begin{proof}
Let ${d = ((), (), q_0, 0)}$ be the empty rule list;
it has objective ${\Obj(\RL, \x, \y) = \Loss(\RL, \x, \y) \le}$ ${1/2}$,
which gives an upper bound on~$\CurrentObj$.
%
Combining with~\eqref{eq:max-length}
and Proposition~\ref{prop:trivial-length}
gives~\eqref{eq:max-length-trivial}.
\end{proof}
\end{arxiv}

For any particular prefix~$\Prefix$, we can obtain potentially tighter
upper bounds on prefix length for
\begin{arxiv}
the family of
\end{arxiv}
all prefixes that start with~$\Prefix$.

\begin{theorem}[Prefix-specific upper bound on prefix length]
\label{thm:ub-prefix-specific}
Let ${\RL = (\Prefix, \Labels, \Default, K)}$ be a rule list, let
${\RL' = (\Prefix', \Labels', \Default', K') \in \StartsWith(\Prefix)}$
be any rule list such that~$\Prefix'$ starts with~$\Prefix$,
and let~$\CurrentObj$ be the current best objective.
%
If~$\Prefix'$ has lower bound~${b(\Prefix', \x, \y) < \CurrentObj}$, then
\begin{align}
K' < \min \left( K + \left\lfloor \frac{\CurrentObj - b(\Prefix, \x, \y)}{\Reg} \right\rfloor, M \right).
\label{eq:max-length-prefix}
\end{align}
\end{theorem}

\begin{arxiv}
\begin{proof}
First, note that~${K' \ge K}$, since~$\Prefix'$ starts with~$\Prefix$.
%
Now recall from~\eqref{eq:prefix-lb} that
%
\begin{align}
b(\Prefix, \x, \y) = \Loss_p(\Prefix, \Labels, \x, \y) + \Reg K
\le \Loss_p(\Prefix', \Labels', \x, \y) + \Reg K' = b(\Prefix', \x, \y), \nn
\end{align}
%
and from~\eqref{eq:prefix-loss} that
${\Loss_p(\Prefix, \Labels, \x, \y) \le \Loss_p(\Prefix', \Labels', \x, \y)}$.
%
Combining these bounds and rearranging gives
\begin{align}
b(\Prefix', \x, \y) &= \Loss_p(\Prefix', \Labels', \x, \y) + \Reg K + \Reg(K' - K) \nn \\
&\ge \Loss_p(\Prefix, \Labels, \x, \y) + \Reg K + \Reg(K' - K)
= b(\Prefix, \x, \y) + \Reg (K' - K).
\label{eq:length-diff}
\end{align}
Combining~\eqref{eq:length-diff} with~${b(\Prefix', \x, \y) < \CurrentObj}$
and Proposition~\ref{prop:trivial-length} gives~\eqref{eq:max-length-prefix}.
\end{proof}
\end{arxiv}

We can view Theorem~\ref{thm:ub-prefix-specific} as a generalization
of our one-step lookahead bound (Lemma~\ref{lemma:lookahead}),
as~\eqref{eq:max-length-prefix} is equivalently a bound on ${K' - K}$,
an upper bound on the number of remaining `steps' corresponding to
an iterative sequence of single-rule extensions of a prefix~$\Prefix$.
%
\begin{arxiv}
Notice that when~${\RL = ((), (), q_0, 0)}$ is the empty rule list,
this bound replicates~\eqref{eq:max-length}, since ${b(\Prefix, \x, \y) = 0}$.
\end{arxiv}

\begin{arxiv}
\subsection{Upper Bounds on the Number of Prefix Evaluations}
\end{arxiv}
\begin{kdd}
\subsection{Upper bounds on prefix evaluations}
\end{kdd}
\label{sec:ub-size}

\begin{arxiv}
In this section, we use our upper bounds on prefix length
from~\S\ref{sec:ub-prefix-length} to derive corresponding
upper bounds on the number of prefix evaluations made by
Algorithm~\ref{alg:branch-and-bound}.
%
First, we
\end{arxiv}
\begin{kdd}
In this section, we use Theorem~\ref{thm:ub-prefix-specific}'s
upper bound on prefix length to derive a corresponding
upper bound on the number of prefix evaluations made by
Algorithm~\ref{alg:branch-and-bound}.
%
We
\end{kdd}
present Theorem~\ref{thm:remaining-eval-fine},
in which we use information about the state of
Algorithm~\ref{alg:branch-and-bound}'s execution
to calculate, for any given execution state,
upper bounds on the number of additional prefix evaluations that might
be required for the execution to complete.
%
The relevant execution state depends on the current
best objective~$\CurrentObj$ and information about
prefixes we are planning to evaluate, \ie prefixes in the
queue~$\Queue$ of Algorithm~\ref{alg:branch-and-bound}.
%
We define the number of \emph{remaining prefix evaluations} as the number of
prefixes that are currently in or will be inserted into the queue.

\begin{arxiv}
We use Theorem~\ref{thm:remaining-eval-fine} in some of our empirical results
(\S\ref{sec:experiments}, Figure~\ref{fig:objective}) to help illustrate
the dramatic impact of certain algorithm optimizations.
%
The execution trace of this upper bound on remaining prefix evaluations
complements the execution traces of other quantities,
\eg that of the current best objective~$\CurrentObj$.
%
After presenting Theorem~\ref{thm:remaining-eval-fine}, we also give two
weaker propositions that provide useful intuition.
%
In particular, Proposition~\ref{prop:remaining-eval-coarse} is a practical
approximation to Theorem~\ref{thm:remaining-eval-fine} that is significantly
easier to compute; we use it in our implementation as a metric of
execution progress that we display to the user.
\end{arxiv}

\begin{arxiv}
\begin{theorem}[Fine-grained upper bound on remaining prefix evaluations]~
\end{arxiv}
\begin{kdd}
\begin{theorem}[Upper bound on the number of remaining prefix evaluations]
\end{kdd}
\label{thm:remaining-eval-fine}
~Consider the state space of all rule lists formed from a set of~$M$ antecedents,
and consider Algorithm~\ref{alg:branch-and-bound} at a particular instant
during execution.
%
Let~$\CurrentObj$ be the current best objective, let~$\Queue$ be the queue,
and let~$L(\Prefix)$ be the length of prefix~$\Prefix$.
%
Define~${\Remaining(\CurrentObj, \Queue)}$ to be the number of remaining
prefix evaluations, then
\begin{align}
\Remaining(\CurrentObj, \Queue)
\le \sum_{\Prefix \in Q} \sum_{k=0}^{f(\Prefix)} \frac{(M - L(\Prefix))!}{(M - L(\Prefix) - k)!},
\label{eq:remaining}
\end{align}
\begin{arxiv}
where
\begin{align}
f(\Prefix) = \min \left( \left\lfloor
  \frac{\CurrentObj - b(\Prefix, \x, \y)}{\Reg} \right\rfloor, M - L(\Prefix)\right). \nn
%\label{eq:f}
\end{align}
\end{arxiv}
\begin{kdd}
\begin{align}
\text{where} \quad f(\Prefix) = \min \left( \left\lfloor
  \frac{\CurrentObj - b(\Prefix, \x, \y)}{\Reg} \right\rfloor, M - L(\Prefix)\right).
%\label{eq:f}
\end{align}
\end{kdd}
\end{theorem}

\begin{proof}
The number of remaining prefix evaluations is equal to the number of
prefixes that are currently in or will be inserted into queue~$\Queue$.
%
For any such prefix~$\Prefix$, Theorem~\ref{thm:ub-prefix-specific}
gives an upper bound on the length of any prefix~$\Prefix'$
that starts with~$\Prefix$:
\begin{align}
L(\Prefix') \le \min \left( L(\Prefix) + \left\lfloor \frac{\CurrentObj - b(\Prefix, \x, \y)}{\Reg} \right\rfloor, M \right)
\equiv U(\Prefix). \nn
\end{align}
This gives an upper bound on the number of remaining prefix evaluations:
\begin{arxiv}
\begin{align}
\Remaining(\CurrentObj, \Queue)
\le \sum_{\Prefix \in Q} \sum_{k=0}^{U(\Prefix) - L(\Prefix)} P(M - L(\Prefix), k)
= \sum_{\Prefix \in Q} \sum_{k=0}^{f(\Prefix)} \frac{(M - L(\Prefix))!}{(M - L(\Prefix) - k)!}\,, \nn
\end{align}
where~$P(m, k)$ denotes the number of $k$-permutations of~$m$.
\end{arxiv}
\begin{kdd}
${\Remaining(\CurrentObj, \Queue)
\le \sum_{\Prefix \in Q} \sum_{k=0}^{U(\Prefix) - L(\Prefix)} P(M - L(\Prefix), k)}$.
\end{kdd}
\end{proof}

\begin{arxiv}
Proposition~\ref{thm:ub-total-eval} is strictly weaker than
Theorem~\ref{thm:remaining-eval-fine} and is the starting point for its derivation.
It
\end{arxiv}
\begin{kdd}
The proposition below
\end{kdd}
is a na\"ive upper bound on
the total number of prefix evaluations over the course of
Algorithm~\ref{alg:branch-and-bound}'s execution.
%
It only depends on the number of rules and
the regularization parameter~$\Reg$;
\ie unlike Theorem~\ref{thm:remaining-eval-fine},
it does not use algorithm execution state to
bound the size of the search space.

\begin{arxiv}
\begin{proposition}[Upper bound on the total number of prefix evaluations]
\end{arxiv}
\begin{kdd}
\begin{proposition}[Upper bound on the total number of prefix evaluations]
\end{kdd}
\label{thm:ub-total-eval}
~Define $\TotalRemaining(\RuleSet)$ to be the total number of prefixes
evaluated by Algorithm~\ref{alg:branch-and-bound}, given the state space of
all rule lists formed from a set~$\RuleSet$ of~$M$ rules.
%
For any set~$\RuleSet$ of $M$ rules,
\begin{arxiv}
\begin{align}
\TotalRemaining(\RuleSet) \le \sum_{k=0}^K \frac{M!}{(M - k)!}, \nn
%\label{eq:size-naive}
\end{align}
where ${K = \min(\lfloor 1/2 \Reg \rfloor, M)}$.
\end{arxiv}
\begin{kdd}
\begin{align}
\TotalRemaining(\RuleSet) \le \sum_{k=0}^K \frac{M!}{(M - k)!},
\quad \text{where} \quad K = \min(\lfloor 1/2 \Reg \rfloor, M).
\label{eq:size-naive}
\end{align}
\end{kdd}
\end{proposition}

\begin{proof}
By Corollary~\ref{cor:ub-prefix-length},
${K \equiv \min(\lfloor 1 / 2 \Reg \rfloor, M)}$
gives an upper bound on the length of any optimal rule list.
%
\begin{arxiv}
Since we can think of our problem as finding the optimal
selection and permutation of~$k$ out of~$M$ rules,
over all~${k \le K}$,
\begin{align}
\TotalRemaining(\RuleSet) \le 1 + \sum_{k=1}^K P(M, k)
= \sum_{k=0}^K \frac{M!}{(M - k)!}. \nn
\end{align}
\end{arxiv}
\begin{kdd}
We obtain~\eqref{eq:size-naive} by viewing
our problem as finding the optimal
selection and permutation of~$k$ out of~$M$ rules,
over all~${k \le K}$.
\end{kdd}
\end{proof}

\begin{arxiv}

Our next upper bound is strictly tighter than the bound in
Proposition~\ref{thm:ub-total-eval}.
%
Like Theorem~\ref{thm:remaining-eval-fine}, it uses the
current best objective and information about
the lengths of prefixes in the queue to constrain
the lengths of prefixes in the remaining search space.
%
However, Proposition~\ref{prop:remaining-eval-coarse}
is weaker than Theorem~\ref{thm:remaining-eval-fine} because
it leverages only coarse-grained information from the queue.
%
Specifically, Theorem~\ref{thm:remaining-eval-fine} is
strictly tighter because it additionally incorporates
prefix-specific objective lower bound information from
prefixes in the queue, which further constrains
the lengths of prefixes in the remaining search space.

\begin{proposition}[Coarse-grained upper bound on remaining prefix evaluations] \hfill
\label{prop:remaining-eval-coarse}
Consider a state space of all rule lists formed from a set of~$M$ antecedents,
and consider Algorithm~\ref{alg:branch-and-bound} at a particular instant
during execution.
%
Let~$\CurrentObj$ be the current best objective, let~$\Queue$ be the queue,
and let~$L(\Prefix)$ be the length of prefix~$\Prefix$.
%
Let~$\Queue_j$ be the number of prefixes of length~$j$ in~$\Queue$,
\begin{align}
\Queue_j = \big | \{ \Prefix : L(\Prefix) = j, \Prefix \in \Queue \} \big | \nn
\end{align}
and let~${J = \argmax_{\Prefix \in \Queue} L(\Prefix)}$
be the length of the longest prefix in~$\Queue$.
%
Define~${\Remaining(\CurrentObj, \Queue)}$ to be the number of remaining
prefix evaluations, then
\begin{align}
\Remaining(\CurrentObj, \Queue)
\le \sum_{j=1}^J \Queue_j \left( \sum_{k=0}^{K-j} \frac{(M-j)!}{(M-j - k)!} \right), \nn
\end{align}
where~${K = \min(\lfloor \CurrentObj / \Reg \rfloor, M)}$.
\end{proposition}

\begin{proof}
The number of remaining prefix evaluations is equal to the number of
prefixes that are currently in or will be inserted into queue~$\Queue$.
%
For any such remaining prefix~$\Prefix$,
Theorem~\ref{thm:ub-prefix-length} gives an upper bound on its length;
define~$K$ to be this bound:
${L(\Prefix) \le \min(\lfloor \CurrentObj / \Reg \rfloor, M) \equiv K}$.
%
For any prefix~$\Prefix$ in queue~$\Queue$ with length~${L(\Prefix) = j}$,
the maximum number of prefixes that start with~$\Prefix$
and remain to be evaluated is:
\begin{align}
\sum_{k=0}^{K-j} P(M-j, k) = \sum_{k=0}^{K-j} \frac{(M-j)!}{(M-j - k)!}, \nn
\end{align}
where~${P(T, k)}$ denotes the number of $k$-permutations of~$T$.
%
This gives an upper bound on the number of remaining prefix evaluations:
\begin{align}
\Remaining(\CurrentObj, \Queue)
\le \sum_{j=0}^J \Queue_j \left( \sum_{k=0}^{K-j} P(M-j, k) \right)
= \sum_{j=0}^J \Queue_j \left( \sum_{k=0}^{K-j} \frac{(M-j)!}{(M-j - k)!} \right). \nn
\end{align}
\end{proof}
\end{arxiv}

\subsection{Lower Bounds on Antecedent Support}
\label{sec:lb-support}

In this section, we give two lower bounds on the normalized support
of each antecedent in any optimal rule list;
both are related to the regularization parameter~$\Reg$.

\begin{theorem}[Lower bound on antecedent support]
\label{thm:min-capture}
\begin{arxiv}
~Let ${\OptimalRL = (\Prefix, \Labels, \Default, K)}$
be any optimal rule list with objective~$\OptimalObj$, \ie
${\OptimalRL \in \argmin_\RL \Obj(\RL, \x, \y)}$.
\end{arxiv}
\begin{kdd}
Let ${\OptimalRL = (\Prefix, \Labels, \Default, K) \in \argmin_\RL \Obj(\RL, \x, \y)}$
be any optimal rule list, with objective~$\OptimalObj$.
\end{kdd}
For each antecedent~$p_k$ in prefix ${\Prefix = (p_1, \dots, p_K)}$,
\begin{arxiv}
the regularization parameter~$\Reg$ provides a lower bound
on the normalized support of~$p_k$,
\begin{align}
\Reg \le \Supp(p_k, \x \given \Prefix).
\label{eq:min-capture}
\end{align}
\end{arxiv}
\begin{kdd}
the regularization parameter provides a lower bound,
${\Reg \le \Supp(p_k, \x \given \Prefix)}$, on the normalized support of~$p_k$.
\end{kdd}
\end{theorem}

\begin{arxiv}
\begin{proof}
Let ${\OptimalRL = (\Prefix, \Labels, \Default, K)}$ be an optimal
rule list with prefix ${\Prefix = (p_1, \dots, p_K)}$
and labels ${\Labels = (q_1, \dots, q_K)}$.
%
Consider the rule list ${\RL = (\Prefix', \Labels', \Default', K-1)}$
derived from~$\OptimalRL$ by deleting a rule ${p_i \rightarrow q_i}$,
therefore ${\Prefix' = (p_1, \dots, p_{i-1}, p_{i+1}, \dots, p_K)}$
and ${\Labels' = (q_1, \dots, q_{i-1},}$ ${q'_{i+1}, \dots, q'_K)}$,
where~$q'_k$ need not be the same as~$q_k$, for ${k > i}$ and~${k = 0}$.

The largest possible discrepancy between~$\OptimalRL$ and~$\RL$ would occur
if~$\OptimalRL$ correctly classified all the data captured by~$p_i$,
while~$\RL$ misclassified these data.
%
This gives an upper bound:
\begin{align}
\Obj(\RL, \x, \y) = \Loss(\RL, \x, \y) + \Reg (K - 1)
&\le \Loss(\OptimalRL, \x, \y) + \Supp(p_i, \x \given \Prefix) + \Reg(K - 1) \nn \\
&= \Obj(\OptimalRL, \x, \y) + \Supp(p_i, \x \given \Prefix) - \Reg \nn \\
&= \OptimalObj + \Supp(p_i, \x \given \Prefix) - \Reg
\label{eq:ub-i}
\end{align}
where~$\Supp(p_i, \x \given \Prefix)$ is the normalized support of~$p_i$
in the context of~$\Prefix$, defined in~\eqref{eq:support-context},
and the regularization `bonus' comes from the fact that~$\RL$
is one rule shorter than~$\OptimalRL$.

At the same time, we must have ${\OptimalObj \le \Obj(\RL, \x, \y)}$ for~$\OptimalRL$ to be optimal.
%
Combining this with~\eqref{eq:ub-i} and rearranging gives~\eqref{eq:min-capture},
therefore the regularization parameter~$\Reg$ provides a lower bound
on the support of an antecedent~$p_i$ in an optimal rule list~$\OptimalRL$.
\end{proof}
\end{arxiv}

Thus, we can prune a prefix~$\Prefix$ if any of its antecedents captures less than
a fraction~$\Reg$ of data, even if~${b(\Prefix, \x, \y) < \OptimalObj}$.
%
\begin{arxiv}
Notice that the
\end{arxiv}
\begin{kdd}
The
\end{kdd}
bound in Theorem~\ref{thm:min-capture}
depends on the antecedents, but not the label predictions,
and thus does not account for misclassification error.
%
\begin{arxiv}
Theorem~\ref{thm:min-capture-correct} gives a tighter bound
by leveraging this additional information, which specifically
tightens the upper bound on~$\Obj(\RL, \x, \y)$ in~\eqref{eq:ub-i}.
\end{arxiv}
\begin{kdd}
Theorem~\ref{thm:min-capture-correct} gives a tighter bound
by leveraging this information.
\end{kdd}

\begin{theorem}[Lower bound on accurate antecedent support]
\label{thm:min-capture-correct}
\begin{arxiv}
Let ${\OptimalRL}$
be any optimal rule list with objective~$\OptimalObj$, \ie
${\OptimalRL = (\Prefix, \Labels, \Default, K) \in \argmin_\RL \Obj(\RL, \x, \y)}$.
%
Let $\OptimalRL$ have prefix ${\Prefix = (p_1, \dots, p_K)}$
and labels ${\Labels = (q_1, \dots, q_K)}$.
\end{arxiv}
\begin{kdd}
Let ${\OptimalRL \in \argmin_\RL \Obj(\RL, \x, \y)}$
be any optimal rule list, with objective~$\OptimalObj$;
let ${\OptimalRL = (\Prefix, \Labels, \Default, K)}$,
with prefix ${\Prefix = (p_1, \dots, p_K)}$
and labels ${\Labels = (q_1, \dots, q_K)}$.
\end{kdd}
%
For each rule~${p_k \rightarrow q_k}$ in~$\OptimalRL$,
define~$a_k$ to be the fraction of data that are captured by~$p_k$
and correctly classified:
\begin{align}
a_k \equiv \frac{1}{N} \sum_{n=1}^N
  \Cap(x_n, p_k \given \Prefix) \wedge \one [ q_k = y_n ].
\label{eq:rule-correct}
\end{align}
\begin{arxiv}
The regularization parameter~$\Reg$ provides a lower bound on~$a_k$:
\begin{align}
\Reg \le a_k.
\label{eq:min-capture-correct}
\end{align}
\end{arxiv}
\begin{kdd}
The regularization parameter provides a lower bound, $\Reg \le a_k$.
\end{kdd}
\end{theorem}

\begin{arxiv}
\begin{proof}
As in Theorem~\ref{thm:min-capture},
let ${\RL =  (\Prefix', \Labels', \Default', K-1)}$ be the rule list
derived from~$\OptimalRL$ by deleting a rule~${p_i \rightarrow q_i}$.
%
Now, let us define~$\Loss_i$ to be the portion of~$\OptimalObj$
due to this rule's misclassification error,
\begin{align}
\Loss_i \equiv \frac{1}{N} \sum_{n=1}^N
  \Cap(x_n, p_i \given \Prefix) \wedge \one [ q_i \neq y_n ]. \nn
\end{align}
The largest discrepancy between~$\OptimalRL$ and~$\RL$ would
occur if~$\RL$ misclassified all the data captured by~$p_i$.
%
This gives an upper bound on the difference between
the misclassification error of~$\RL$ and~$\OptimalRL$:
\begin{align}
\Loss(\RL, \x, \y) - \Loss(\OptimalRL, \x, \y)
&\le \Supp(p_i, \x \given \Prefix) - \Loss_i \nn \\
&= \frac{1}{N} \sum_{n=1}^N \Cap(x_n, p_i \given \Prefix)
  - \frac{1}{N} \sum_{n=1}^N
  \Cap(x_n, p_i \given \Prefix) \wedge \one [ q_i \neq y_n ] \nn \\
&= \frac{1}{N} \sum_{n=1}^N
  \Cap(x_n, p_i \given \Prefix) \wedge \one [ q_i = y_n ] = a_i, \nn
\end{align}
where we defined~$a_i$ in~\eqref{eq:rule-correct}.
%
Relating this bound to the objectives of~$\RL$ and~$\OptimalRL$ gives
\begin{align}
\Obj(\RL, \x, \y) = \Loss(\RL, \x, \y) + \Reg (K - 1)
&\le \Loss(\OptimalRL, \x, \y) + a_i + \Reg(K - 1) \nn \\
&= \Obj(\OptimalRL, \x, \y) + a_i - \Reg \nn \\
&= \OptimalObj + a_i - \Reg.
\label{eq:ub-ii}
\end{align}
Combining~\eqref{eq:ub-ii} with the requirement
${\OptimalObj \le \Obj(\RL, \x, \y)}$ gives the bound~${\Reg \le a_i}$.
\end{proof}
\end{arxiv}

Thus, we can prune a prefix if any of its rules correctly classifies
less than a fraction~$\Reg$ of data.
%
While the lower bound in Theorem~\ref{thm:min-capture} is a sub-condition
of the lower bound in Theorem~\ref{thm:min-capture-correct},
we can still leverage both---since the sub-condition is easier to check,
checking it first can accelerate pruning.
%
In addition to applying Theorem~\ref{thm:min-capture} in the context of
constructing rule lists, we can furthermore apply it in the context of
rule mining~(\S\ref{sec:setup}).
%
Specifically, it implies that we should only mine rules with
normalized support of at least~$\Reg$;
we need not mine rules with a smaller fraction of
\begin{arxiv}
observations.\footnote{We describe our application of this idea in
Appendix~\ref{appendix:data}, where we provide details on data processing.}
\end{arxiv}
\begin{kdd}
observations.\footnote{We describe our application of this idea in the appendix
of our long report~\citep{AngelinoLaAlSeRu17}.}
\end{kdd}
%
In contrast, we can only apply Theorem~\ref{thm:min-capture-correct}
in the context of constructing rule lists;
it depends on the misclassification error associated with each
rule in a rule list, thus it provides a lower bound on the number of
observations that each such rule must correctly classify.

\begin{arxiv}
\subsection{Upper Bound on Antecedent Support}
\label{sec:ub-support}

In the previous section~(\S\ref{sec:lb-support}), we proved lower bounds on
antecedent support; in Appendix~\ref{appendix:ub-supp},
we give an upper bound on antecedent support.
%
Specifically, Theorem~\ref{thm:ub-support} shows that an antecedent's
support in a rule list cannot be too similar to the set of data not
captured by preceding antecedents in the rule list.
%
In particular, Theorem~\ref{thm:ub-support} implies that we should
only mine rules with normalized support less than or equal to ${1 - \Reg}$;
we need not mine rules with a larger fraction of observations.
%
Note that we do not otherwise use this bound in our implementation,
because we did not observe a meaningful benefit in preliminary experiments.
\end{arxiv}

\begin{arxiv}
\subsection{Antecedent Rejection and its Propagation}
\label{sec:reject}

In this section, we demonstrate further consequences of
our lower~(\S\ref{sec:lb-support}) and upper
bounds (\S\ref{sec:ub-support}) on antecedent support,
under a unified framework we refer to as antecedent rejection.
%
Let ${\Prefix = (p_1, \dots, p_K)}$ be a prefix,
and let~$p_k$ be an antecedent in~$\Prefix$.
%
Define~$p_k$ to have insufficient support in~$\Prefix$
if it does not obey the bound in~\eqref{eq:min-capture}
of Theorem~\ref{thm:min-capture}.
%
Define~$p_k$ to have insufficient accurate support in~$\Prefix$
if it does not obey the bound in~\eqref{eq:min-capture-correct}
of Theorem~\ref{thm:min-capture-correct}.
%
Define~$p_k$ to have excessive support in~$\Prefix$
if it does not obey the bound in~\eqref{eq:ub-support}
of Theorem~\ref{thm:ub-support} (Appendix~\ref{appendix:ub-supp}).
%
If~$p_k$ in the context of~$\Prefix$ has insufficient support,
insufficient accurate support, or excessive support,
%or support too similar to the set of data not captured by
%preceding antecedents (Theorem~\ref{thm:ub-support}),
let us say that prefix~$\Prefix$ rejects antecedent~$p_K$.
%
Next, in Theorem~\ref{thm:reject}, we describe large classes of
related rule lists whose prefixes all reject the same antecedent.

\begin{theorem}[Antecedent rejection propagates]
\label{thm:reject}
For any prefix ${\Prefix = (p_1, \dots, p_K)}$,
let $\StartContains(\Prefix)$ denote the set of all
prefixes~$\Prefix'$ such that
the set of all antecedents in~$\Prefix$ is a subset of
the set of all antecedents in~$\Prefix'$, \ie
\begin{align}
\StartContains(\Prefix) =
\{\Prefix' = (p'_1, \dots, p'_{K'})
~s.t.~ \{p_k : p_k \in \Prefix \} \subseteq
\{p'_\kappa : p'_\kappa \in \Prefix'\}, K' \ge K \}.
\label{eq:start-contains}
\end{align}
%
Let ${\RL = (\Prefix, \Labels, \Default, K)}$ be a rule list
with prefix ${\Prefix = (p_1, \dots, p_{K-1}, p_{K})}$,
such that~$\Prefix$ rejects its last antecedent~$p_{K}$,
either because~$p_{K}$ in the context of~$\Prefix$ has
insufficient support, insufficient accurate support,
or excessive support.
%
Let ${\Prefix^{K-1} = (p_1, \dots, p_{K-1})}$ be the
first~${K - 1}$ antecedents of~$\Prefix$.
%
Let ${\RLB = (\PrefixB, \LabelsB, \DefaultB, \kappa)}$
be any rule list with prefix
${\PrefixB = (P_1, \dots, P_{K'-1},}$ ${P_{K'}, \dots, P_{\kappa})}$
such that~$\PrefixB$ starts with ${\PrefixB^{K'-1} =}$
${(P_1, \dots, P_{K'-1}) \in}$ ${\StartContains(\Prefix^{K-1})}$
and antecedent ${P_{K'} = p_{K}}$.
%
It follows that prefix~$\PrefixB$ rejects~$P_{K'}$
for the same reason that~$\Prefix$ rejects~$p_{K}$,
and furthermore, $\RLB$~cannot be optimal, \ie
${\RLB \notin \argmin_{\RL^\dagger} \Obj(\RL^\dagger, \x, \y)}$.
\end{theorem}

\begin{proof}
Combine Proposition~\ref{prop:min-capture}, Proposition~\ref{prop:min-capture-correct},
and Proposition~\ref{prop:ub-support}.
%
The first two are found below, and the last in Appendix~\ref{appendix:ub-supp}.
\end{proof}

Theorem~\ref{thm:reject} implies potentially significant
computational savings.
%
We know from Theorems~\ref{thm:min-capture},
\ref{thm:min-capture-correct}, and~\ref{thm:ub-support}
that during branch-and-bound execution, if we ever encounter a
prefix ${\Prefix = (p_1, \dots, p_{K-1}, p_K)}$ that rejects its
last antecedent~$p_K$, then we can prune~$\Prefix$.
%
By Theorem~\ref{thm:reject}, we can also prune \emph{any} prefix~$\Prefix'$
whose antecedents contains the set of antecedents in~$\Prefix$,
in almost any order, with the constraint that all antecedents
in ${\{p_1, \dots, p_{K-1}\}}$ precede~$p_K$.
%
These latter antecedents are also rejected
directly by the bounds in Theorems~\ref{thm:min-capture},
\ref{thm:min-capture-correct}, and~\ref{thm:ub-support};
this is how our implementation works in practice.
%
In a preliminary implementation (not shown), we maintained additional
data structures to support the direct use of Theorem~\ref{thm:reject}.
%
We leave the design of efficient data structures for this task as future work.

\begin{proposition}[Insufficient antecedent support propagates]
\label{prop:min-capture}
First define~$\StartContains(\Prefix)$ as in~\eqref{eq:start-contains},
and let ${\Prefix = (p_1, \dots, p_{K-1}, p_{K})}$ be a prefix,
such that its last antecedent~$p_{K}$ has insufficient support,
\ie the opposite of the bound in~\eqref{eq:min-capture}:
${\Supp(p_K, \x \given \Prefix) < \Reg}$.
%
Let ${\Prefix^{K-1} =}$ ${(p_1, \dots, p_{K-1})}$,
and let ${\RLB = (\PrefixB, \LabelsB, \DefaultB, \kappa)}$
be any rule list with prefix
${\PrefixB = (P_1, \dots, P_{K'-1},}$ ${P_{K'}, \dots, P_{\kappa})}$,
such that~$\PrefixB$ starts with ${\PrefixB^{K'-1} =}$
${(P_1, \dots, P_{K'-1}) \in \StartContains(\Prefix^{K-1})}$
and~${P_{K'} = p_{K}}$.
%
It follows that~$P_{K'}$ has insufficient support in
prefix~$\PrefixB$, and furthermore, $\RLB$~cannot be optimal,
\ie ${\RLB \notin \argmin_{\RL} \Obj(\RL, \x, \y)}$.
\end{proposition}

\begin{proof}
The support of~$p_K$ in~$\Prefix$ depends only on the
set of antecedents in ${\Prefix^{K} = (p_1, \dots, p_{K})}$:
\begin{align}
\Supp(p_K, \x \given \Prefix)
= \frac{1}{N} \sum_{n=1}^N \Cap(x_n, p_K \given \Prefix)
&= \frac{1}{N} \sum_{n=1}^N \left( \neg\, \Cap(x_n, \Prefix^{K-1}) \right)
  \wedge \Cap(x_n, p_K) \nn \\
&= \frac{1}{N} \sum_{n=1}^N \left( \bigwedge_{k=1}^{K-1} \neg\, \Cap(x_n, p_k) \right)
  \wedge \Cap(x_n, p_K)
< \Reg, \nn
\end{align}
and the support of~$P_{K'}$ in~$\PrefixB$ depends only on
the set of antecedents in ${\PrefixB^{K'} =}$ ${(P_1, \dots, P_{K'})}$:
\begin{align}
\Supp(P_{K'}, \x \given \PrefixB)
= \frac{1}{N} \sum_{n=1}^N \Cap(x_n, P_{K'} \given \PrefixB) %\nn \\
%&= \frac{1}{N} \sum_{n=1}^N \left( \neg\, \Cap(x_n, \PrefixB^{K'-1}) \right)
%  \wedge \Cap(x_n, P_{K'}) \nn \\
&= \frac{1}{N} \sum_{n=1}^N \left( \bigwedge_{k=1}^{K'-1} \neg\, \Cap(x_n, P_k) \right)
   \wedge \Cap(x_n, P_{K'}) \nn \\
&\le \frac{1}{N} \sum_{n=1}^N \left( \bigwedge_{k=1}^{K-1} \neg\, \Cap(x_n, p_k) \right)
  \wedge \Cap(x_n, P_{K'}) \nn \\
&= \frac{1}{N} \sum_{n=1}^N \left( \bigwedge_{k=1}^{K-1} \neg\, \Cap(x_n, p_k) \right)
  \wedge \Cap(x_n, p_{K}) \nn \\
&= \Supp(p_K, \x \given \Prefix) < \Reg.
\label{ineq:supp}
\end{align}
The first inequality reflects the condition that
${\PrefixB^{K'-1} \in \StartContains(\Prefix^{K-1})}$,
which implies that the set of antecedents in~$\PrefixB^{K'-1}$
contains the set of antecedents in~$\Prefix^{K-1}$,
and the next equality reflects the fact that~${P_{K'} = p_K}$.
%
Thus,~$P_K'$ has insufficient support in prefix~$\PrefixB$,
therefore by Theorem~\ref{thm:min-capture}, $\RLB$~cannot be optimal,
\ie ${\RLB \notin \argmin_{\RL} \Obj(\RL, \x, \y)}$.
\end{proof}

\begin{proposition}[Insufficient accurate antecedent support propagates]
\label{prop:min-capture-correct}
~Let~$\StartContains(\Prefix)$ denote the set of all
prefixes~$\Prefix'$ such that
the set of all antecedents in~$\Prefix$ is a subset of
the set of all antecedents in~$\Prefix'$,
as in~\eqref{eq:start-contains}.
%
Let ${\RL = (\Prefix, \Labels, \Default, K)}$ be a rule list
with prefix ${\Prefix = (p_1, \dots, p_{K})}$
and labels ${\Labels = (q_1, \dots, q_{K})}$, such that
the last antecedent~$p_{K}$ has insufficient accurate support,
\ie the opposite of the bound in~\eqref{eq:min-capture-correct}:
\begin{align}
\frac{1}{N} \sum_{n=1}^N \Cap(x_n, p_K \given \Prefix) \wedge \one [ q_K = y_n ]
< \Reg. \nn
\end{align}
%
Let ${\Prefix^{K-1} = (p_1, \dots, p_{K-1})}$
and let ${\RLB = (\PrefixB, \LabelsB, \DefaultB, \kappa)}$
be any rule list with prefix ${\PrefixB =}$ ${(P_1, \dots, P_{\kappa})}$
and labels ${\LabelsB = (Q_1, \dots, Q_{\kappa})}$,
such that~$\PrefixB$ starts with ${\PrefixB^{K'-1} =}$
${(P_1, \dots, P_{K'-1})}$ ${\in \StartContains(\Prefix^{K-1})}$
and ${P_{K'} = p_{K}}$.
%
It follows that~$P_{K'}$ has insufficient accurate support in
prefix~$\PrefixB$, and furthermore,
${\RLB \notin \argmin_{\RL^\dagger} \Obj(\RL^\dagger, \x, \y)}$.
\end{proposition}

\begin{proof}
The accurate support of~$P_{K'}$ in~$\PrefixB$ is insufficient:
\begin{align}
\frac{1}{N} \sum_{n=1}^N \Cap(x_n, P_{K'} &\given \PrefixB) \wedge \one [ Q_{K'} = y_n ] \nn \\
&= \frac{1}{N} \sum_{n=1}^N \left( \bigwedge_{k=1}^{K'-1} \neg\, \Cap(x_n, P_k) \right)
   \wedge \Cap(x_n, P_{K'}) \wedge \one [ Q_{K'} = y_n ] \nn \\
&\le \frac{1}{N} \sum_{n=1}^N \left( \bigwedge_{k=1}^{K-1} \neg\, \Cap(x_n, p_k) \right)
   \wedge \Cap(x_n, P_{K'}) \wedge \one [ Q_{K'} = y_n ] \nn \\
&= \frac{1}{N} \sum_{n=1}^N \left( \bigwedge_{k=1}^{K-1} \neg\, \Cap(x_n, p_k) \right)
   \wedge \Cap(x_n, p_K) \wedge \one [ Q_{K'} = y_n ] \nn \\
&= \frac{1}{N} \sum_{n=1}^N \Cap(x_n, p_K \given \Prefix) \wedge \one [ Q_{K'} = y_n ] \nn \\
&\le \frac{1}{N} \sum_{n=1}^N \Cap(x_n, p_K \given \Prefix) \wedge \one [ q_{K} = y_n ]
< \Reg. \nn
\end{align}
The first inequality reflects the condition that
${\PrefixB^{K'-1} \in \StartContains(\Prefix^{K-1})}$,
the next equality reflects the fact that~${P_{K'} = p_K}$.
%
For the following equality, notice that~$Q_{K'}$ is the majority
class label of data captured by~$P_{K'}$ in~$\PrefixB$, and~$q_K$
is the majority class label of data captured by~$P_K$ in~$\Prefix$,
and recall from~\eqref{ineq:supp} that
${\Supp(P_{K'}, \x \given \PrefixB) \le \Supp(p_{K}, \x \given \Prefix)}$.
%
By Theorem~\ref{thm:min-capture-correct},
${\RLB \notin \argmin_{\RL^\dagger} \Obj(\RL^\dagger, \x, \y)}$.
\end{proof}

Propositions~\ref{prop:min-capture} and~\ref{prop:min-capture-correct},
combined with Proposition~\ref{prop:ub-support} (Appendix~\ref{appendix:ub-supp}),
constitute the proof of Theorem~\ref{thm:reject}.
\end{arxiv}

\subsection{Equivalent Support Bound}
\label{sec:equivalent}

If two prefixes capture the same data, and one is more accurate than the other,
then there is no benefit to considering prefixes that start with the less accurate one.
%
Let~$\Prefix$ be a prefix,
and consider the best possible rule list whose prefix starts with~$\Prefix$.
%
If we take its antecedents in~$\Prefix$ and replace them with another prefix
with the same support (that could include different antecedents),
then its objective can only become worse or remain the same.

Formally, let~$\PrefixB$ be a prefix, and let~$\xi(\PrefixB)$ be the set
of all prefixes that capture exactly the same data as~$\PrefixB$.
%
Now, let~$\RL$ be a rule list with prefix~$\Prefix$
in~$\xi(\PrefixB)$, such that~$\RL$ has the minimum objective
over all rule lists with prefixes in~$\xi(\PrefixB)$.
%
Finally, let~$\RL'$ be a rule list whose prefix~$\Prefix'$
starts with~$\Prefix$, such that~$\RL'$ has the minimum objective
over all rule lists whose prefixes start with~$\Prefix$.
%
Theorem~\ref{thm:equivalent} below implies that~$\RL'$ also has
the minimum objective over all rule lists whose prefixes start with
\emph{any} prefix in~$\xi(\PrefixB)$.

\begin{theorem}[Equivalent support bound]
\label{thm:equivalent}
Define $\StartsWith(\Prefix)$ to be the set of all rule lists
whose prefixes start with~$\Prefix$, as in~\eqref{eq:starts-with}.
%
Let ${\RL = (\Prefix, \Labels, \Default, K)}$
be a rule list with prefix ${\Prefix = (p_1, \dots, p_K)}$,
and let ${\RLB = (\PrefixB, \LabelsB, \DefaultB, \kappa)}$
be a rule list with prefix ${\PrefixB = (P_1, \dots, P_{\kappa})}$,
such that~$\Prefix$ and~$\PrefixB$ capture the same data,~\ie
\begin{align}
\{x_n : \Cap(x_n, \Prefix)\} = \{x_n : \Cap(x_n, \PrefixB)\}. \nn
\end{align}
%
If the objective lower bounds of~$\RL$ and~$\RLB$
obey ${b(\Prefix, \x, \y) \le b(\PrefixB, \x, \y)}$,
then the objective of the optimal rule list in~$\StartsWith(\Prefix)$ gives a
lower bound on the objective of the optimal rule list in~$\StartsWith(\PrefixB)$:
\begin{align}
\min_{\RL' \in \StartsWith(\Prefix)} \Obj(\RL', \x, \y)
\le \min_{\RLB' \in \StartsWith(\PrefixB)} \Obj(\RLB', \x, \y).
\label{eq:equivalent}
\end{align}
\end{theorem}

\begin{arxiv}
\begin{proof}
See Appendix~\ref{appendix:equiv-supp} for the proof of Theorem~\ref{thm:equivalent}.
\end{proof}
\end{arxiv}

Thus, if prefixes~$\Prefix$ and~$\PrefixB$ capture the same data,
and their objective lower bounds obey
${b(\Prefix, \x, \y) \le b(\PrefixB, \x, \y)}$,
Theorem~\ref{thm:equivalent} implies that we can prune~$\PrefixB$.
%
%In our implementation, we call this symmetry-aware garbage collection.
%
Next, in Sections~\ref{sec:permutation} and~\ref{sec:permutation-counting},
we highlight and analyze the special case of prefixes that capture
the same data because they contain the same antecedents.

\subsection{Permutation Bound}% for permutation-aware garbage collection}
\label{sec:permutation}

If two prefixes are composed of the same antecedents,
\ie they contain the same antecedents up to a permutation,
then they capture the same data, and thus Theorem~\ref{thm:equivalent} applies.
%
Therefore, if one is more accurate than the other, then there is no benefit to
considering prefixes that start with the less accurate one.
%
Let~$\Prefix$ be a prefix,
and consider the best possible rule list whose prefix starts with~$\Prefix$.
%
If we permute its antecedents in~$\Prefix$,
then its objective can only become worse or remain the same.

Formally, let~${P = \{p_k\}_{k=1}^K}$ be a set of~$K$ antecedents,
and let~$\Pi$ be the set of all $K$-prefixes corresponding to
permutations of antecedents in~$P$.
%
Let prefix~$\Prefix$ in~$\Pi$ have the minimum prefix misclassification
error over all prefixes in~$\Pi$.
%
Also, let~$\RL'$ be a rule list whose prefix~$\Prefix'$
starts with~$\Prefix$, such that~$\RL'$ has the minimum objective
over all rule lists whose prefixes start with~$\Prefix$.
%
Corollary~\ref{thm:permutation} below,
which can be viewed as special case of Theorem~\ref{thm:equivalent},
implies that~$\RL'$ also has the minimum objective over all
rule lists whose prefixes start with \emph{any} prefix in~$\Pi$.

\begin{corollary}[Permutation bound]
\label{thm:permutation}
Let~$\pi$ be any permutation of ${\{1, \dots, K\}}$,
\begin{arxiv}
and define ${\StartsWith(\Prefix) = }$
${\{(\Prefix', \Labels', \Default', K') : \Prefix' \textnormal{ starts with } \Prefix \}}$
to be the set of all rule lists whose prefixes start with~$\Prefix$.
\end{arxiv}
\begin{kdd}
and define $\StartsWith(\Prefix)$
to be the set of all rule lists whose prefix starts with~$\Prefix$,
as in~\eqref{eq:starts-with}.
\end{kdd}
%
Let ${\RL = (\Prefix, \Labels, \Default, K)}$
and ${\RLB = (\PrefixB, \LabelsB, \DefaultB, K)}$
denote rule lists with prefixes ${\Prefix = (p_1, \dots, p_K)}$
and ${\PrefixB = (p_{\pi(1)}, \dots, p_{\pi(K)})}$,
respectively, \ie the antecedents in~$\PrefixB$
correspond to a permutation of the antecedents in~$\Prefix$.
%
If the objective lower bounds of~$\RL$ and~$\RLB$
obey ${b(\Prefix, \x, \y) \le b(\PrefixB, \x, \y)}$,
then the objective of the optimal rule list in~$\StartsWith(\Prefix)$ gives a
lower bound on the objective of the optimal rule list in~$\StartsWith(\PrefixB)$:
\begin{align}
\min_{\RL' \in \StartsWith(\Prefix)} \Obj(\RL', \x, \y)
\le \min_{\RLB' \in \StartsWith(\PrefixB)} \Obj(\RLB', \x, \y). \nn
%\label{eq:permutation}
\end{align}
\end{corollary}

\begin{arxiv}
\begin{proof}
Since prefixes~$\Prefix$ and~$\PrefixB$ contain
the same antecedents, they both capture the same data.
Thus, we can apply Theorem~\ref{thm:equivalent}.
\end{proof}
\end{arxiv}

Thus if prefixes~$\Prefix$ and~$\PrefixB$ have the same antecedents,
up to a permutation, and their objective lower bounds
obey~${b(\Prefix, \x, \y) \le}$ ${b(\PrefixB, \x, \y)}$,
Corollary~\ref{thm:permutation} implies that we can prune~$\PrefixB$.
%
We call this %permutation-aware garbage collection,
symmetry-aware pruning,
and we illustrate the subsequent
computational savings next in~\S\ref{sec:permutation-counting}.

\begin{arxiv}
\subsection{Upper Bound on Prefix Evaluations with Symmetry-aware Pruning}
\end{arxiv}
\begin{kdd}
\subsubsection{Upper bound on prefix evaluations with symmetry-aware pruning}
\end{kdd}
\label{sec:permutation-counting}

Here, we present an upper bound on the total number of prefix
evaluations that accounts for the effect of symmetry-aware
pruning~(\S\ref{sec:permutation}).
%
Since every subset of~$K$ antecedents generates an equivalence
class of~$K!$ prefixes equivalent up to permutation, symmetry-aware
pruning dramatically reduces the search space.

\begin{arxiv}
First, notice that
\end{arxiv}
Algorithm~\ref{alg:branch-and-bound} describes a
breadth-first exploration of the state space of rule lists.
%
Now suppose we integrate symmetry-aware pruning into
our execution of branch-and-bound, so that after evaluating
prefixes of length~$K$, we only keep a single best prefix
from each set of prefixes equivalent up to a permutation.

\begin{theorem}[\fontdimen2\font=0.6ex Upper bound on prefix evaluations with symmetry-aware pruning]
%
Consider a state space of all rule lists formed from a set~$\RuleSet$
of~$M$ antecedents, and consider the branch-and-bound algorithm with
symmetry-aware pruning.
%
Define $\TotalRemaining(\RuleSet)$ to be the total number of prefixes evaluated.
%
For any set~$\RuleSet$ of $M$ rules,
\begin{align}
\TotalRemaining(\RuleSet)
\le  1 + \sum_{k=1}^K \frac{1}{(k - 1)!} \cdot \frac{M!}{(M - k)!}, \nn
\end{align}
where ${K = \min(\lfloor 1 / 2 \Reg \rfloor, M)}$.
\end{theorem}

\begin{proof}
By Corollary~\ref{cor:ub-prefix-length},
${K \equiv \min(\lfloor 1 / 2 \Reg \rfloor, M)}$
gives an upper bound on the length of any optimal rule list.
%
The algorithm begins by evaluating the empty prefix,
followed by~$M$ prefixes of length~${k=1}$,
then~${P(M, 2)}$ prefixes of length~${k=2}$,
where~${P(M, 2)}$ is the number of size-2 subsets of~$\{1, \dots, M \}$.
%
Before proceeding to length~${k=3}$, we keep only~${C(M, 2)}$
prefixes of length~${k=2}$, where~${C(M, k)}$ denotes the
number of $k$-combinations of~$M$.
%
Now, the number of length~${k=3}$ prefixes we evaluate is~${C(M, 2) (M - 2)}$.
%
Propagating this forward~gives
\begin{arxiv}
\begin{align}
\TotalRemaining(\RuleSet) \le 1 + \sum_{k=1}^K C(M, k-1) (M - k + 1)
%= 1 + \sum_{k=1}^K {M \choose k-1}(M - k + 1)
%= 1 + \sum_{k=1}^K \frac{M! (M - k + 1)}{(k - 1)! (M - k + 1)!}
= 1 + \sum_{k=1}^K \frac{1}{(k - 1)!} \cdot \frac{M!}{(M - k)!}. \nn
\end{align}
\end{arxiv}
\begin{kdd}
${\TotalRemaining(\RuleSet) \le 1 + \sum_{k=1}^K C(M, k-1) (M - k + 1)}$.
\end{kdd}
\end{proof}

Pruning based on permutation symmetries thus yields significant
computational savings.
%
Let us compare, for example, to the na\"ive number of prefix evaluations
given by the upper bound in Proposition~\ref{thm:ub-total-eval}.
%
If~${M = 100}$ and~${K = 5}$, then the na\"ive number is about
${9.1 \times 10^9}$, while the reduced number due to symmetry-aware
pruning is about ${3.9 \times 10^8}$,
which is smaller by a factor of about~23.
%
If~${M=1000}$ and~${K = 10}$, the number of evaluations falls from
about~${9.6 \times 10^{29}}$ to about~${2.7 \times 10^{24}}$,
which is smaller by a factor of about~360,000.
\begin{arxiv}

\end{arxiv}
\begin{kdd}
%
\end{kdd}
While~$10^{24}$ seems infeasibly enormous,
it does not represent the number of rule lists we evaluate.
%
\begin{kdd}
As we show in~\S\ref{sec:experiments},
\end{kdd}
\begin{arxiv}
As we show in our experiments~(\S\ref{sec:experiments}),
\end{arxiv}
our permutation bound in Corollary~\ref{thm:permutation}
and our other bounds together conspire to reduce the search space
to a size manageable on a single computer.
%
The choice of ${M=1000}$ and ${K=10}$ in our example above
corresponds to the state space size our efforts target.
%
${K=10}$ rules represents a (heuristic) upper limit on
the size of an interpretable rule list,
and ${M=1000}$ represents the approximate number of rules
with sufficiently high support (Theorem~\ref{thm:min-capture})
we expect to obtain via rule mining~(\S\ref{sec:setup}).

\begin{arxiv}
\subsection{Similar Support Bound}
\label{sec:similar}

We now present a relaxation of Theorem~\ref{thm:equivalent},
our equivalent support bound.
%
Theorem~\ref{thm:similar} implies that if we know that no extensions of
a prefix~$\Prefix$ are better than the current best objective,
then we can prune all prefixes with support similar to~$\Prefix$'s support.
%
Understanding how to exploit this result in practice
represents an exciting direction for future work;
our implementation~(\S\ref{sec:implementation}) does not
currently leverage the bound in Theorem~\ref{thm:similar}.

\begin{theorem}[Similar support bound]
\label{thm:similar}
Define $\StartsWith(\Prefix)$ to be the set of all rule lists
whose prefixes start with~$\Prefix$, as in~\eqref{eq:starts-with}.
%
Let ${\Prefix = (p_1, \dots, p_K)}$ and
${\PrefixB = (P_1, \dots, P_{\kappa})}$ be prefixes
that capture nearly the same data.
%
Specifically, define~$\omega$ to be the normalized support
of data captured by~$\Prefix$ and not captured by~$\PrefixB$, \ie
%and let us require that~${\omega \le \Reg}$, \ie
\begin{align}
\omega \equiv \frac{1}{N} \sum_{n=1}^N
  \neg\, \Cap(x_n, \PrefixB)
  \wedge \Cap(x_n, \Prefix). % \le \Reg,
\label{eq:omega}
\end{align}
%where~$\Reg$ is the regularization parameter.
%
Similarly, define~$\Omega$ to be the normalized support
of data captured by~$\PrefixB$ and not captured by~$\Prefix$, \ie
%and let us require that~${\Omega \le \Reg}$, \ie
\begin{align}
\Omega \equiv \frac{1}{N} \sum_{n=1}^N
  \neg\, \Cap(x_n, \Prefix)
  \wedge \Cap(x_n, \PrefixB). %\le \Reg.
\label{eq:big-omega}
\end{align}
We can bound the difference between the objectives of the
optimal rule lists in~$\StartsWith(\Prefix)$
and $\StartsWith(\PrefixB)$ as follows:
\begin{align}
\min_{\RLB^\dagger \in \StartsWith(\PrefixB)} \Obj(\RLB^\dagger, \x, \y)
- \min_{\RL^\dagger \in \StartsWith(\Prefix)} \Obj(\RL^\dagger, \x, \y)
&\ge b(\PrefixB, \x, \y) - b(\Prefix, \x, \y) - \omega - \Omega,
\label{eq:similar}
\end{align}
where~$b(\Prefix, \x, \y)$ and~$b(\PrefixB, \x, \y)$ are the
objective lower bounds of~$\RL$ and~$\RLB$, respectively.
\end{theorem}

\begin{proof}
See Appendix~\ref{appendix:similar-supp} for the proof of Theorem~\ref{thm:similar}.
\end{proof}

Theorem~\ref{thm:similar} implies that if prefixes~$\Prefix$
and~$\PrefixB$ are similar, and we know the optimal objective
of rule lists starting with~$\Prefix$, then
\begin{align}
\min_{\RLB' \in \StartsWith(\PrefixB)} \Obj(\RLB', \x, \y)
&\ge \min_{\RL' \in \StartsWith(\Prefix)} \Obj(\RL', \x, \y)
+ b(\PrefixB, \x, \y) - b(\Prefix, \x, \y) - \chi \nn \\
&\ge \CurrentObj + b(\PrefixB, \x, \y) - b(\Prefix, \x, \y) - \chi, \nn
\end{align}
where~$\CurrentObj$ is the current best objective,
and~$\chi$ is the normalized support of the set of data captured
either exclusively by~$\Prefix$ or exclusively by~$\PrefixB$.
%
It follows that
\begin{align}
\min_{\RLB' \in \StartsWith(\PrefixB)} \Obj(\RLB', \x, \y)
\ge \CurrentObj + b(\PrefixB, \x, \y) - b(\Prefix, \x, \y) - \chi \ge \CurrentObj \nn
\end{align}
if ${b(\PrefixB, \x, \y) - b(\Prefix, \x, \y) \ge \chi}$.
%
To conclude, we summarize this result and combine it with
our notion of lookahead from Lemma~\ref{lemma:lookahead}.
%
During branch-and-bound execution, if we demonstrate that
${\min_{\RL' \in \StartsWith(\Prefix)} \Obj(\RL', \x, \y) \ge \CurrentObj}$,
then we can prune all prefixes that start with any
prefix~$\PrefixB'$ in the following set:
\begin{align}
\left\{ \PrefixB' : b(\PrefixB', \x, \y) + \Reg - b(\Prefix, \x, \y) \ge
\frac{1}{N} \sum_{n=1}^N \Cap(x_n, \Prefix) \oplus \Cap(x_n, \PrefixB') \right\}, \nn
\end{align}
where the symbol~$\oplus$ denotes the logical operation, exclusive or (XOR).

\begin{comment}
\begin{theorem}[Very similar support bound]
\label{thm:very-similar}
Define ${\StartsWith(\Prefix) = }$
${\{(\Prefix', \Labels', \Default', K') : \Prefix' \textnormal{ starts with } \Prefix \}}$
to be the set of all rule lists whose prefixes start with~$\Prefix$.
%
Let ${\Prefix = (p_1, \dots, p_K)}$ and
${\PrefixB = (P_1, \dots, P_{\kappa})}$ be prefixes
that capture the same data, \ie
\begin{align}
\{x_n : \Cap(x_n, \Prefix)\} = \{x_n : \Cap(x_n, \PrefixB)\}.
\end{align}
Now let ${\Prefix' = (p_1, \dots, p_K, p_{K+1})}$ be a prefix
that starts with~$\Prefix$ and ends with antecedent~$p_{K+1}$,
and let ${\PrefixB' = (P_1, \dots, P_{\kappa}, P_{\kappa + 1})}$
be a prefix that starts with~$\PrefixB$ and ends with
antecedent~$P_{\kappa + 1}$, such that~$p_{K+1}$
and~$P_{\kappa + 1}$ have nearly the same support in~$\Prefix'$
and~$\PrefixB'$, respectively.
%
Specifically, define~$\omega$ to be the normalized support
of data captured by~$p_{K + 1}$ in~$\Prefix'$ and not
captured by~$P_{\kappa + 1}$ in~$\PrefixB'$,
and let us require that~${\omega \le \Reg}$, \ie
\begin{align}
\omega \equiv \frac{1}{N} \sum_{n=1}^N
  \neg\, \Cap(x_n, P_{\kappa+1} \given \PrefixB')
  \wedge \Cap(x_n, p_{K+1} \given \Prefix') \le \Reg,
\end{align}
where~$\Reg$ is the regularization parameter.
%
Similarly, define~$\Omega$ to be the normalized support
of data captured by~$P_{\kappa + 1}$ in~$\PrefixB'$ and not
captured by~$p_{K + 1}$ in~$\Prefix'$,
and let us require that~${\Omega \le \Reg}$, \ie
\begin{align}
\Omega \equiv \frac{1}{N} \sum_{n=1}^N
  \neg\, \Cap(x_n, p_{K+1} \given \Prefix')
  \wedge \Cap(x_n, P_{\kappa+1} \given \PrefixB') \le \Reg.
\end{align}
The difference between the objective of the optimal rule list
in~$\StartsWith(\Prefix')$ and that in~$\StartsWith(\PrefixB')$
is at most~$2\Reg$:
\begin{align}
\left | \min_{\RL^\dagger \in \StartsWith(\Prefix')} \Obj(\RL^\dagger, \x, \y)
  - \min_{\RLB^\dagger \in \StartsWith(\PrefixB')} \Obj(\RLB^\dagger, \x, \y)
  \right | \le 2 \Reg.
\label{eq:very-similar}
\end{align}
\end{theorem}

\begin{proof}
\dots
\end{proof}

Suppose prefixes~$\cal P$ and~$\cal Q$ capture the same data,
and now derive~$\cal P'$ from~$\cal P$ by appending antecedent~$p$
and derive~$\cal Q'$ from~$\cal Q$ by appending antecedent~$q$.
%
Suppose further that~$p$ and~$q$ capture nearly the same data, except that
they exclusively capture data~$x_p$ and~$x_q$ in their respective contexts,
such that the normalized support of~$x_p$ and of~$x_q$ are each bounded by
the regularization parameter: ${s(x_p), s(x_q) < c}$.
%
Our minimum support bound~\eqref{eq:min-capture} implies
that~$p$ would never be placed below~$\cal Q'$ nor~$q$ below~$\cal P'$.

Extensions of~$\cal P'$ and~$\cal Q'$ will behave similarly.
%
The largest difference would occur if a rule list starting with~$\cal P'$
misclassified all of~$x_p$ and~$x_q$, while the analogous rule list starting
with~$\cal Q'$ correctly classified all these data, or vice versa,
yielding a difference between objectives bounded by~$2c$.
%
Let~$\cal P^*$ and~$\cal Q^*$ be the optimal prefixes
starting with~$\cal P'$ and~$\cal Q'$, respectively.
%
Note that~$\cal P^*$ and~$\cal Q^*$ need not be derived from~$\cal P'$ and~$\cal Q'$
via analogous extensions.
%
If we know~$\cal P^*$, then we can avoid evaluating \emph{any} extensions of~$\cal Q'$ if
\begin{align}
\Obj(\Prefix^*) - 2 c \ge \Obj^*,
\end{align}
where~$\Obj^*$ is the best known objective, since the left had expression
provides a lower bound on~$\Obj(\cal{Q}^*)$.
\end{comment}

\end{arxiv}

\subsection{Equivalent Points Bound}
\label{sec:identical}

The bounds in this section quantify the following:
%
If multiple observations that are not captured by a prefix~$\Prefix$
have identical features and opposite labels, then no rule list that
starts with~$\Prefix$ can correctly classify all these observations.
%
For each set of such observations, the number of mistakes is at least
the number of observations with the minority label within the set.

Consider a data set~${\{(x_n, y_n)\}_{n=1}^N}$ and also a set of antecedents
${\{s_m\}_{m=1}^M}$.
%
Define distinct observations to be equivalent if they are captured by
exactly the same antecedents, \ie ${x_i \neq x_j}$ are equivalent~if
\begin{align}
\frac{1}{M} \sum_{m=1}^M \one [ \Cap(x_i, s_m) = \Cap(x_j, s_m) ] = 1. \nn
\end{align}
Notice that we can partition a data set into sets of equivalent points;
let~${\{e_u\}_{u=1}^U}$ enumerate these sets.
%
Let~$e_u$ be the equivalent points set that contains observation~$x_i$.
%
Now define~$\theta(e_u)$ to be the normalized support of the minority
class label with respect to set~$e_u$, \eg let
\begin{arxiv}
\begin{align}
{e_u = \{x_n : \forall m \in [M],\, \one [ \Cap(x_n, s_m) = \Cap(x_i, s_m) ] \}}, \nn
\end{align}
\end{arxiv}
\begin{kdd}
${e_u = \{x_n : \forall m \in [M],\, \one [ \Cap(x_n, s_m) = \Cap(x_i, s_m) ] \}}$,
\end{kdd}
and let~$q_u$ be the minority class label among points in~$e_u$, then
\begin{align}
\theta(e_u) = \frac{1}{N} \sum_{n=1}^N \one [ x_n \in e_u ]\, \one [ y_n = q_u ].
\label{eq:theta}
\end{align}

The existence of equivalent points sets with non-singleton support
yields a tighter objective lower bound that we can combine with our other bounds;
as our experiments demonstrate~(\S\ref{sec:experiments}),
the practical consequences can be dramatic.
%
First, for intuition, we present a general bound in
Proposition~\ref{prop:identical}; next, we explicitly integrate
this bound into our framework in Theorem~\ref{thm:identical}.

\begin{proposition}[General equivalent points bound]
\label{prop:identical}
Let ${\RL = (\Prefix, \Labels, \Default, K)}$ be a rule list, then
\begin{arxiv}
\begin{align}
\Obj(\RL, \x, \y) \ge \sum_{u=1}^U \theta(e_u) + \Reg K. \nn
\end{align}
\end{arxiv}
\begin{kdd}
${\Obj(\RL, \x, \y) \ge \sum_{u=1}^U \theta(e_u) + \Reg K}$.
\end{kdd}
\end{proposition}

\begin{arxiv}
\begin{proof}
Recall that the objective is ${\Obj(\RL, \x, \y) = \Loss(\RL, \x, \y) + \Reg K}$,
where the misclassification error~${\Loss(\RL, \x, \y)}$ is given by
\begin{align}
\Loss(\RL, \x, \y) &= \Loss_0(\Prefix, \Default, \x, \y) + \Loss_p(\Prefix, \Labels, \x, \y) \nn \\
&= \frac{1}{N} \sum_{n=1}^N \left( \neg\, \Cap(x_n, \Prefix) \wedge \one[q_0 \neq y_n]
   + \sum_{k=1}^K \Cap(x_n, p_k \given \Prefix) \wedge \one [q_k \neq y_n] \right). \nn
\end{align}
Any particular rule list uses a specific rule, and therefore a single class label,
to classify all points within a set of equivalent points.
%
Thus, for a set of equivalent points~$u$, the rule list~$\RL$ correctly classifies either
points that have the majority class label, or points that have the minority class label.
%
It follows that~$\RL$ misclassifies a number of points in~$u$ at least as great as
the number of points with the minority class label.
%
To translate this into a lower bound on~$\Loss(\RL, \x, \y)$,
we first sum over all sets of equivalent points, and then for each such set,
count differences between class labels and the minority class label of the set,
instead of counting mistakes:
\begin{align}
&\Loss(\RL, \x, \y) \nn \\
&= \frac{1}{N} \sum_{u=1}^U \sum_{n=1}^N \left( \neg\, \Cap(x_n, \Prefix) \wedge \one[q_0 \neq y_n]
   + \sum_{k=1}^K \Cap(x_n, p_k \given \Prefix) \wedge \one [q_k \neq y_n] \right)
   \one [x_n \in e_u]  \nn \\
&\ge \frac{1}{N} \sum_{u=1}^U \sum_{n=1}^N \left( \neg\, \Cap(x_n, \Prefix) \wedge \one[y_n = q_u]
   + \sum_{k=1}^K \Cap(x_n, p_k \given \Prefix) \wedge \one [y_n = q_u] \right)
   \one [x_n \in e_u].
\label{eq:lb-equiv-pts}
\end{align}
Next, we factor out the indicator for equivalent point set membership,
which yields a term that sums to one, because every datum is either captured or
not captured by prefix~$\Prefix$.
\begin{align}
\Loss(\RL, \x, \y) &= \frac{1}{N} \sum_{u=1}^U \sum_{n=1}^N \left( \neg\, \Cap(x_n, \Prefix)
   + \sum_{k=1}^K \Cap(x_n, p_k \given \Prefix) \right)
   \wedge \one [x_n \in e_u]\, \one[y_n = q_u] \nn \\
&= \frac{1}{N} \sum_{u=1}^U \sum_{n=1}^N \left( \neg\, \Cap(x_n, \Prefix)
   + \Cap(x_n, \Prefix) \right)
   \wedge \one [x_n \in e_u]\, \one[y_n = q_u] \nn \\
&= \frac{1}{N} \sum_{u=1}^U \sum_{n=1}^N \one [ x_n \in e_u ]\, \one [ y_n = q_u ]
= \sum_{u=1}^U \theta(e_u), \nn
\end{align}
where the final equality applies the definition of~$\theta(e_u)$ in~\eqref{eq:theta}.
%
Therefore, ${\Obj(\RL, \x, \y) =}$ ${\Loss(\RL, \x, \y) + \Reg K}$ ${\ge \sum_{u=1}^U \theta(e_u) + \Reg K}$.
\end{proof}
\end{arxiv}

Now, recall that to obtain our lower bound~${b(\Prefix, \x, \y)}$
in~\eqref{eq:lower-bound}, we simply deleted the
default rule misclassification error~$\Loss_0(\Prefix, \Default, \x, \y)$
from the objective~${\Obj(\RL, \x, \y)}$.
%
Theorem~\ref{thm:identical} obtains a tighter objective lower bound
via a tighter lower bound on the default rule misclassification error,
${0 \le b_0(\Prefix, \x, \y) \le}$ $\Loss_0(\Prefix, \Default, \x, \y)$.

\begin{theorem}[Equivalent points bound]
\label{thm:identical}
Let~$\RL$ be a rule list with prefix~$\Prefix$
and lower bound ${b(\Prefix, \x, \y)}$,
then for any rule list~${\RL' \in \StartsWith(\RL)}$
whose prefix~$\Prefix'$ starts with~$\Prefix$,
\begin{arxiv}
\begin{align}
\Obj(\RL', \x, \y) \ge b(\Prefix, \x, \y) + b_0(\Prefix, \x, \y),
\label{eq:identical}
\end{align}
where
\end{arxiv}
\begin{kdd}
\begin{align}
\Obj(\RL', \x, \y) \ge b(\Prefix, \x, \y) + b_0(\Prefix, \x, \y), \quad \text{where}
\end{align}
\end{kdd}
\begin{arxiv}
\begin{align}
b_0(\Prefix, \x, \y) = \frac{1}{N} \sum_{u=1}^U \sum_{n=1}^N
    \neg\, \Cap(x_n, \Prefix) \wedge \one [ x_n \in e_u ]\, \one [ y_n = q_u ].
\label{eq:lb-b0}
\end{align}
\end{arxiv}
\begin{kdd}
\begin{align}
b_0(\Prefix, \x, \y) = \frac{1}{N} \sum_{u=1}^U \sum_{n=1}^N
    \neg\, \Cap(x_n, \Prefix) \wedge \one [ x_n \in e_u ]\, \one [ y_n = q_u ]. \nn
\end{align}
\end{kdd}
\end{theorem}

\begin{arxiv}
\begin{proof}
See Appendix~\ref{appendix:equiv-pts} for the proof of Theorem~\ref{thm:identical}.
\end{proof}
\end{arxiv}

\begin{arxiv}
\section{Incremental Computation}
\label{sec:incremental}
\end{arxiv}

For every prefix~$\Prefix$ evaluated during
Algorithm~\ref{alg:branch-and-bound}'s execution, we compute
the objective lower bound~${b(\Prefix, \x, \y)}$ and sometimes
the objective~${\Obj(\RL, \x, \y)}$ of the corresponding rule list~$\RL$.
%
These calculations are the dominant
\begin{arxiv}
computations with respect to execution time.
%
This motivates
\end{arxiv}
\begin{kdd}
computations and motivate
\end{kdd}
our use of a highly optimized library,
designed by~\citet{YangRuSe16}, for representing rule lists and
performing operations encountered in evaluating functions of rule lists.
%
Furthermore, we exploit the hierarchical nature of the objective
function and its lower bound to compute these quantities
incrementally throughout branch-and-bound execution.
%
\begin{arxiv}
In this section, we provide explicit expressions for
the incremental computations that are central to our approach.
%
Later, in~\S\ref{sec:implementation}, we describe a cache data structure
for supporting our incremental framework in practice.

For completeness, before presenting our incremental expressions,
let us begin by writing down the objective lower bound and objective
of the empty rule list, ${\RL = ((), (), \Default, 0)}$,
the first rule list evaluated in Algorithm~\ref{alg:branch-and-bound}.
%
Since its prefix contains zero rules, it has zero prefix
misclassification error and also has length zero.
%
Thus, the empty rule list's objective lower bound is zero, \ie ${b((), \x, \y) = 0}$.
%
Since none of the data are captured by the empty prefix, the default rule
corresponds to the majority class, and the objective corresponds to the
default rule misclassification error, \ie ${\Obj(\RL, \x, \y) = \Loss_0((), \Default, \x, \y)}$.

Now, we derive our incremental expressions for the objective function and its lower bound.
%
Let ${\RL = (\Prefix, \Labels, \Default, K)}$ and
${\RL' = (\Prefix', \Labels', \Default', K + 1)}$
be rule lists such that prefix ${\Prefix =}$ ${(p_1, \dots, p_K)}$
is the parent of ${\Prefix' = (p_1, \dots, p_K, p_{K+1})}$.
%
Let ${\Labels = (q_1, \dots, q_K)}$ and
${\Labels' = (q_1, \dots,}$ ${q_K, q_{K+1})}$ be the corresponding labels.
%
The hierarchical structure of Algorithm~\ref{alg:branch-and-bound}
enforces that if we ever evaluate~$\RL'$, then we will have already
evaluated both the objective and objective lower bound of its parent,~$\RL$.
%
We would like to reuse as much of these computations as possible
in our evaluation of~$\RL'$.
%
We can write the objective lower bound of~$\RL'$ incrementally,
with respect to the objective lower bound of~$\RL$:
\begin{align}
b(\Prefix', \x, \y)
  &= \Loss_p(\Prefix', \Labels', \x, \y) + \Reg (K + 1) \nn \\
&= \frac{1}{N} \sum_{n=1}^N \sum_{k=1}^{K+1} \Cap(x_n, p_k \given \Prefix')
  \wedge \one [ q_k \neq y_n ] + \Reg (K+1) \label{eq:non-inc-lb} \\
&= \Loss_p(\Prefix, \Labels, \x, \y) + \Reg K + \Reg
  + \frac{1}{N} \sum_{n=1}^N \Cap(x_n, p_{K+1} \given \Prefix') \wedge \one [q_{K+1} \neq y_n ] \nn \\
&= b(\Prefix, \x, \y) + \Reg
  + \frac{1}{N} \sum_{n=1}^N \Cap(x_n, p_{K+1} \given \Prefix') \wedge \one [q_{K+1} \neq y_n ] \nn \\
&= b(\Prefix, \x, \y) + \Reg  + \frac{1}{N} \sum_{n=1}^N \neg\, \Cap(x_n, \Prefix) \wedge
  \Cap(x_n, p_{K+1}) \wedge \one [q_{K+1} \neq y_n].
\label{eq:inc-lb}
\end{align}
Thus, if we store $b(\Prefix, \x, \y)$, % and ${\neg\, \Cap(\x, \Prefix)}$,
then we can reuse this quantity when computing $b(\Prefix', \x, \y)$.
%
Transforming~\eqref{eq:non-inc-lb} into~\eqref{eq:inc-lb} yields a
significantly simpler expression that is a function of the stored
quantity~$b(\Prefix, \x, \y)$.
%
For the objective of~$\RL'$, first let us write a na\"ive expression:
\begin{align}
&\Obj(\RL', \x, \y) = \Loss(\RL', \x, \y) + \Reg (K + 1)
= \Loss_p(\Prefix', \Labels', \x, \y) + \Loss_0(\Prefix', \Default', \x, \y) + \Reg(K + 1) \nn \\
&= \frac{1}{N} \sum_{n=1}^N \sum_{k=1}^{K+1} \Cap(x_n, p_k \given \Prefix')
  \wedge \one [ q_k \neq y_n ] + \frac{1}{N}\sum_{n=1}^N \neg\, \Cap(x_n, \Prefix') \wedge
  \one [q'_0 \neq y_n] + \Reg (K+1). \label{eq:non-inc-obj}
\end{align}
Instead, we can compute the objective of~$\RL'$ incrementally
with respect to its objective lower bound:
\begin{align}
\Obj(\RL', \x, \y) &=  \Loss_p(\Prefix', \Labels', \x, \y) +
  \Loss_0(\Prefix', \Default', \x, \y) + \Reg (K + 1) \nn \\
&= b(\Prefix', \x, \y) + \Loss_0(\Prefix', \Default', \x, \y) \nn \\
&= b(\Prefix', \x, \y) + \frac{1}{N}\sum_{n=1}^N \neg\, \Cap(x_n, \Prefix') \wedge
  \one [q'_0 \neq y_n] \nn \\
&= b(\Prefix', \x, \y) + \frac{1}{N}\sum_{n=1}^N \neg\, \Cap(x_n, \Prefix) \wedge
  (\neg\, \Cap(x_n, p_{K+1})) \wedge \one [q'_0 \neq y_n].
\label{eq:inc-obj}
\end{align}
The expression in~\eqref{eq:inc-obj} is simpler to compute than that
in~\eqref{eq:non-inc-obj}, because the former reuses $b(\Prefix', \x, \y)$,
which we already computed in~\eqref{eq:inc-lb}.
%
Note that instead of computing~$\Obj(\RL', \x, \y)$ incrementally from $b(\Prefix', \x, \y)$
as in~\eqref{eq:inc-obj}, we could have computed it incrementally from $\Obj(\RL, \x, \y)$.
However, doing so would in practice require that we store~$\Obj(\RL, \x, \y)$
in addition to~$b(\Prefix, \x, \y)$, which we already must store to support~\eqref{eq:inc-lb}.
We prefer the incremental approach suggested by~\eqref{eq:inc-obj}
since it avoids this additional storage overhead.

\begin{algorithm}[t!]
  \caption{Incremental branch-and-bound for learning rule lists, for simplicity, from a cold start.
  We explicitly show the incremental objective lower bound and objective functions in Algorithms~\ref{alg:incremental-lb} and~\ref{alg:incremental-obj}, respectively.}
\label{alg:incremental}
\begin{algorithmic}
\normalsize
\State \textbf{Input:} Objective function~$\Obj(\RL, \x, \y)$,
objective lower bound~${b(\Prefix, \x, \y)}$,
set of antecedents ${\RuleSet = \{s_m\}_{m=1}^M}$,
training data~$(\x, \y) = {\{(x_n, y_n)\}_{n=1}^N}$,
regularization parameter~$\Reg$
\State \textbf{Output:} Provably optimal rule list~$\OptimalRL$ with minimum objective~$\OptimalObj$ \\

\State $\CurrentRL \gets ((), (), \Default, 0)$ \Comment{Initialize current best rule list with empty rule list}
\State $\CurrentObj \gets \Obj(\CurrentRL, \x, \y)$ \Comment{Initialize current best objective}
\State $Q \gets $ queue$(\,[\,(\,)\,]\,)$ \Comment{Initialize queue with empty prefix}
\State $C \gets $ cache$(\,[\,(\,(\,)\,, 0\,)\,]\,)$ \Comment{Initialize cache with empty prefix and its objective lower bound}
\While {$Q$ not empty} \Comment{Optimization complete when the queue is empty}
	\State $\Prefix \gets Q$.pop(\,) \Comment{Remove a length-$K$ prefix~$\Prefix$ from the queue}
        \State $b(\Prefix, \x, \y) \gets C$.find$(\Prefix)$ \Comment{Look up $\Prefix$'s lower bound in the cache}
        \State $\mathbf{u} \gets \neg\,\Cap(\x, \Prefix)$ \Comment{Bit vector indicating data not captured by $\Prefix$}
        \For {$s$ in $\RuleSet$} \Comment{Evaluate all of $\Prefix$'s children}
            \If {$s$ not in $\Prefix$}
                \State $\PrefixB \gets (\Prefix, s)$ \Comment{\textbf{Branch}: Generate child $\PrefixB$}
                \State $\mathbf{v} \gets \mathbf{u} \wedge \Cap(\x, s)$ \Comment{Bit vector indicating data captured by $s$ in $\PrefixB$}
                \State $b(\PrefixB, \x, \y) \gets b(\Prefix, \x, \y) + \Reg~ + $ \Call{IncrementalLowerBound}{$\mathbf{v}, \y, N$} %\Comment{Eq.~\eqref{eq:inc-lb}}
                \If {$b(\PrefixB, \x, \y) < \CurrentObj$} \Comment{\textbf{Bound}: Apply bound from Theorem~\ref{thm:bound}}
                    \State $\Obj(\RLB, \x, \y) \gets b(\PrefixB, \x, \y)~ + $ \Call{IncrementalObjective}{$\mathbf{u}, \mathbf{v}, \y, N$} %\Comment{Eq.~\eqref{eq:inc-obj}}
                    \State $\RLB \gets (\PrefixB, \LabelsB, \DefaultB, K+1)$ \Comment{$\LabelsB, \DefaultB$ are set in the incremental functions}
                    \If {$\Obj(\RLB, \x, \y) < \CurrentObj$}
                        \State $(\CurrentRL, \CurrentObj) \gets (\RLB, \Obj(\RLB, \x, \y))$ \Comment{Update current best rule list and objective}
                    \EndIf
                    \State $Q$.push$(\PrefixB)$ \Comment{Add $\PrefixB$ to the queue}
                    \State $C$.insert$(\PrefixB, b(\PrefixB, \x, \y))$ \Comment{Add $\PrefixB$ and its lower bound to the cache}
                \EndIf
            \EndIf
        \EndFor
\EndWhile
\State $(\OptimalRL, \OptimalObj) \gets (\CurrentRL, \CurrentObj)$ \Comment{Identify provably optimal rule list and objective}
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[t!]
  \caption{Incremental objective lower bound~\eqref{eq:inc-lb} used in Algorithm~\ref{alg:incremental}.}
\label{alg:incremental-lb}
\begin{algorithmic}
\normalsize
\State \textbf{Input:}
Bit vector~${\mathbf{v} \in \{0, 1\}^N}$ indicating data captured by $s$, the last antecedent in~$\PrefixB$,
bit vector of class labels~${\y \in \{0, 1\}^N}$,
number of observations~$N$
\State \textbf{Output:} Component of~$\RLB$'s misclassification error due to data captured by~$s$ \\

\Function{IncrementalLowerBound}{$\mathbf{v}, \y, N$}
    \State $n_v = \Count(\mathbf{v})$ \Comment{Number of data captured by $s$, the last antecedent in $\PrefixB$}
    \State $\mathbf{w} \gets \mathbf{v} \wedge \y$ \Comment{Bit vector indicating data captured by $s$ with label $1$}
    \State $n_w = \Count(\mathbf{w})$ \Comment{Number of data captured by $s$ with label $1$}
    \If {$n_w / n_v > 0.5$}
        \State \Return $(n_v - n_w) / N$ \Comment{Misclassification error of the rule $s \rightarrow 1$}
    \Else
        \State \Return $n_w / N$ \Comment{Misclassification error of the rule $s \rightarrow 0$}
    \EndIf
    \EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[t!]
  \caption{Incremental objective function~\eqref{eq:inc-obj} used in Algorithm~\ref{alg:incremental}.}
\label{alg:incremental-obj}
\begin{algorithmic}
\normalsize
\State \textbf{Input:}
Bit vector~${\mathbf{u} \in \{0, 1\}^N}$ indicating data not captured by~$\PrefixB$'s parent prefix,
bit vector~${\mathbf{v} \in \{0, 1\}^N}$ indicating data not captured by $s$, the last antecedent in~$\PrefixB$,
bit vector of class labels~${\y \in \{0, 1\}^N}$,
number of observations~$N$
\State \textbf{Output:} Component of~$\RLB$'s misclassification error due to its default rule \\

 \Function{IncrementalObjective}{$\mathbf{u}, \mathbf{v}, \y, N$}
    \State $\mathbf{f} \gets \mathbf{u} \wedge \neg\,\mathbf{v} $ \Comment{Bit vector indicating data not captured by $\PrefixB$}
    \State $n_f = \Count(\mathbf{f})$ \Comment{Number of data not captured by $\PrefixB$}
    \State $\mathbf{g} \gets \mathbf{f} \wedge \y$ \Comment{Bit vector indicating data not captured by $\PrefixB$ with label $1$}
    \State $n_g = \Count(\mathbf{g})$ \Comment{Number of data not captued by $\PrefixB$ with label $1$}
    \If {$n_g / n_f > 0.5$}
        \State \Return $(n_f - n_g) / N$ \Comment{Misclassification error of the default label prediction $1$}
    \Else
        \State \Return $n_g / N$ \Comment{Misclassification error of the default label prediction $0$}
    \EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

We present an incremental branch-and-bound procedure in
Algorithm~\ref{alg:incremental}, and show the incremental computations
of the objective lower bound~\eqref{eq:inc-lb} and objective~\eqref{eq:inc-obj}
as two separate functions in Algorithms~\ref{alg:incremental-lb}
and~\ref{alg:incremental-obj}, respectively.
%
In Algorithm~\ref{alg:incremental}, we use a cache to store
prefixes and their objective lower bounds.
%
Algorithm~\ref{alg:incremental} additionally reorganizes the structure
of Algorithm~\ref{alg:branch-and-bound} to group together the computations
associated with all children of a particular prefix.
%
This has two advantages.
%
The first is to consolidate cache queries: all children of the same
parent prefix compute their objective lower bounds with respect to
the parent's stored value, and we only require one cache `find' operation
for the entire group of children, instead of a separate query for each child.
%
The second is to shrink the queue's size:
instead of adding all of a prefix's children as separate queue elements,
we represent the entire group of children in the queue by a single element.
%
Since the number of children associated with each prefix
is close to the total number of possible antecedents,
both of these effects can yield significant savings.
%
For example, if we are trying to optimize over rule lists formed
from a set of 1000 antecedents, then the maximum queue size in
Algorithm~\ref{alg:incremental} will be smaller than that in
Algorithm~\ref{alg:branch-and-bound} by a factor of nearly 1000.

\end{arxiv}

\clearpage
\section{Implementation}
\label{sec:implementation}

We implement our algorithm using a collection of optimized data structures
that we describe in this section.
%
First, we explain how we use a prefix tree~(\S\ref{sec:trie})
to support the incremental computations that we motivated in~\S\ref{sec:incremental}.
%
Second, we describe several queue designs
that implement different search policies~(\S\ref{sec:queue}).
%
Third, we introduce a symmetry-aware map~(\S\ref{sec:pmap}) to support
symmetry-aware pruning~(Corollary~\ref{thm:permutation},~\S\ref{sec:permutation}).
%
Next, we summarize how these data structures interact throughout
our model of incremental execution~(\S\ref{sec:execution}).
%
In particular, Algorithms~\ref{alg:bounds} and~\ref{alg:pmap} illustrate many
of the computational details from CORELS' inner loop, highlighting each of
the bounds from~\S\ref{sec:framework} that we use to prune the search space.
%
We additionally describe how we garbage collect our data structures~(\S\ref{sec:gc}).
%
Finally, we explore how our queue can be used to support
custom scheduling policies designed to improve performance~(\S\ref{sec:scheduling}).

\subsection{Prefix Tree}
\label{sec:trie}

Our incremental computations (\S\ref{sec:incremental}) require a
cache to keep track of prefixes that we have already evaluated
and that are also still under consideration by the algorithm.
%
We implement this cache as a prefix tree, a data structure also known as a trie,
which allows us to efficiently represent structure shared between related prefixes.
%
Each node in the prefix tree encodes an individual rule ${r_k = p_k \rightarrow q_k}$.
%
Each path starting from the root represents a prefix, such that the final node
in the path also contains metadata associated with that prefix.
%
For a %rule list ${\RL = (\Prefix, \Labels, \Default, K)}$, with
prefix ${\Prefix = (p_1, \dots, p_K)}$,
let~$\varphi(\Prefix)$ denote the corresponding node in the trie.
%
The metadata at node~$\varphi(\Prefix)$ supports the incremental computation
and includes:
\begin{itemize}
\item An index encoding~$p_K$, the last antecedent.
\item The objective lower bound $b(\Prefix, \x, \y)$, defined in~\eqref{eq:lower-bound},
  the central bound in our framework (Theorem~\ref{thm:bound}).
\item The lower bound on the default rule misclassification error
  $b_0(\Prefix, \x, \y)$, defined in~\eqref{eq:lb-b0},
  to support our equivalent points bound (Theorem~\ref{thm:identical}).
\item An indicator denoting whether this node should be deleted (see~\S\ref{sec:gc}).
\item A representation of viable extensions of~$\Prefix$,
  \ie length ${K+1}$ prefix that start with~$\Prefix$ and have not been pruned.
\end{itemize}
For evaluation purposes and convenience, we store additional information in
the prefix tree; for a prefix~$\Prefix$ with corresponding rule list
${\RL = (\Prefix, \Labels, \Default, K)}$, the node~$\varphi(\Prefix)$ also stores:
\begin{itemize}
\item The length~$K$; equivalently, node~$\varphi(\Prefix)$'s depth in the trie. 
\item The label prediction~$q_K$ corresponding to antecedent~$p_K$.
\item The default rule label prediction~$\Default$.
\item $\NCap$, the number of samples captured by prefix $\Prefix$, as in~\eqref{eq:num-cap}.
\item The objective value $\Obj(\RL, \x, \y)$, defined in~\eqref{eq:objective}.
\end{itemize}
%
Finally, we note that we implement the prefix tree as a custom C++ class.

\subsection{Queue}
\label{sec:queue}

The queue is a worklist that orders exploration over the search space of possible
rule lists; every queue element corresponds to a leaf in the prefix tree, and vice versa.
%
In our implementation, each queue element points to a leaf;
when we pop an element off the queue, we use the leaf's metadata to
incrementally evaluate the corresponding prefix's children.

We order entries in the queue to implement several different search policies.
%
For example, a first-in-first-out~(FIFO) queue implements breadth-first search~(BFS),
and a priority queue implements best-first search.
%
In our experiments~(\S\ref{sec:experiments}), we use the C++ Standard Template Library~(STL)
queue and priority queue to implement BFS and best-first search, respectively.
%
For CORELS, priority queue policies of interest include ordering by the lower bound,
the objective, or more generally, any function that maps prefixes to real values;
stably ordering by prefix length and inverse prefix length implement
BFS and depth-first search (DFS), respectively.
%
In our released code, we present a unified implementation,
where we use the STL priority queue to support BFS, DFS,
and several best-first search policies.
%
As we demonstrate in our experiments~(\S\ref{sec:ablation}),
we find that using a custom search strategy,
such as ordering by the lower bound, usually leads to a faster runtime than BFS.

We motivate the design of additional custom search strategies in~\S\ref{sec:scheduling}.
%
In preliminary work (not shown), we also experimented with
stochastic exploration processes that bypass the need for a queue
by instead following random paths from the root to leaves;
developing such methods could be an interesting direction for future work.
%
We note that these search policies are referred to as node selection strategies
in the MIP literature.
%
Strategies such as best-first (best-bound) search and DFS are known as static methods, and the framework we present in~\S\ref{sec:scheduling}
has the spirit of estimate-based methods~\citep{Linderoth1999}.

\subsection{Symmetry-aware Map}
\label{sec:pmap}

The symmetry-aware map supports the symmetry-aware pruning justified in~\S\ref{sec:equivalent}.
%
In our implementation, we specifically leverage our permutation bound
(Corollary~\ref{thm:permutation}), though it is also possible to directly
exploit the more general equivalent support bound (Theorem~\ref{thm:equivalent}).
%
We use the C++ STL unordered map to keep track of the best known ordering
of each evaluated set of antecedents.
%
The keys of our symmetry-aware map encode antecedents in canonical order,
\ie antecedent indices in numerically sorted order,
and we associate all permutations of a set of antecedents with a single key.
%
Each key maps to a value that encodes the best known prefix in the permutation
group of the key's antecedents, as well as the objective lower bound of that prefix.

Before we consider adding a prefix~$\Prefix$ to the trie and queue, we check
whether the map already contains a permutation~$\pi(\Prefix)$ of that prefix.
%
If no such permutation exists, then we insert~$\Prefix$ into the map, trie, and queue.
%
Otherwise, if a permutation~$\pi(\Prefix)$ exists and the lower bound of~$\Prefix$ is better
than that of~$\pi(\Prefix)$, \ie ${b(\Prefix, \x, \y) <}$ ${b(\pi(\Prefix), \x, \y)}$,
then we update the map and remove~$\pi(\Prefix)$ and its entire subtree from the trie;
we also insert~$\Prefix$ into the trie and queue.
%
Otherwise, if there exists a permutation~$\pi(\Prefix)$ such that
${b(\pi(\Prefix), \x, \y) \le}$ ${b(\Prefix, \x, \y)}$,
then we do nothing, \ie we do not insert~$\Prefix$ into any data structures.

\subsection{Incremental Execution}
\label{sec:execution}

Mapping our algorithm to our data structures produces the following execution strategy,
which we also illustrate in Algorithms~\ref{alg:bounds} and~\ref{alg:pmap}.
%
We initialize the current best objective~$\CurrentObj$ and rule list~$\CurrentRL$.
%
While the trie contains unexplored leaves, a scheduling policy selects the next prefix~$\Prefix$
to extend; in our implementation, we pop elements from a (priority) queue, until the queue is empty.
%
Then, for every antecedent~$s$ that is not in~$\Prefix$,
we construct a new prefix~$\PrefixB$ by appending~$s$ to~$\Prefix$;
we incrementally calculate the lower bound~$b(\PrefixB, \x, \y)$,
the objective~$\Obj(\RLB, \x, \y)$, of the associated rule list~$\RLB$,
and other quantities used by our algorithm, summarized by the metadata fields of
the (potential) prefix tree node~$\varphi(\PrefixB)$.

If the objective~$\Obj(\RLB, \x, \y)$ is less than the current best objective~$\CurrentObj$,
then we update~$\CurrentObj$ and~$\CurrentRL$.
%
If the lower bound of the new prefix~$\PrefixB$ is less than the current best objective,
%\ie ${b(\Prefix', \x, \y) < \CurrentObj}$,
then as described in~\S\ref{sec:pmap}, we query the symmetry-aware map for~$\PrefixB$;
if we insert~$\Prefix'$ into the symmetry-aware map, then we also insert it into the trie and queue.
%
Otherwise, %\ie ${b(\Prefix', \x, \y) \ge \CurrentObj}$
then by our hierarchical lower bound (Theorem~\ref{thm:bound}),
no extension of~$\PrefixB$ could possibly lead to a rule list with objective
better than~$\CurrentObj$, thus we do not insert~$\PrefixB$ into the tree or queue.
%
We also leverage our other bounds from~\S\ref{sec:framework}
to aggressively prune the search space; we highlight each of these bounds
in Algorithms~\ref{alg:bounds} and~\ref{alg:pmap},
which summarize the computations and data structure operations performed in CORELS' inner loop.
%
When there are no more leaves to explore, \ie the queue is empty, we output the optimal rule list.
%
We can optionally terminate early according to some alternate condition,
\eg when the size of the prefix tree exceeds some threshold.

\begin{algorithm}[t!]
  \caption{The inner loop of CORELS, which evaluates all children of a prefix~$\Prefix$.}
%  For details about symmetry-aware map queries, see Algorithm~\ref{alg:pmap}.}
\label{alg:bounds}
\begin{algorithmic}
\small
\State Define $\mathbf{z} \in \{0, 1\}^N$, s.t. ${z_n = \sum_{u=1}^U \one [x_n \in e_u] [y_n = q_u]}$ \\
\Comment{$e_u$
is the equivalent points set containing~$x_n$ and $q_u$ is the minority class label of~$e_u$ (\S\ref{sec:identical})}
\State Define $b(\Prefix, \x, \y)$ and $\mathbf{u} = \neg\,\Cap(\x, \Prefix)$ \Comment{$\mathbf{u}$ is a bit vector indicating data not captured by $\Prefix$}
\vspace{1.5mm}
\For {$s$ in $\RuleSet$ \textbf{if} $s$ not in $\Prefix$ \textbf{then}} \Comment{Evaluate all of $\Prefix$'s children}
%    \If {$s$ not in $\Prefix$}
        \State $\PrefixB \gets (\Prefix, s)$ \Comment{\textbf{Branch}: Generate child $\PrefixB$}
        \State $\mathbf{v} \gets \mathbf{u} \wedge \Cap(\x, s)$ \Comment{Bit vector indicating data captured by $s$ in $\PrefixB$}
        \State $n_v = \Count(\mathbf{v})$ \Comment{Number of data captured by $s$, the last antecedent in $\PrefixB$}
        \If {$n_v / N < \Reg$}
            \State \Continue \Comment{\textbf{Lower bound on antecedent support (Theorem\ref{thm:min-capture})}}
        \EndIf
        \State $\mathbf{w} \gets \mathbf{v} \wedge \y$ \Comment{Bit vector indicating data captured by $s$ with label $1$}
        \State $n_w = \Count(\mathbf{w})$ \Comment{Number of data captured by $s$ with label $1$}
        \If {$n_w / n_v \ge 0.5$}
            \State $n_c \gets n_w$ \Comment{Number of correct predictions by the new rule $s \rightarrow 1$}
        \Else
            \State $n_c \gets n_v - n_w$ \Comment{Number of correct predictions by the new rule $s \rightarrow 0$}
        \EndIf
        \If {$n_c / N < \Reg$}
            \State \Continue \Comment{\textbf{Lower bound on accurate antecedent support (Theorem~\ref{thm:min-capture-correct})}}
        \EndIf
        \State $\delta_b \gets (n_v - n_c) / N$ \Comment{Misclassification error of the new rule}
       \State $b(\PrefixB, \x, \y) \gets b(\Prefix, \x, \y) + \Reg + \delta_b$ \Comment{Incremental lower bound~\eqref{eq:inc-lb}}
       \If {$b(\PrefixB, \x, \y) \ge \CurrentObj$} \Comment{\textbf{Hierarchical objective lower bound (Theorem~\ref{thm:bound})}}
           \State \Continue
       \EndIf
       \State $\mathbf{f} \gets \mathbf{u} \wedge \neg\,\mathbf{v} $ \Comment{Bit vector indicating data not captured by $\PrefixB$}
       \State $n_f = \Count(\mathbf{f})$ \Comment{Number of data not captured by $\PrefixB$}
       \State $\mathbf{g} \gets \mathbf{f} \wedge \y$ \Comment{Bit vector indicating data not captured by $\PrefixB$ with label $1$}
       \State $n_g = \Count(\mathbf{g})$ \Comment{Number of data not captued by $\PrefixB$ with label $1$}
       \If {$n_g / n_f \ge 0.5$}
           \State $\delta_\Obj \gets (n_f - n_g) / N$ \Comment{Misclassification error of the default label prediction $1$}
       \Else
           \State $\delta_\Obj \gets n_g / N$ \Comment{Misclassification error of the default label prediction $0$}
       \EndIf
       \State $\Obj(\RLB, \x, \y) \gets b(\PrefixB, \x, \y) + \delta_\Obj$ \Comment{Incremental objective~\eqref{eq:inc-obj}}
       %\State $\Obj(\RLB, \x, \y) \gets b(\PrefixB, \x, \y)~ + $ \Call{IncrementalObjective}{$\mathbf{u}, \mathbf{v}, \y, N$}  \Comment{Inc. objective~\eqref{eq:inc-lb}}
       \State $\RLB \gets (\PrefixB, \LabelsB, \DefaultB, K+1)$ \Comment{$\LabelsB, \DefaultB$ are set in the incremental functions}
       \If {$\Obj(\RLB, \x, \y) < \CurrentObj$}
            \State $(\CurrentRL, \CurrentObj) \gets (\RLB, \Obj(\RLB, \x, \y))$ \Comment{Update current best rule list and objective}
            \State \Call{GarbageCollectPrefixTree}{$\CurrentObj$} \Comment{Delete nodes with lower bound $< \CurrentObj$ (\S\ref{sec:gc})}
        \EndIf
        \State $b_0(\PrefixB, \x, \y) \gets \Count(\mathbf{f} \wedge \mathbf{z}) / N$ \Comment{Lower bound on the default rule misclassification}
        \State $b \gets b(\PrefixB, \x, \y) + b_0(\PrefixB, \x, \y)$ \hfill error defined in~\eqref{eq:lb-b0}
        \If {$b + \Reg \ge \CurrentObj$} \Comment{\textbf{Equivalent points bound (Theorem~\ref{thm:identical})}}
            \State \Continue \hfill {combined with the \textbf{Lookahead bound (Lemma~\ref{lemma:lookahead})}}
        \EndIf
        \State \Call{CheckMapAndInsert}{$\PrefixB, b$} \Comment{Check the \textbf{Permutation bound (Corollary~\ref{thm:permutation})} and}
\EndFor \hfill {possibly insert $\PrefixB$ into data structures (Algorithm~\ref{alg:pmap})}
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[t!]
  \caption{Possibly insert a prefix into CORELS' data structures, after first
  checking the symmetry-aware map, which supports search space pruning
  triggered by the permutation bound (Corollary~\ref{thm:permutation}).
  For further context, see Algorithm~\ref{alg:bounds}.}
\label{alg:pmap}
\begin{algorithmic}
\State $T$ is the prefix tree (\S\ref{sec:trie})
\State $Q$ is the queue, for concreteness, a priority queue ordered by the lower bound (\S\ref{sec:queue})
\State $\PMap$ is the symmetry-aware map (\S\ref{sec:pmap}) \\

\Function{CheckMapAndInsert}{$\PrefixB, b$}
    \State $\pi_0 \gets$ sort($\PrefixB$) \Comment{$\PrefixB$'s antecedents in canonical order}
    \State $(D_\pi, b_\pi) \gets \PMap$.find($\pi_0$) \Comment{Look for a permutation of $\PrefixB$}
    \If {$D_\pi$ exists}
        \If {$b < b_\pi$} \Comment{$\PrefixB$ is better than $D_\pi$}
            \State $\PMap$.update($\pi_0, (\PrefixB, b)$) \Comment{Replace $D_\pi$ with $\PrefixB$ in the map}
            \State $T$.delete\_subtree($D_\pi$) \Comment{Delete $D_\pi$ and its subtree from the prefix tree}
            \State $T$.insert$(\varphi(\PrefixB))$ \Comment{Add node for $\PrefixB$ to the prefix tree}
            \State $Q$.push$(\PrefixB, b)$ \Comment{Add $\PrefixB$ to the queue}
        \Else
            \State \textbf{pass} \Comment{$\PrefixB$ is inferior to $D_\pi$, thus do not insert it into any data structures}
        \EndIf
    \Else
        \State $\PMap$.insert($\pi_0, (\PrefixB, b)$) \Comment{Add $\PrefixB$ to the map}
        \State $T$.insert$(\varphi(\PrefixB))$ \Comment{Add node for $\PrefixB$ to the prefix tree}
        \State $Q$.push$(\PrefixB, b)$ \Comment{Add $\PrefixB$ to the queue}
    \EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{Garbage Collection}
\label{sec:gc}

During execution, we garbage collect the trie.
%
Each time we update the minimum objective,
we traverse the trie in a depth-first manner, deleting all subtrees
of any node with lower bound larger than the current minimum objective.
%
At other times, when we encounter a node with no children, we prune upwards,
deleting that node and recursively traversing the tree towards the root,
deleting any childless nodes.
%
This garbage collection allows us to constrain the trie's memory consumption, though in our
experiments we observe the minimum objective to decrease only a small number of times.

In our implementation, we cannot immediately delete prefix tree leaves
because each corresponds to a queue element that points to it.
%
The C++ STL priority queue is a wrapper container that prevents access to the
underlying data structure, and thus we cannot access elements in the middle of the queue,
even if we know the relevant identifying information.
%
We therefore have no way to update the queue without iterating over every element.
%
We address this by marking prefix tree leaves that we wish to delete (see~\S\ref{sec:trie}),
deleting the physical nodes lazily, after they are popped from the queue.
%
Later, in our section on experiments~(\S\ref{sec:experiments}),
we refer to two different queues that we define here: the physical queue
corresponds to the C++ queue, and thus all prefix tree leaves, and the logical queue
corresponds only to those prefix tree leaves that have not been marked for deletion.

\subsection{Custom Scheduling Policies}
\label{sec:scheduling}

In our setting, an ideal scheduling policy would immediately identify an optimal
rule list, and then certify its optimality by systematically eliminating the
remaining search space.
%
This motivates trying to design scheduling policies that tend to quickly find optimal rule lists.
%
When we use a priority queue to order the set of prefixes to evaluate next,
we are free to implement different scheduling policies via the ordering of
elements in the queue.
%
This motivates designing functions that assign higher priorities to `better'
prefixes that we believe are more likely to lead to optimal rule lists.
%
We follow the convention that priority queue elements are ordered
by keys, such that keys with smaller values have higher priorities.

We introduce a custom class of functions that we call \emph{curiosity} functions.
%
Broadly, we think of the curiosity of a rule list~$\RL$
as the expected objective value of another rule list~$\RL'$ that is related to~$\RL$;
different models of the relationship between~$\RL$ and~$\RL'$ lead to different
curiosity functions.
%
In general, the curiosity of~$\RL$ is, by definition, equal to the sum of the expected
misclassification error and the expected regularization penalty of~$\RL'$:
\begin{align}
\Curiosity(\Prefix, \x, \y) \equiv \E[ \Obj(\RL', \x, \y) ]
&= \E[\Loss(\Prefix', \Labels', \x, \y)] + \Reg \E[ K' ].
\label{eq:curiosity}
\end{align}

Next, we describe a simple curiosity function for a rule list~$\RL$ with prefix~$\Prefix$.
%
First, let~$\NCap$ denote the number of observations captured by~$\Prefix$, \ie
\begin{align}
\NCap \equiv \sum_{n=1}^N \Cap(x_n, \Prefix).
\label{eq:num-cap}
\end{align}
We now describe a model that generates another
rule list~${\RL' = (\Prefix', \Labels', \Default', K')}$ from~$\Prefix$.
%
Assume that prefix~$\Prefix'$ starts with~$\Prefix$ and captures all the data,
such that each additional antecedent in~$\Prefix'$
captures as many `new' observations as each antecedent in~$\Prefix$, on average;
then, the expected length of~$\Prefix'$ is
\begin{align}
\E[ K' ] = \frac{N}{\NCap / K}.
\label{eq:curiosity-length}
\end{align}
Furthermore, assume that each additional antecedent in~$\Prefix'$
makes as many mistakes as each antecedent in~$\Prefix$, on average,
thus the expected misclassification error of~$\Prefix'$ is
\begin{align}
\E[\Loss(\Prefix', \Labels', \x, \y)]
&= \E[\Loss_p(\Prefix', \Labels', \x, \y)] + \E[\Loss_0(\Prefix', \Default', \x, \y)] \nn \\
&= \E[\Loss_p(\Prefix', \Labels', \x, \y)]
=  \E[ K' ] \left(\frac{\Loss_p(\Prefix, \Labels, \x, \y)}{K}\right).
\label{eq:curiosity-error}
\end{align}
Note that the default rule misclassification error~$\Loss_0(\Prefix', \Default', \x, \y)$
is zero because we assume that~$\Prefix'$ captures all the data.
%
Inserting~\eqref{eq:curiosity-length} and~\eqref{eq:curiosity-error}
into~\eqref{eq:curiosity} thus gives curiosity for this model:
\begin{align*}
\Curiosity(\Prefix, \x, \y)
%= \E[\Loss_p(\Prefix', \Labels', \x, \y)] + \Reg \E[ K' ]
%&= \E[ K' ] \left(\frac{\Loss_p(\Prefix, \Labels, \x, \y)}{K}\right) + \Reg \E[ K' ] \\
%&=  \left(\frac{N}{\NCap / K}\right)
%  \left(\frac{\Loss_p(\Prefix, \Labels, \x, \y)}{K}\right)
%  + \Reg \left(\frac{N}{\NCap / K}\right) \nn \\
&= \left( \frac{N}{\NCap} \right) \biggl(\Loss_p(\Prefix, \Labels, \x, \y) + \Reg K \biggr) \\
&= \left( \frac{1}{N} \sum_{n=1}^N \Cap(x_n, \Prefix) \right)^{-1} b(\Prefix, \x, \y)
= \frac{b(\Prefix, \x, \y)}{\Supp(\Prefix, \x)},
\end{align*}
where for the second equality, we used the definitions in~\eqref{eq:num-cap} of~$\NCap$
and in~\eqref{eq:lower-bound} of~$\Prefix$'s lower bound, and for the last equality,
we used the definition in~\eqref{eq:support} of~$\Prefix$'s normalized support.

The curiosity for a prefix~$\Prefix$ is thus also equal to its objective lower bound,
scaled by the inverse of its normalized support.
%
For two prefixes with the same lower bound, curiosity gives higher priority to
the one that captures more data.
%
This is a well-motivated scheduling strategy if we model prefixes that extend
the prefix with smaller support as having more `potential' to make mistakes.
%
We note that using curiosity in practice does not introduce new bit vector
or other expensive computations; during execution, we can calculate curiosity
as a simple function of already derived quantities.

In preliminary experiments, we observe that using a priority queue ordered by
curiosity sometimes yields a dramatic reduction in execution time,
compared to using a priority queue ordered by the objective lower bound.
%
Thus far, we have observed significant benefits on specific small problems,
where the structure of the solutions happen to render curiosity particularly
effective (not shown).
%
Designing and studying other `curious' functions, that are effective in more
general settings, is an exciting direction for future work.

\input{sections/experiments-long}

\input{sections/conclusion}

% Acknowledgements should go at the end, before appendices and references
\acks{E.A. conducted most of this work while supported by the Miller Institute for Basic Research
in Science, University of California, Berkeley, and hosted by Prof. M.I. Jordan at RISELab.
%
C.D.R. is supported in part by MIT-Lincoln Labs and the National Science Foundation under IIS-1053407.
%
E.A. would like to thank E.~Jonas, E.~Kohler, and S.~Tu for early implementation
guidance, A.~D'Amour for pointing out the work by~\citet{Goel16}, V.~Kanade, S.~McCurdy,
J.~Schleier-Smith and E.~Thewalt for helpful conversations, and members of RISELab,
SAIL, and the UC Berkeley Database Group for their support and feedback.
%
We thank H.~Yang and B.~Letham for sharing advice and code for processing data
and mining rules, B.~Coker for his critical advice on using the ProPublica COMPAS data set,
as well as V.~Kaxiras and A.~Saligrama for their recent contributions to our implementation
and for creating the CORELS website. We are very grateful to our editor and anonymous reviewers.
}

\vskip 0.2in

\renewcommand{\theHsection}{A\arabic{section}}
\appendix
%\renewcommand{\thesection}{\Alph{section}}
\input{sections/appendix-bounds}
\input{sections/appendix}

\bibliography{refs}

\end{document}
