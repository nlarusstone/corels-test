\documentclass[twoside,11pt]{article}
\usepackage{jmlr2e}            % <- uncomment for JMLR submission
%\usepackage{jmlr2e-stripped}    % <- uncomment for arXiv version

\usepackage{amsmath,amsfonts}
\usepackage{url}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}

\input{latex_front}
\input{latex_macros}

\usepackage{caption}
\captionsetup{format=hang}

\includecomment{arxiv}\excludecomment{kdd}

% See http://www.jmlr.org/format/sample.tex

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}
\jmlrheading{19}{2018}{1-78}{11/17}{5/18}{17-716}{Elaine Angelino, Nicholas Larus-Stone, Daniel Alabi, Margo Seltzer, and Cynthia Rudin}

% Short headings should be running head and authors last names
\ShortHeadings{Learning Certifiably Optimal Rule Lists}{Angelino, Larus-Stone, Alabi, Seltzer, and Rudin}
\firstpageno{1}

\begin{document}

\title{Learning Certifiably Optimal Rule Lists for Categorical Data}

\author{\name Elaine Angelino \email elaine@eecs.berkeley.edu \\
        \addr Department of Electrical Engineering and Computer Sciences\\
        University of California, Berkeley,
        Berkeley, CA 94720
        \AND
        \name Nicholas Larus-Stone \email nlarusstone@alumni.harvard.edu \\
        \name Daniel Alabi \email alabid@g.harvard.edu \\
        \name Margo Seltzer \email margo@eecs.harvard.edu \\
        \addr School of Engineering and Applied Sciences\\
        Harvard University,
        Cambridge, MA 02138
        \AND
        \name Cynthia Rudin$^*$ \email cynthia@cs.duke.edu \\
        \addr Department of Computer Science and
        Department of Electrical and Computer Engineering\\
        Duke University,
        Durham, NC 27708}

\editor{Maya Gupta\\ $^*$To whom correspondence should be addressed.}
%\footnote{}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
\input{sections/abstract}
\end{abstract}

\begin{keywords}
    rule lists, decision trees, optimization, interpretable models, criminal justice applications
\end{keywords}

\section{Introduction}

As machine learning continues to gain prominence in socially-important decision-making,
the interpretability of predictive models remains a crucial problem.
%
Our goal is to build models that are highly predictive, transparent, and easily understood by humans.
%
We use rule lists, also known as decision lists, to achieve this goal.
%
Rule lists are predictive models composed of if-then statements;
these models are interpretable because the rules provide a reason for each prediction~(Figure~\ref{fig:rule-list}).

Constructing rule lists, or more generally, decision trees, has been a challenge for more than
30 years; most approaches use greedy splitting techniques~\citep{Rivest87,Breiman84,Quinlan93}.
%
Recent approaches use Bayesian analysis, either to find a locally optimal solution~\citep{Chipman:1998jh} or to explore the search space~\citep{LethamRuMcMa15, YangRuSe16}.
%
These approaches achieve high accuracy while also managing to run reasonably quickly.
%
However, despite the apparent accuracy of the rule lists generated by these algorithms,
there is no way to determine either if the generated rule list is optimal or how close it is to optimal,
where optimality is defined with respect to minimization of a regularized loss function.

\begin{arxiv}
\begin{figure}[t!]
\begin{algorithmic}
\State \bif $(age=18-20) \band (sex=male)$ \bthen $yes$
\State \belif $(age=21-23)	 \band (priors=2-3)$ \bthen $yes$
\State \belif $(priors>3)$ \bthen $yes$
\State \belse $no$
\end{algorithmic}
\caption{An example rule list that predicts two-year recidivism
for the ProPublica data set, found by CORELS.
}
\label{fig:rule-list}
\end{figure}
\end{arxiv}

Optimality is important, because there are societal implications for a lack of optimality.
%
Consider the ProPublica article on the Correctional Offender Management Profiling for Alternative Sanctions
(COMPAS) recidivism prediction tool~\citep{LarsonMaKiAn16}.
%
It highlights a case where a black box, proprietary predictive model is being used for recidivism prediction.
%
The authors hypothesize that the COMPAS scores are racially biased,
but since the model is not transparent, no one (outside of the creators of COMPAS)
can determine the reason or extent of the bias~\citep{LarsonMaKiAn16},
nor can anyone determine the reason for any particular prediction.
%
By using COMPAS, users implicitly assumed that a transparent model
would not be sufficiently accurate for recidivism prediction,
\ie they assumed that a black box model would provide better accuracy.
%
We wondered whether there was indeed no transparent and sufficiently accurate model.
%
Answering this question requires solving a computationally hard problem.
%
Namely, we would like to both find a transparent model that is optimal
within a particular pre-determined class of models
and produce a certificate of its optimality, with respect to the regularized empirical risk.
%
This would enable one to say, for this problem and model class,
with certainty and before resorting to black box methods,
whether there exists a transparent~model.
%
While there may be differences between training and test performance,
finding the simplest model with optimal training performance is prescribed by
statistical learning theory.

To that end, we consider the class of rule lists assembled from pre-mined frequent itemsets
and search for an optimal rule list that minimizes a regularized risk function,~$R$.
%
This is a hard discrete optimization problem.
%
Brute force solutions that minimize~$R$ are computationally prohibitive
due to the exponential number of possible rule lists.
%
However, this is a worst case bound that is not realized in practical settings.
%
For realistic cases, it is possible to solve fairly large cases of this problem to optimality,
with the careful use of algorithms, data structures, and implementation techniques.

\begin{kdd}
\begin{figure}[b!]
\vspace{-3mm}
\begin{algorithmic}
\normalsize
\State \bif $(age=23-25) \band (priors=2-3)$ \bthen $yes$
\State \belif $(age=18-20)$ \bthen $yes$
\State \belif $(sex=male) \band (age=21-22)$ \bthen $yes$
\State \belif $(priors>3)$ \bthen $yes$
\State \belse $no$
\end{algorithmic}
\vspace{-3mm}
\caption{An example rule list that predicts two-year recidivism
for the ProPublica data set, found by CORELS.
}
\label{fig:rule-list}
\end{figure}
\end{kdd}

We develop specialized tools from the fields of discrete optimization and artificial intelligence.
%
Specifically, we introduce a special branch-and bound algorithm, called
Certifiably Optimal RulE ListS (CORELS), that provides the optimal solution
according to the training objective, along with a certificate of optimality.
%
The certificate of optimality means that we can investigate how close other models
(\eg models provided by greedy algorithms) are to optimal.

\begin{arxiv}
Within its branch-and-bound procedure, CORELS maintains a lower bound on the
minimum value of~$R$ that each incomplete rule list can achieve.
%
This allows CORELS to prune an incomplete rule list (and every possible extension)
if the bound is larger than the error of the best rule list that it has already evaluated.
%
The use of careful bounding techniques leads to massive pruning of
the search space of potential rule lists.
%
The algorithm continues to consider incomplete and complete rule lists until it has either
examined or eliminated every rule list from consideration.
%
Thus, CORELS terminates with the optimal rule list and a certificate of optimality.
\end{arxiv}

The efficiency of CORELS depends on how much of the search space our bounds
allow us to prune; we seek a tight lower bound on~$R$.
%
The bound we maintain throughout execution is a maximum of several bounds
that come in three categories.
%
The first category of bounds are those intrinsic to the rules themselves.
%
This category includes bounds stating that each rule must capture sufficient data;
if not, the rule list is provably non-optimal.
%
The second type of bound compares a lower bound on the value of~$R$
to that of the current best solution.
%
This allows us to exclude parts of the search space that could never be better
than our current solution.
%
Finally, our last type of bound is based on comparing incomplete rule lists that
capture the same data and allows us to pursue only the most accurate option.
%
This last class of bounds is especially important---without our use of a novel
\textit{symmetry-aware map}, we are unable to solve most problems of reasonable scale.
%
This symmetry-aware map keeps track of the best accuracy
over all observed permutations of a given incomplete rule list.

We keep track of these bounds using a modified \emph{prefix tree},
a data structure also known as a trie.
%
Each node in the prefix tree represents an individual rule;
thus, each path in the tree represents a rule list such that
the final node in the path contains metrics about that rule list.
%
This tree structure, together with a search policy and sometimes a queue,
enables a variety of strategies, including breadth-first,
best-first, and stochastic search.
%
In particular, we can design different best-first strategies
by customizing how we order elements in a priority queue.
%
In addition, we are able to limit the number of nodes in the trie
and thereby enable tuning of space-time tradeoffs in a robust manner.
%
This trie structure is a useful way of organizing the generation
and evaluation of rule lists.

\begin{arxiv}
We evaluated CORELS on a number of publicly available data sets.
%
Our metric of success was 10-fold cross-validated prediction accuracy on a subset of the data.
%
These data sets involve hundreds of rules and thousands of observations.
%
CORELS is generally able to find an optimal rule list in a matter of seconds
and certify its optimality within about 10 minutes.
%
We show that we are able to achieve better or similar out-of-sample accuracy on these
data sets compared to the popular greedy algorithms, CART and C4.5.
\end{arxiv}

CORELS targets large (not massive) problems,
where interpretability and certifiable optimality are important.
%
We illustrate the efficacy of our approach using (1)~the ProPublica COMPAS data set~\citep{LarsonMaKiAn16}, for the problem of two-year recidivism prediction,
and (2)~stop-and-frisk data sets from the
\begin{kdd}
New York Civil Liberties Union (NYCLU)~\citep{nyclu:2014},
\end{kdd}
\begin{arxiv}
NYPD~\citep{nypd} and the NYCLU~\citep{nyclu:2014},
\end{arxiv}
to predict whether a weapon will be found
on a stopped individual who is frisked or searched.
%
On these data, we produce certifiably optimal, interpretable rule lists that achieve
the same accuracy as approaches such as random forests.
%
This calls into question the need for use of a proprietary,
black box algorithm for recidivism prediction.

Our work overlaps with the thesis of~\citet{Larus-Stone17}.
%
We have also written a
\begin{kdd}
long version of this report that includes proofs to all
bounds in~\S\ref{sec:framework}, additional bounds and empirical results,
and further implementation and data processing details~\citep{AngelinoLaAlSeRu17}. \\

Our code is at \textbf{\url{https://github.com/nlarusstone/corels}}.
\end{kdd}
\begin{arxiv}
preliminary conference version of this article~\citep{AngelinoLaAlSeRu17-kdd}, and a report
highlighting systems optimizations of our implementation~\citep{Larus-Stone18-sysml}; the latter includes
additional empirical measurements not presented here. \\

Our code is at \textbf{\url{https://github.com/nlarusstone/corels}},
where we provide the C++ implementation we used in our experiments~(\S\ref{sec:experiments}).
% Python and R wrappers?
%
\citet{corels-website} have also created an interactive web interface at
\textbf{\url{https://corels.eecs.harvard.edu}}, where a user can upload data and
run CORELS from a browser.
\end{arxiv}

\input{sections/related_work}

\input{sections/framework}

\input{sections/bounds}

\input{sections/implementation-long}

\input{sections/experiments-long}

\input{sections/conclusion}

% Acknowledgements should go at the end, before appendices and references
\acks{E.A. conducted most of this work while supported by the Miller Institute for Basic Research
in Science, University of California, Berkeley, and hosted by Prof. M.I. Jordan at RISELab.
%
C.D.R. is supported in part by MIT-Lincoln Labs and the National Science Foundation under IIS-1053407.
%
E.A. would like to thank E.~Jonas, E.~Kohler, and S.~Tu for early implementation
guidance, A.~D'Amour for pointing out the work by~\citet{Goel16}, V.~Kanade, S.~McCurdy,
J.~Schleier-Smith and E.~Thewalt for helpful conversations, and members of RISELab,
SAIL, and the UC Berkeley Database Group for their support and feedback.
%
We thank H.~Yang and B.~Letham for sharing advice and code for processing data
and mining rules, B.~Coker for his critical advice on using the ProPublica COMPAS data set,
as well as V.~Kaxiras and A.~Saligrama for their recent contributions to our implementation
and for creating the CORELS website. We are very grateful to our editor and anonymous reviewers.
}

\vskip 0.2in

\renewcommand{\theHsection}{A\arabic{section}}
\appendix
%\renewcommand{\thesection}{\Alph{section}}
\input{sections/appendix-bounds}
\input{sections/appendix}

\bibliography{refs}

\end{document}
