%\documentclass[aoas,preprint]{imsart}
%\usepackage{fullpage}
%\setattribute{journal}{name}{}
%\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
%
%\usepackage{graphicx,verbatim}
%\usepackage[round]{natbib}
%\usepackage{url}
%\usepackage{amsmath,amssymb,amsthm,amsfonts}
%\usepackage{algorithm}
%\usepackage{algpseudocode}
%\usepackage{todonotes}
%\usepackage{subfig}
%\usepackage{dsfont}
%\usepackage{listings}
%\usepackage{comment}
%
%\usepackage{tikz}
%\usetikzlibrary{arrows}
%
%\input{latex_macros}
%\frenchspacing
%\hyphenation{speed-up}
%
%\begin{document}

\section{Related Work}


We discuss related literature in several subfields.

\textit{Interpretable Models:} There is a growing interest in interpretable (transparent, comprehensible) models because of their societal importance~\citep[see ][]{ruping2006learning,bratko1997machine,dawes1979robust,VellidoEtAl12,Giraud98,Holte93,Schmueli10,Huysmans11,Freitas14}. There are now regulations on algorithmic decision-making in the European Union on the ``right to an explanation" \citep{Goodman2016EU} that would legally require interpretability in predictions.


\textit{Optimal Decision Tree Modeling}: The body of work closest to ours is possibly that of optimal decision tree modeling.
%
Since the late 1990's, there has been research on building optimal decision trees
using optimization techniques~\citep{Bennett96optimaldecision,dobkininduction},
continuing until the present~\citep{FarhangfarGZ08}.
%
A particularly interesting paper along these lines is that of \citet{NijssenFromont2010}, who created a ``bottom-up" way to form optimal decision trees. Their method performs an expensive search step, mining all possible leaves (rather than all possible rules), and uses those leaves to form trees. Their method can lead to memory problems, but it is possible that these memory issues can be mitigated using the theorems in this paper. \footnote{There is no public version of their code for distribution as of this writing.} Another work close to ours is that of \citet{GarofalakisHyRaSh00}, who introduce an algorithm to generate more interpretable decision trees by allowing constraints to be placed on the size of the decision tree. During tree construction, they bound the possible Minimum Description Length (MDL) cost of every different split at a given node. If every split at that node is more expensive than the actual cost of the current subtree, then that node can be pruned. In this way, they are able to prune the tree while constructing it instead of just constructing the tree and then pruning at the end. They do not aim for optimal trees; they build trees that obey constraints, and find optimal subtrees within the trees that were built during the building phase.
%\textcolor{red}{Hm?} However, even with the added bounds, this approach does not generally yield globally optimal decision trees because they constrained the number of nodes in the tree.

\textit{Greedy splitting and pruning:} Unlike optimal decision tree methods, methods like
CART \citep{Breiman84} and C4.5 \citep{Quinlan93} do not perform exploration of the search space beyond greedy splitting. There are a huge number of algorithms in this class.

\textit{Bayesian tree and rule list methods}: Some of these approaches that aim to explore
the space of trees~\citep{Dension:1998hl,Chipman:2002hc,Chipman10} use Monte Carlo methods.
%
However, the space of trees of a given depth is much larger than the space of
rule lists of that same level of depth, and the trees within these algorithms
are grown in a top-down greedy way.
%
Because of this, the authors noted that their MCMC chains tend to reach only
locally optimal solutions.
%
This explains why Bayesian rule-based methods \citep{LethamRuMcMa15,YangRuSe16} have tended to be more successful in escaping local minima.
%
Our work builds specifically on that of \citet{YangRuSe16}.
%
In particular, we use their library for efficiently representing and
operating on bit vectors, and build on their bounds.
%
Note that the 1995 RIPPER algorithm \cite{ripper} is similar to the Bayesian tree methods in that it grows, prunes, and then locally optimizes.

\textit{Rule learning methods:} 
Most rule learning methods are not designed for optimality or interpretability, but for computational speed and/or accuracy. In \textit{associative classification} \citep{Vanhoof10,Liu98,Li01,Yin03}, classifiers are often formed greedily from the top down as rule lists, or they are formed by taking the simple union of pre-mined rules, whereby any observation that fits into any of the rules is classified as positive.
%
In \textit{inductive logic programming} \citep{muggleton1994inductive},
algorithms construct disjunctive normal form patterns via a set of operations
(rather than using optimization).
%
These approaches are not appropriate for obtaining a guarantee of optimality.
%
Methods for decision list learning construct rule lists iteratively in a greedy way
\citep{Rivest87,Sokolova03,Marchand05,RudinLeMa13,Goessling2015};
these too have no guarantee of optimality, and tend not to produce optimal rule lists in general.
%
Some methods allow for interpretations of single rules, without constructing rule lists \citep{McCormick:2011ws}.

There is a tremendous amount of related work in other subfields that
\begin{arxiv}
are too numerous to discuss at length here.
\end{arxiv}
\begin{kdd}
we do not discuss here.
\end{kdd}
We have not discussed \textit{rule mining} algorithms since they are part of an interchangeable preprocessing step for our algorithm and are deterministically fast (\ie they will not generally slow our algorithm down). We also did not discuss methods that create disjunctive normal form models, \eg logical analysis of data, and many associative classification methods. 

\textit{Related problems with interpretable lists of rules:} Beyond trees that are optimized 
for accuracy and sparsity, rule lists have been developed for various applications,
and with exotic types of constraints.
%
For example, Falling Rule Lists \citep{WangRu15} are constrained to have decreasing probabilities down the list as are rule lists for dynamic treatment regimes \citep{ZhangEtAl15} and cost-sensitive dynamic treatment regimes \citep{LakkarajuRu17}. Both Wang et. al \cite{WangRu15} and Lakkaraju et. al \cite{LakkarajuRu17} use Monte Carlo searches through the space of rule lists. The method proposed in this work could potentially be adapted to handle these kinds of interesting problems. We are currently working on bounds for Falling Rule Lists \citep{ChenRu17} similar to those presented here.




%	Recent work in the field of decision lists has focused on the creation of probabilistic decision lists that generate a posterior distribution over the space of potential decision lists\citep{LethamRuMcMa15,YangRuSe16}. These methods achieve good accuracy while maintaining a small execution time. In addition, these methods improve on existing methods such as CART or C5.0 by optimizing over the global space of decision lists as opposed to searching for rules greedily and getting stuck at local optima. We take the same approach towards optimizing over the global search space, though we donâ€™t use probabilistic techniques. In addition, we use the rule mining framework from \citep{LethamRuMcMa15} to generate the rules for our data sets. \citep{YangRuSe16} builds on \citep{LethamRuMcMa15} by placing bounds on the search space and creating a high performance bit vector manipulation library. We use that bit vector manipulation library to perform our computations, and add additional bounds to further prune the search space.




%Efficient Algorithms for Constructing Decision Trees with Constraints, Scalable Data Mining with Model Constraints, Building Decision Trees with Constraints

%	Our use of a branch and bound technique has also been applied to decision tree generation methods. \citep{garofalakis:2000-kdd} created an algorithm to generate more interpretable decision trees by allowing one to constrain the size of the decision tree. \citep{garofalakis:2000-kdd} uses branch-and-bound to constrain the size of the search space and limit the eventual size of the decision tree. During tree construction, \citep{garofalakis:2000-kdd} bounds the possible MDL cost of every different split at a given node. If every split at that node is more expensive than the actual cost of the current subtree, then that node can be pruned. In this way, they were able to prune the tree while constructing it instead of just constructing the tree and then pruning at the end.

%ProPublica
%	Certain problems require that the model used to solve that problem be interpretable as well as accurate. \citep{LarsonMaKiAn16} examines the problem of predicting recidivism and shows that a black box model, specifically the COMPAS score from the company Northpointe, has racially biased prediction. Black defendants are misclassified at a higher risk for recidivism than in actuality, while white defendants are misclassified at a lower risk. The model which produces the COMPAS scores is a black box algorithm which is not interpretable, and therefore the model does not provide a way for human input to correct for these racial biases. Our model produces similar accuracies to the logistic regression and COMPAS scores from \citep{LarsonMaKiAn16} while maintaining its interpretability.

%\bibliographystyle{abbrvnat}
%\bibliography{refs}
%
%\end{document}
