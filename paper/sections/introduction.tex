%\documentclass[aoas,preprint]{imsart}
%\usepackage{fullpage}
%\setattribute{journal}{name}{}
%\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
%
%\usepackage{graphicx,verbatim}
%\usepackage[round]{natbib}
%\usepackage{url}
%\usepackage{amsmath,amssymb,amsthm,amsfonts}
%\usepackage{algorithm}
%\usepackage{algpseudocode}
%\usepackage{todonotes}
%\usepackage{subfig}
%\usepackage{dsfont}
%\usepackage{listings}
%\usepackage{comment}
%
%\usepackage{tikz}
%\usetikzlibrary{arrows}
%
%\input{latex_macros}
%\frenchspacing
%\hyphenation{speed-up}
%
%\begin{document}

\section{Introduction}

As machine learning continues to grow in importance for socially-important decisions, the interpretability of predictive models remains a crucial problem. Our goal is to build models that are both highly predictive and easily understood by humans. We use rule lists, also known as decision lists, to achieve this goal. Rule lists are lists composed of if-then statements, which are easily interpreted; the rules give a reason for each prediction.

Constructing rule lists, or more generally, decision trees, has been a challenge for more than
30 years \cite{Rivest87,Breiman84,Quinlan93}. Most approaches use greedy splitting techniques \cite{Rivest87,Breiman84,Quinlan93}. Recent approaches use Bayesian analysis, either to find a locally optimal solution \cite{Chipman:1998jh} or to explore the search space \citep{LethamRuMcMa15, YangRuSe16}. These approaches achieve high accuracy while also managing to run reasonably quickly. However, despite the apparent accuracy of the rule lists generated by these algorithms, there is no way to determine either if the generated rule list is optimal or how close it is to optimal.

Optimality is important, because there are societal implications for a lack of optimality. The Pro-Publica article on the COMPAS recidivism prediction tool \citep{LarsonMaKiAn16} is one example. It highlights a case where a black-box, proprietary predictive model is being used for recidivism prediction. Larson et al. show that the COMPAS scores are racially biased, but since the model is not transparent, no one (outside of the creators of COMPAS) can determine the reason or extent of the bias \citep{LarsonMaKiAn16}, nor can anyone determine the reason for any particular prediction.
%
By using COMPAS, users implicitly assumed that a transparent model
would not be sufficiently accurate for recidivism prediction,
thus a more accurate, black box model would have to suffice.
%
We wondered whether there was indeed no possible transparent model that would suffice. Answering that question requires solving a computationally hard problem, namely finding a transparent model that is optimal, with a certificate of optimality, among a particular pre-determined class of models. That way one could say, with certainty, whether a transparent model (from this class of models) with sufficient accuracy exists for this problem, before resorting to black box models.

To that end, we consider the class of rule lists created from pre-mined frequent itemsets. The rule list must be assembled from these itemsets to minimize a regularized risk function, $R$. This is a hard discrete optimization problem. Brute force solutions that minimize $R$ are computationally prohibitive due to the exponential number of possible rule lists. However, this is a worst case bound that is not realized in practical settings. For realistic cases, it is possible to solve fairly large cases of this problem to optimality, with the careful use of algorithms, data structures, and implementation techniques.

We develop specialized tools from the field of discrete optimization and artificial intelligence, and in particular, a special branch-and-cut algorithm, called  Certifiably Optimal RulE ListS (CORELS), that provides (1) the optimal solution, (2) a certficiate of optimality and (3) a collection of near-optimal solutions and the distance between each such solution and the optimal one. The certificate of optimality means that we can investigate how close other models (e.g., models provided by greedy algorithms) are to optimal. In particular, we can investigate if the rule lists from probabilistic approaches are nearly optimal or whether those approaches sacrifice too much accuracy in the interest of speed.

\begin{arxiv}
Within its branch-and-cut procedure, CORELS maintains an upper bound on the maximum value of $R$ that each incomplete rule list can achieve. This allows it to prune an incomplete rule list (and every possible extension) if the bound is worse than the accuracy of the best rule list that we've already looked at. The use of careful bounding techniques leads to massive pruning of the search space of potential rule lists. We continue to consider incomplete and complete rule lists until we have either examined or eliminated every rule list from consideration. Thus, we terminate with the optimal rule list, the close-to-optimal rule lists, and a certificate of optimality.
\end{arxiv}

The efficacy of CORELS depends on how much of the search space our bounds allow us to prune. The upper bound on $R$ must thus be as tight as reasonably possible. The bound we maintain throughout the calculation is a minimum of several bounds, that come in three categories. The first category of bounds are those intrinsic to the rules themselves. This category includes bounds stating that each rule must capture sufficient data; if not, the rule list is provably non-optimal. The second type of bound compares an upper bound on the value of $R$ to that of the current best solution. This allows us to exclude parts of the search space that could never be better than our current solution. Finally, our last type of bound is based on comparing incomplete rule lists that capture the same data and pursuing only the more accurate option. This last class of bounds is especially important -- without our use of a novel \textit{symmetry-aware map}, we are unable to solve most problems of reasonable scale. This symmetry-aware map keeps track of the best accuracy for all the permutations of a given incomplete rule list.

We keep track of these bounds using a modified trie, called a \emph{prefix tree}. Each node in the prefix tree represents an individual rule; thus, each path in the tree represents a rule list with the final node in the path contains the metrics about that rule list. This tree structure facilitates the use of multiple different selection algorithms including breadth-first search, a priority queue based on a custom curiosity function, and a stochastic selection process. In addition, we are able to limit the number of nodes in the tree and thereby achieve a way of tuning space-time tradeoffs in a robust manner. This tree structure is a useful way of organizing the generation of rule lists and should be parallelizable.

\begin{arxiv}
We evaluated CORELS on a number of publicly available datasets and have made code for our algorithm and experiments publicly available. Our metric of success was 10-fold cross validated prediction accuracy on a subset of the data. These datasets involve hundreds of rules and hundreds or thousands of observations. CORELS is generally able to find the optimal rule list in a matter of seconds and certify it within about 10 minutes. We show that we are able to achieve better out-of-sample accuracy on these datasets than the popular greedy algorithms, CART and C5.0.
\end{arxiv}

This algorithm is designed for solving large (not massive) problems, where interpretability and certifiable optimality are important, such as recidivism prediction. We illustrate the efficacy of our approach using the COMPAS dataset \cite{LarsonMaKiAn16}. We produce a certifiably optimal, interpretable rule list that achieves the same accuracy as a random forest approach, thereby calling into question the need for use of a proprietary algorithm for recidivism prediction.

%\bibliographystyle{abbrvnat}
%\bibliography{refs}
%
%\end{document}
