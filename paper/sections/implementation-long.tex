\input{sections/incremental}

\section{Implementation}
\label{sec:implementation}

We implement our algorithm using a collection of optimized data structures.
%
First, in~\S\ref{sec:trie}, we explain our choice of a prefix tree
to support incremental computation~(\S\ref{sec:incremental}).
%
Second, in~\S\ref{sec:queue}, we describe several queue designs
that implement different search policies.
%
Third, in~\S\ref{sec:pmap}, we introduce a symmetry-aware map to support
symmetry-aware pruning~(Corollary~\ref{thm:permutation},~\S\ref{sec:permutation}).
%
Next, in~\S\ref{sec:execution}, we summarize our incremental execution model,
and summarize CORELS's inner computational loop in Algorithm~\ref{alg:bounds},
where we highlight each of bounds from~\S\ref{sec:bounds} that we use to prune the search space.
%
We additionally describe in~\S\ref{sec:gc} how we garbage collect our data structures.
%
Finally, we explore in~\S\ref{sec:scheduling} how our queue can be used to support
custom scheduling policies designed to improve performance.
%
Our implementation of CORELS can be found~at: \\

\centerline{\url{https://github.com/nlarusstone/corels}.}

\subsection{Prefix tree}
\label{sec:trie}

Our incremental computations (\S\ref{sec:incremental}) require a
cache to keep track of rule lists we have already evaluated.
%
We implement this cache as a prefix tree, a data structure also known as a trie,
which allows us to efficiently represent structure shared between related rule lists.
%
Each node in the prefix tree encodes an individual rule ${r_k = p_k \rightarrow q_k}$.
%
Each path starting from the root represents a rule list, such that the final node
in the path also contains metadata associated with that corresponding rule list.
%
For a rule list ${\RL = (\Prefix, \Labels, \Default, K)}$,
with prefix ${\Prefix = (p_1, \dots, p_K)}$,
let~$\varphi(\RL)$ denote the corresponding node in the trie.
%
The metadata at node~$\varphi(\RL)$ supports the incremental computations
we described in~\S\ref{sec:incremental}, and includes:
\begin{itemize}
\item An index encoding antecedent~$p_K$.
\item The corresponding label prediction~$q_K$.
%\item $\RL$'s length~$K$; equivalently, node~$\varphi(\RL)$'s depth in the trie. % <-- not strictly necessary
\item The default rule label prediction~$\Default$.
\item $\NCap$, the number of samples captured by prefix $\Prefix$, as in~\eqref{eq:num-cap}. % <-- not strictly necessary
% \item The objective $\Obj(\RL, \x, \y)$~\eqref{eq:objective}. % <-- not strictly necessary
\item The objective lower bound $b(\Prefix, \x, \y)$, defined in~\eqref{eq:lower-bound},
  the central bound in our framework (Theorem~\ref{thm:bound}).
\item The lower bound on the default rule misclassification error
  $b_0(\Prefix, \x, \y)$, defined in~\eqref{eq:lb-b0},
  to support our equivalent points bound (Theorem~\ref{thm:identical}).
\item An indicator denoting whether or not this node should be deleted (see~\S\ref{sec:gc}).
\item A representation of viable extensions of~$\Prefix$,
  \ie length ${K+1}$ rule lists that start with~$\Prefix$ and have not been pruned.
\end{itemize}
We note that we implement the prefix tree as a custom C++ class. % decouple artifact from design
%
% This might be a bit much detail for here
%In addition to our base trie class, we also implemented a different node type that we use in our algorithm.
%This sub-class has an additional field that can hold custom metrics that we use to order the search space.
%Since this additional field is just a double, the memory overhead is minimal.
%
% Interesting trie-related subroutines besides garbage collection?

\subsection{Queue}
\label{sec:queue}

The queue is a worklist that orders exploration over the search space of possible
rule lists; every queue element corresponds to a prefix tree leaf, and vice versa.
%
In our implementation, each queue element points to a leaf;
when we pop an element off the queue, we use the leaf's metadata to
incrementally evaluate the corresponding prefix's children.

We order entries in the queue to implement several different policies.
%
A first-in-first-out (FIFO) queue implements breadth-first search (BFS),
and a priority queue implements best-first search; in our released code,
we implement all scheduling policies, including BFS, using a STL C++ priority queue.
%
Example priority queue policies include ordering
by the lower bound, the objective, a custom metric that maps prefixes to real values,
or prefix length, which corresponds to depth-first search (DFS).
%
As we demonstrate in our experiments~(\S\ref{sec:ablation}),
we find that using an custom search strategy,
such as ordering by the lower bound, usually leads to a faster runtime than BFS.

In preliminary work (not shown), we also experimented with
stochastic exploration processes that bypass the need for a queue
by instead following random paths from the root to leaves;
developing such methods could be an interesting direction for future work.

\subsection{Symmetry-aware map}
\label{sec:pmap}

The symmetry-aware map supports symmetry-aware pruning~(\S\ref{sec:permutation}).
%
In our implementation, we specifically leverage our permutation bound
(Corollary~\ref{thm:permutation}), though it is also possible to directly
exploit the more general equivalent support bound (Theorem~\ref{thm:equivalent}).
%
We implement this symmetry-aware map using the C++ STL unordered\_map,
to map all permutations of a set of antecedents to a key whose value contains
the best ordering of those antecedents (\ie the prefix with the smallest lower bound).
%
Every antecedent is associated with an index, and we call the numerically
sorted order of a set of antecedents its canonical order.
%
Thus by querying a set of antecedents by its canonical order,
all permutations map to the same key.
%
The value stored in the map consists of the lower bound and the actual ordering
of the rules that is best for that permutation.

Before we consider adding a prefix~$\Prefix$ to the trie and queue, we check
whether the map already contains a permutation~$\pi(\Prefix)$ of that prefix.
%
If no such permutation exists, then we insert~$\Prefix$ into the map, trie, and queue.
%
Otherwise, if a permutation~$\pi(\Prefix)$ exists and the lower bound of~$\Prefix$ is better
than that of~$\pi(\Prefix)$, \ie ${b(\Prefix, \x, \y) <}$ ${b(\pi(\Prefix), \x, \y)}$,
then we update the map and remove~$\pi(P_j)$ and its entire subtree from the trie;
we also insert~$\Prefix$ into the trie and queue.
%
Otherwise, if there exists a permutation~$\pi(\Prefix)$ such that
${b(\pi(\Prefix), \x, \y) \le}$ ${b(\Prefix, \x, \y)}$,
then we do nothing, \ie we do not insert~$\Prefix$ into any data structures.

\subsection{Incremental execution}
\label{sec:execution}

\begin{algorithm}[t!]
  \caption{CORELS's inner loop evaluates all of a prefix~$\Prefix$'s children.}
\label{alg:bounds}
\begin{algorithmic}
\small
\State Define $\mathbf{z} \in \{0, 1\}^N$, s.t. ${z_n = \sum_{u=1}^U \one [x_n \in e_u] [y_n = q_u]}$, where~$e_u$ and~$q_u$ are defined as in~\S\ref{sec:identical} \\

\For {$s$ in $\RuleSet$} \Comment{Evaluate all of $\Prefix$'s children}
    \If {$s$ not in $\Prefix$}
        \State $\PrefixB \gets (\Prefix, s)$ \Comment{\textbf{Branch}: Generate child $\PrefixB$}
        \State $\mathbf{v} \gets \mathbf{u} \wedge \Cap(\x, s)$ \Comment{Bit vector indicating data captured by $s$ in $\PrefixB$}
        \State $n_v = \Count(\mathbf{v})$ \Comment{Number of data captured by $s$, the last antecedent in $\PrefixB$}
        \If {$n_v < \Reg$}
            \State \Continue \Comment{\textbf{Lower bound on antecedent support (Theorem\ref{thm:min-capture})}}
        \EndIf
        \State $\mathbf{w} \gets \mathbf{v} \wedge \y$ \Comment{Bit vector indicating data captured by $s$ with label $1$}
        \State $n_w = \Count(\mathbf{w})$ \Comment{Number of data captured by $s$ with label $1$}
       \If {$n_w < \Reg$}
           \State \Continue \Comment{\textbf{Lower bound on accurate antecedent support (Theorem~\ref{thm:min-capture-correct})}}
       \EndIf
       \If {$n_w / n_v > 0.5$}
           \State $\delta b \gets (n_v - n_w) / N$ \Comment{Misclassification error of the rule $s \rightarrow 1$}
       \Else
           \State $\delta b \gets n_w / N$ \Comment{Misclassification error of the rule $s \rightarrow 0$}
       \EndIf
       \State $b(\PrefixB, \x, \y) \gets b(\Prefix, \x, \y) + \Reg + \delta b$ \Comment{Incremental lower bound~\eqref{eq:inc-lb}}
       \If {$b(\PrefixB, \x, \y) + \Reg \ge \CurrentObj$} \Comment{\textbf{Hierarchical objective lower bound (Theorem~\ref{thm:bound})}}
           \State \Continue \hfill {combined with the \textbf{Lookahead bound (Lemma~\ref{lemma:lookahead})}}
       \EndIf
       \State $\mathbf{f} \gets \mathbf{u} \wedge \neg\,\mathbf{v} $ \Comment{Bit vector indicating data not captured by $\PrefixB$}
       \State $n_f = \Count(\mathbf{f})$ \Comment{Number of data not captured by $\PrefixB$}
       \State $\mathbf{g} \gets \mathbf{f} \wedge \y$ \Comment{Bit vector indicating data not captured by $\PrefixB$ with label $1$}
       \State $n_g = \Count(\mathbf{g})$ \Comment{Number of data not captued by $\PrefixB$ with label $1$}
       \If {$n_g / n_f > 0.5$}
           \State $\delta\Obj \gets (n_f - n_g) / N$ \Comment{Misclassification error of the default label prediction $1$}
       \Else
           \State $\delta\Obj \gets n_g / N$ \Comment{Misclassification error of the default label prediction $0$}
       \EndIf
       \State $\Obj(\RLB, \x, \y) \gets b(\PrefixB, \x, \y) + \delta\Obj$ \Comment{Incremental objective~\eqref{eq:inc-obj}}
       \State $\RLB \gets (\PrefixB, \LabelsB, \DefaultB, K+1)$ \Comment{$\LabelsB, \DefaultB$ are set in the incremental functions}
       \If {$\Obj(\RLB, \x, \y) < \CurrentObj$}
           \State $(\CurrentRL, \CurrentObj) \gets (\RLB, \Obj(\RLB, \x, \y))$ \Comment{Update current best rule list and objective}
       \EndIf
       \State $Q$.push$(\PrefixB)$ \Comment{Add $\PrefixB$ to the queue}
       \State $C$.insert$(\PrefixB, b(\PrefixB, \x, \y))$ \Comment{Add $\PrefixB$ and its lower bound to the cache}
    \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

Mapping our algorithm to our data structures produces the following execution strategy.
%
We initialize the current best objective~$\CurrentObj$ and rule list~$\CurrentRL$.
%
While the trie contains unexplored leaves, a scheduling policy selects the next prefix~$\Prefix$
to extend; in our implementation, we pop elements from a (priority) queue, until the queue is empty.
%
Then, for every antecedent~$s$ that is not in~$\Prefix$,
we construct a new prefix~$\Prefix'$ by appending~$s$ to~$\Prefix$;
we incrementally calculate the lower bound~$b(\Prefix', \x, \y)$,
the objective~$\Obj(\RL', \x, \y)$, of the associated rule list~$\RL'$,
and other quantities used by our algorithm, summarized by the metadata fields of
the (potential) prefix tree node~$\varphi(\Prefix')$.

If the objective~$\Obj(\RL', \x, \y)$ is less than the current best objective~$\CurrentObj$,
then we update~$\CurrentObj$ and~$\RL$.
%
If the lower bound of the new prefix~$\Prefix'$ is less than the current best objective,
%\ie ${b(\Prefix', \x, \y) < \CurrentObj}$,
then as described in~\S\ref{sec:pmap}, we query the symmetry-aware map for~$\Prefix'$;
if we insert~$\Prefix'$ into the symmetry-aware map, then we also insert it into the trie and queue.
%
Otherwise, %\ie ${b(\Prefix', \x, \y) \ge \CurrentObj}$
then by our hierarchical lower bound (Theorem~\ref{thm:bound}),
no extension of~$\Prefix'$ could possibly lead to a rule list with objective
better than~$\CurrentObj$, thus we do not insert~$\Prefix'$ into the tree or queue.
%
We also leverage our other bounds from~\S\ref{sec:framework}
to aggressively prune the search space; we highlight each of these bounds
in Algorithm~\ref{alg:bounds}, which summarize CORELS's inner computational loop.
%
When there are no more leaves to explore, \ie the queue is empty, we output the optimal rule list.
%
We can also choose to terminate early according to an alternate condition,
\eg when the size of the prefix tree exceeds some threshold.

\subsection{Garbage collection}
\label{sec:gc}

During execution, we garbage collect the trie.
%
Each time we update the minimum objective,
we traverse the trie in a depth-first manner, deleting all subtrees
of any node with lower bound larger than the current minimum objective.
%
At other times, when we encounter a node with no children, we prune upwards,
deleting that node and recursively traversing the tree towards the root,
deleting any childless nodes.
%
This garbage collection allows us to constrain the trie's memory consumption, though in our
experiments we observe the minimum objective to decrease only a small number of times.

In our implementation, we cannot immediately garbage collect trie elements that are currently in the queue.
%
The STL C++ priority queue is a wrapper container that prevents access to the underlying data structure.
%
Therefore we cannot access elements in the middle of the queue,
even know the relevant identifying information.
%
Thus, we have no way to update the queue without iterating over every element.
%
We address this by lazily marking nodes in the prefix tree as deleted (see~\S\ref{sec:trie}),
without deleting the physical node until it has been removed from the queue.
%
We define two different queues that we refer to in our experiments~(\S\ref{sec:experiments}):
the physical queue corresponds to all elements in the C++ queue, and thus all prefix tree leaves,
and the logical queue corresponds only to those prefix tree leaves that have not been marked deleted.

\subsection{Custom scheduling policies}
\label{sec:scheduling}

In our setting, an ideal scheduling policy would immediately identify an optimal
rule list, and then certify its optimality by systematically eliminating the
remaining search space.
%
This motivates trying to design scheduling policies that tend to quickly find optimal rule lists.
%
When we use a priority queue to order the set of prefixes to evaluate next,
we are free to implement different scheduling policies via the ordering of
elements in the queue.
%
This motivates designing functions that assign higher priorities to `better'
prefixes that we believe are more likely to lead to optimal rule lists.
%
Note that we follow the convention that priority queue elements are ordered
by keys, such that keys with smaller values correspond to higher priorities.

We introduce a custom class of functions that we call \emph{curiosity} functions.
%
Broadly, we think of the curiosity of a rule list~$\RL$
as the expected objective value of another rule list~$\RL'$ that is related to~$\RL$;
different models of the relationship between~$\RL$ and~$\RL'$ lead to different
curiosity functions.
%
In general, the curiosity of~$\RL$ is by definition equal to the sum of the expected
misclassification error and the expected regularization penalty of~$\RL'$:
\begin{align}
\Curiosity(\Prefix, \x, \y) \equiv \E[ \Obj(\RL', \x, \y) ]
&= \E[\Loss(\Prefix', \Labels', \x, \y)] + \Reg \E[ K' ].
\label{eq:curiosity}
\end{align}

Next, we describe a simple curiosity function for a rule list~$\RL$ with prefix~$\Prefix$.
%
First, let~$\NCap$ denote the number of datapoints captured by~$\Prefix$, \ie
\begin{align}
\NCap \equiv \sum_{n=1}^N \Cap(x_n, \Prefix).
\label{eq:num-cap}
\end{align}
We now describe a model that generates another
rule list~${\RL' = (\Prefix', \Labels', \Default', K')}$ from~$\Prefix$.
%
Assume that prefix~$\Prefix'$ starts with~$\Prefix$ and captures all the data,
such that each additional antecedent in~$\Prefix'$
captures as many `new' datapoints as each antecedent in~$\Prefix$, on average;
then, the expected length of~$\Prefix'$ is:
\begin{align}
\E[ K' ] = \frac{N}{\NCap / K}.
\label{eq:curiosity-length}
\end{align}
Furthermore, assume that each additional antecedent in~$\Prefix'$
makes as many mistakes as each antecedent in~$\Prefix$, on average,
thus the expected misclassification error of~$\Prefix'$ is:
\begin{align}
\E[\Loss(\Prefix', \Labels', \x, \y)]
&= \E[\Loss_p(\Prefix', \Labels', \x, \y)] + \E[\Loss_0(\Prefix', \Default', \x, \y)] \nn \\
&= \E[\Loss_p(\Prefix', \Labels', \x, \y)]
=  \E[ K' ] \left(\frac{\Loss_p(\Prefix, \Labels, \x, \y)}{K}\right).
\label{eq:curiosity-error}
\end{align}
Note that the default rule misclassification error~$\Loss_0(\Prefix', \Default', \x, \y)$
is zero because we assume that~$\Prefix'$ captures all the data.
%
Combining~\eqref{eq:curiosity-length}~\eqref{eq:curiosity-error} and thus gives
curiosity for this model:
\begin{align*}
\Curiosity(\Prefix, \x, \y)
%= \E[\Loss_p(\Prefix', \Labels', \x, \y)] + \Reg \E[ K' ]
%&= \E[ K' ] \left(\frac{\Loss_p(\Prefix, \Labels, \x, \y)}{K}\right) + \Reg \E[ K' ] \\
%&=  \left(\frac{N}{\NCap / K}\right)
%  \left(\frac{\Loss_p(\Prefix, \Labels, \x, \y)}{K}\right)
%  + \Reg \left(\frac{N}{\NCap / K}\right) \nn \\
&= \left( \frac{N}{\NCap} \right) \biggl(\Loss_p(\Prefix, \Labels, \x, \y) + \Reg K \biggr) \\
&= \left( \frac{1}{N} \sum_{n=1}^N \Cap(x_n, \Prefix) \right)^{-1} b(\Prefix, \x, \y)
= \frac{b(\Prefix, \x, \y)}{\Supp(\Prefix, \x)},
\end{align*}
where for the second equality, we used the definitions of $\NCap$~\eqref{eq:num-cap}
and $\Prefix$'s lower bound~\eqref{eq:lower-bound}, and for the last equality,
we used the definition of $\Prefix$'s normalized support~\eqref{eq:support}.

The curiosity for a prefix~$\Prefix$ is thus also equal to its objective lower bound,
scaled by the inverse of its normalized support.
%
For two prefixes with the same lower bound, curiosity gives higher priority to
the one that captures more data.
%
This is a well-motivated scheduling strategy if we model prefixes that extend
the prefix with smaller support as having more `potential' to make mistakes.
%
We note that using curiosity in practice does not introduce new bit vector
or other expensive computations; during execution, we can calculate curiosity
as a simple function of already derived quantities.

In preliminary experiments, we observe that using a priority queue ordered by
curiosity sometimes yields a dramatic reduction in execution time,
compared to using a priority queue ordered by the objective lower bound.
%
Thus far, we have observed significant benefits on specific small problems,
where the structure of the solutions happen to render curiosity particularly
effective (not shown).
%
Designing and studying other `curious' functions, that are effective in more
general settings, is an exciting direction for future work.
