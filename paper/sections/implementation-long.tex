\input{sections/incremental}

\section{Implementation}
\label{sec:implementation}

Our implementation of CORELS can be found at \\

\url{https://github.com/nlarusstone/corels}.\\

We implement our algorithm using a collection of optimized data structures: a trie (prefix tree), a symmetry-aware map, and a queue.

\subsection{Prefix tree}

The trie is a custom C++ class used as a cache to keep track of rule lists we have already evaluated. 
Each node represents a rule that contains the id and prediction for the rule that the node represents.
Nodes in the trie also contain the metadata associated with that corresponding rule list.
This metadata includes the following bookkeeping information: which child rule lists are viable, lower bound, objective, equivalent points misclassification, length, number of samples captured by this prefix, prediction for the default rule, and whether or not this node should be pruned.
In addition to our base trie class, we also implemented a different node type that we use in our algorithm.
This sub-class has an additional field that can hold custom metrics that we use to order the search space.
Since this additional field is just a double, the memory overhead is minimal.

\subsection{Queue}
\label{sec:queue}

The queue is a worklist that orders exploration over the search space of possible rule lists.
We implement a number of different scheduling schemes including a stochastic exploration process, BFS, DFS, and priority metrics of curiosity, objective, or lower bound.
Our queue contains pointers to leaves in the trie to leverage incremental computation.
The search process involves selecting which leaf node to explore.
The stochastic exploration process bypasses the use of a queue by performing random walks on the trie until a leaf is reached.
Our other scheduling schemes use a STL C++ priority queue to hold and order all of the leaves of the trie that still need to be explored.
We find that using an custom search strategy, such as ordering by lower bound, usually leads to a faster runtime than using BFS.

\subsubsection{Custom scheduling policies}

In our setting, an ideal scheduling policy would immediately identify an optimal
rule list, and then certify its optimality by systematically eliminating the
remaining search space.
%
This motivates trying to design scheduling policies that tend to quickly find optimal rule lists.
%
When we use a priority queue to order the set of prefixes to evaluate next,
we are free to implement different scheduling policies via the ordering of
elements in the queue.
%
This motivates designing functions that assign higher priorities to `better'
prefixes that we believe are more likely to lead to optimal rule lists.
%
Note that we follow the convention that priority queue elements are ordered
by keys, such that keys with smaller values correspond to higher priorities.

We introduce a custom function that we call \emph{curiosity}.
%
First, let~$\NCap$ denote the number of datapoints captured by~$\Prefix$, \ie
\begin{align}
\NCap \equiv \sum_{n=1}^N \Cap(x_n, \Prefix). \nn
\end{align}
%
Now consider a simple model for the expected objective value of a rule list
${\RL' = (\Prefix', \Labels', \Default', K')}$ generated from~$\Prefix$.
%
Assume that prefix~$\Prefix'$ starts with~$\Prefix$ and captures all the data,
such that each additional antecedent in~$\Prefix'$
both captures as many `new' datapoints and makes as many mistakes as
as each antecedent in~$\Prefix$, on average.
%
We define curiosity as the expected objective value of~$\RL'$,
which is equal to the sum of the expected prefix
misclassification error and the expected regularization penalty:
\begin{align}
\Curiosity(\Prefix, \x, \y) \equiv \E[ \Obj(\RL', \x, \y) ] &= \E[\Loss_p(\Prefix, \Labels, \x, \y)] + \E[ \Reg K' ] \nn \\
&= \E[ K' ] \left(\frac{\Loss_p(\Prefix, \Labels, \x, \y)}{K}\right) + \Reg \E[ K' ] \nn \\
&=  \left(\frac{N}{\NCap / K}\right)
  \left(\frac{\Loss_p(\Prefix, \Labels, \x, \y)}{K}\right)
  + \Reg \left(\frac{N}{\NCap / K}\right) \nn \\
&= \left( \frac{N}{\NCap} \right) \biggl(\Loss_p(\Prefix, \Labels, \x, \y) + \Reg K \biggr) \nn \\
&= \left( \frac{1}{N} \sum_{n=1}^N \Cap(x_n, \Prefix) \right)^{-1} b(\Prefix, \x, \y)
= \frac{b(\Prefix, \x, \y)}{\Supp(\Prefix, \x)}. \nn
\end{align}
%
The curiosity for a prefix~$\Prefix$ is thus also equal to its objective lower bound,
scaled by the inverse of its normalized support.
%
For two prefixes with the same lower bound, curiosity gives higher priority to
the one that captures more data.
%
This is a well-motivated scheduling strategy if we model prefixes that extend
the prefix with smaller support as having more `potential' to make mistakes.
%
We note that using curiosity in practice does not introduce new bit vector
or other expensive computations; during execution, we can calculate curiosity
as a simple function of already derived quantities.

In preliminary experiments, we observe that using a priority queue ordered by
curiosity sometimes yields a dramatic reduction in execution time,
compared to using a priority queue ordered by the objective lower bound.
%
Thus far, we have observed significant benefits on specific small problems,
where the structure of the solutions happen to render curiosity particularly
effective (not shown).
%
Designing and studying other `curious' functions, that are effective in more
general settings, is an exciting direction for future work.

\subsection{Symmetry-aware map}
\label{sec:pmap}

The symmetry-aware map supports symmetry-aware pruning.
We implement this using the C++ STL unordered\_map, to map all permutations of a set of antecedents to a key, whose value
contains the best ordering of those antecedents (\ie the prefix with the smallest lower bound).
Every antecedent is associated with an index, and we call the numerically sorted order of a set of antecedents its canonical order.
Thus by querying a set of antecedents by its canonical order, all permutations map to the same key.
The value stored in the map consists of the lower bound and the actual ordering of the rules that is best for that permutation.
Before inserting permutation $P_i$ into the symmetry-aware map, we check if there exists a permutation $P_j$ of $P_i$ already in the map.
If there is no permutation exists, then we insert $P_i$ in the map.
Otherwise, if a permutation $P_j$ exists and the lower bound of $P_i$ is better than that of $P_j$, we update the map and remove $P_j$ and its subtree from the trie.
Else, if $P_j$ exists and has a better lower bound than $P_i$, we do nothing  (\ie we do not insert $P_i$ into the symmetry-aware map or the trie).

\subsection{Incremental execution}

Mapping our algorithm to our data structures produces the following execution strategy.
%
While the trie contains unexplored leaves, a scheduling policy selects the next prefix to extend.
%
Then, for every antecedent that is not already in this prefix, we calculate the lower bound,
objective, and other metrics for the rule list formed by appending the antecedent to the prefix.
%
If the lower bound of the new rule list is less than the current minimum objective, we insert that
rule list into the symmetry-aware map, trie, and queue, and, if relevant, update the
current minimum objective.
%
If the lower bound is greater than the minimum objective,
then no extension of this rule list could possibly be optimal,
thus we do not insert the new rule list into the tree or queue.
%
We also leverage our other bounds from~\S\ref{sec:framework}
to aggressively prune the search space.
%
When there are no more leaves to explore, we are able to return the optimal rule list.
%
We can also choose to terminate

\subsection{Garbage Collection}

During execution, we garbage collect the trie.
%
Each time we update the minimum objective,
we traverse the trie in a depth-first manner, deleting all subtrees
of any node with lower bound larger than the current minimum objective.
%
At other times, when we encounter a node with no children, we prune upwards--deleting that
node and recursively traversing the tree towards the root, deleting any childless nodes.
%
This garbage collection allows us to constrain the trie's memory consumption, though in our
experiments we observe the minimum objective to decrease only a
small number of times.

When performing garbage collection, we have trouble deleting trie elements that are currently in the queue.
In C++ the priority queue is a wrapper container that prevents access to the container underlying the queue.
Therefore we cannot access elements in the middle of the queue, even if we have know the value that we're trying to access.
Thus, we run into a problem where we want to delete something in the prefix tree that is currently in the queue, as we have no way to update the queue without iterating over every element.
We address this by lazily marking nodes deleted in the prefix tree without deleting the physical node until it has been removed from the queue.
We define two different queues: the collection of all leaves in the C++ queue is our physical queue while the collection of leaves that are not marked deleted is our logical queue.