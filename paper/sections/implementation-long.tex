\input{sections/incremental}

\section{Implementation}
\label{sec:implementation}

We implement our algorithm using a collection of optimized data structures
that we describe in this section:
a prefix trie (trie)~(\S\ref{sec:trie}), a queue~(\S\ref{sec:queue}),
and a symmetry-aware map~(\S\ref{sec:pmap}).
%
We additionally highlight how our queue can be used to support
custom scheduling policies designed to improve performance~(\S\ref{sec:scheduling}).
%
Our implementation of CORELS can be found at: \\

\centerline{\url{https://github.com/nlarusstone/corels}.}

\subsection{Prefix tree}
\label{sec:trie}

Our incremental computations (\S\ref{sec:incremental}) require a
cache to keep track of rule lists we have already evaluated.
%
We implement this cache as a prefix tree, which allows us to efficiently
represent shared structure between related rule lists.
%
Each node in the prefix tree encodes an individual rule ${r_k = p_k \rightarrow q_k}$.
%
Each path starting from the root represents a rule list, such that the final node
in the path also contains metadata associated with that corresponding rule list.
%
For a rule list ${\RL = (\Prefix, \Labels, \Default, K)}$,
with prefix ${\Prefix = (p_1, \dots, p_K)}$,
let~$\varphi(\RL)$ denote the corresponding node in the prefix tree.
%
The metadata at node~$\varphi(\RL)$ supports the incremental computations
we described in~\S\ref{sec:incremental}, and includes:
\begin{itemize}
\item An index encoding antecedent~$p_K$.
\item The corresponding label prediction~$q_K$.
%\item $\RL$'s length~$K$; equivalently, node~$\varphi(\RL)$'s depth in the trie. % <-- not strictly necessary
\item The default rule label prediction~$\Default$.
\item $\NCap$, the number of samples captured by prefix $\Prefix$, as in~\eqref{eq:num-cap}. % <-- not strictly necessary
% \item The objective $\Obj(\RL, \x, \y)$~\eqref{eq:objective}. % <-- not strictly necessary
\item The objective lower bound $b(\Prefix, \x, \y)$, defined in~\eqref{eq:lower-bound}.
\item The lower bound on the default rule misclassification error
  $b_0(\Prefix, \x, \y)$, defined in~\eqref{eq:lb-b0}
  from our equivalent points bound (Theorem~\ref{thm:identical}).
\item An indicator denoting whether or not this node should be deleted (see~\S\ref{sec:gc}).
\item A representation of viable extensions of~$\Prefix$,
  \ie length ${K+1}$ rule lists that start with~$\Prefix$ and have not been pruned.
\end{itemize}
We implement the prefix tree as a custom C++ class. % decouple artifact from design
%
% This might be a bit much detail for here
%In addition to our base trie class, we also implemented a different node type that we use in our algorithm.
%This sub-class has an additional field that can hold custom metrics that we use to order the search space.
%Since this additional field is just a double, the memory overhead is minimal.
%
% Interesting trie-related subroutines besides garbage collection?

\subsection{Queue}
\label{sec:queue}

The queue is a worklist that orders exploration over the search space of possible
rule lists; every queue element corresponds to a prefix tree leaf, and vice versa.
%
In our implementation, each queue element points to a leaf;
when we pop an element off the queue, we use the leaf's metadata to
incrementally evaluate the corresponding prefix's children.

We order entries in the queue to implement several different policies.
%
A first-in-first-out (FIFO) queue implements breadth-first search (BFS),
and a priority queue implements best-first search.\footnote{In our released code,
we implement all scheduling policies, including BFS, using a STL C++ priority queue.}
%
Example priority queue policies include ordering
by the lower bound, the objective, a custom metric that maps prefixes to real values,
or prefix length, which corresponds to depth-first search (DFS).
%
As we demonstrate in our experiments~(\S\ref{sec:ablation}),
we find that using an custom search strategy,
such as ordering by the lower bound, usually leads to a faster runtime than BFS.
%
We note that in preliminary work (not shown), we also experimented with
stochastic exploration processes that bypass the need for a queue
by instead following random paths from the root to leaves;
developing stochastic scheduling methods could be an interesting direction for future work.

\subsection{Custom scheduling policies}
\label{sec:scheduling}

In our setting, an ideal scheduling policy would immediately identify an optimal
rule list, and then certify its optimality by systematically eliminating the
remaining search space.
%
This motivates trying to design scheduling policies that tend to quickly find optimal rule lists.
%
When we use a priority queue to order the set of prefixes to evaluate next,
we are free to implement different scheduling policies via the ordering of
elements in the queue.
%
This motivates designing functions that assign higher priorities to `better'
prefixes that we believe are more likely to lead to optimal rule lists.
%
Note that we follow the convention that priority queue elements are ordered
by keys, such that keys with smaller values correspond to higher priorities.

We introduce a custom function that we call \emph{curiosity}.
%
First, let~$\NCap$ denote the number of datapoints captured by~$\Prefix$, \ie
\begin{align}
\NCap \equiv \sum_{n=1}^N \Cap(x_n, \Prefix).
\label{eq:num-cap}
\end{align}
%
Now consider a simple model for the expected objective value of a rule list
${\RL' = (\Prefix', \Labels', \Default', K')}$ generated from~$\Prefix$.
%
Assume that prefix~$\Prefix'$ starts with~$\Prefix$ and captures all the data,
such that each additional antecedent in~$\Prefix'$
both captures as many `new' datapoints and makes as many mistakes as
as each antecedent in~$\Prefix$, on average.
%
We define curiosity as the expected objective value of~$\RL'$,
which is equal to the sum of the expected prefix
misclassification error and the expected regularization penalty:
\begin{align}
\Curiosity(\Prefix, \x, \y) \equiv \E[ \Obj(\RL', \x, \y) ] &= \E[\Loss_p(\Prefix, \Labels, \x, \y)] + \E[ \Reg K' ] \nn \\
&= \E[ K' ] \left(\frac{\Loss_p(\Prefix, \Labels, \x, \y)}{K}\right) + \Reg \E[ K' ] \nn \\
&=  \left(\frac{N}{\NCap / K}\right)
  \left(\frac{\Loss_p(\Prefix, \Labels, \x, \y)}{K}\right)
  + \Reg \left(\frac{N}{\NCap / K}\right) \nn \\
&= \left( \frac{N}{\NCap} \right) \biggl(\Loss_p(\Prefix, \Labels, \x, \y) + \Reg K \biggr) \nn \\
&= \left( \frac{1}{N} \sum_{n=1}^N \Cap(x_n, \Prefix) \right)^{-1} b(\Prefix, \x, \y)
= \frac{b(\Prefix, \x, \y)}{\Supp(\Prefix, \x)}. \nn
\end{align}
%
The curiosity for a prefix~$\Prefix$ is thus also equal to its objective lower bound,
scaled by the inverse of its normalized support.
%
For two prefixes with the same lower bound, curiosity gives higher priority to
the one that captures more data.
%
This is a well-motivated scheduling strategy if we model prefixes that extend
the prefix with smaller support as having more `potential' to make mistakes.
%
We note that using curiosity in practice does not introduce new bit vector
or other expensive computations; during execution, we can calculate curiosity
as a simple function of already derived quantities.

In preliminary experiments, we observe that using a priority queue ordered by
curiosity sometimes yields a dramatic reduction in execution time,
compared to using a priority queue ordered by the objective lower bound.
%
Thus far, we have observed significant benefits on specific small problems,
where the structure of the solutions happen to render curiosity particularly
effective (not shown).
%
Designing and studying other `curious' functions, that are effective in more
general settings, is an exciting direction for future work.

\subsection{Symmetry-aware map}
\label{sec:pmap}

The symmetry-aware map supports symmetry-aware pruning.
We implement this using the C++ STL unordered\_map, to map all permutations of a set of antecedents to a key, whose value
contains the best ordering of those antecedents (\ie the prefix with the smallest lower bound).
Every antecedent is associated with an index, and we call the numerically sorted order of a set of antecedents its canonical order.
Thus by querying a set of antecedents by its canonical order, all permutations map to the same key.
The value stored in the map consists of the lower bound and the actual ordering of the rules that is best for that permutation.
Before inserting permutation $P_i$ into the symmetry-aware map, we check if there exists a permutation $P_j$ of $P_i$ already in the map.
If there is no permutation exists, then we insert $P_i$ in the map.
Otherwise, if a permutation $P_j$ exists and the lower bound of $P_i$ is better than that of $P_j$, we update the map and remove $P_j$ and its subtree from the trie.
Else, if $P_j$ exists and has a better lower bound than $P_i$, we do nothing  (\ie we do not insert $P_i$ into the symmetry-aware map or the trie).

\subsection{Incremental execution}

Mapping our algorithm to our data structures produces the following execution strategy.
%
While the trie contains unexplored leaves, a scheduling policy selects the next prefix to extend.
%
Then, for every antecedent that is not already in this prefix, we calculate the lower bound,
objective, and other metrics for the rule list formed by appending the antecedent to the prefix.
%
If the lower bound of the new rule list is less than the current minimum objective, we insert that
rule list into the symmetry-aware map, trie, and queue, and, if relevant, update the
current minimum objective.
%
If the lower bound is greater than the minimum objective,
then no extension of this rule list could possibly be optimal,
thus we do not insert the new rule list into the tree or queue.
%
We also leverage our other bounds from~\S\ref{sec:framework}
to aggressively prune the search space.
%
When there are no more leaves to explore, we are able to return the optimal rule list.
%
We can also choose to terminate

\subsection{Garbage Collection}
\label{sec:gc}

During execution, we garbage collect the trie.
%
Each time we update the minimum objective,
we traverse the trie in a depth-first manner, deleting all subtrees
of any node with lower bound larger than the current minimum objective.
%
At other times, when we encounter a node with no children, we prune upwards--deleting that
node and recursively traversing the tree towards the root, deleting any childless nodes.
%
This garbage collection allows us to constrain the trie's memory consumption, though in our
experiments we observe the minimum objective to decrease only a
small number of times.

When performing garbage collection, we have trouble deleting trie elements that are currently in the queue.
In C++ the priority queue is a wrapper container that prevents access to the container underlying the queue.
Therefore we cannot access elements in the middle of the queue, even if we have know the value that we're trying to access.
Thus, we run into a problem where we want to delete something in the prefix tree that is currently in the queue, as we have no way to update the queue without iterating over every element.
We address this by lazily marking nodes deleted in the prefix tree without deleting the physical node until it has been removed from the queue.
We define two different queues: the collection of all leaves in the C++ queue is our physical queue while the collection of leaves that are not marked deleted is our logical queue.