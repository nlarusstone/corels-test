\section{Implementation}
\label{sec:implementation}

\input{sections/incremental}

We implement our algorithm using a collection of optimized data structures:
a trie, a symmetry-aware map, and a queue.
The trie acts like a cache, keeping track of rule lists we have already evaluated.
Each node in the trie contains metadata associated with that corresponding rule list;
the metadata consists of bookkeeping information such as what child rule lists are feasible and
the lower bound and accuracy for that rule list.
We also track the best observed minimum objective and its associated rule list.

We implement the symmetry-aware map using the C++ STL unordered\_map, which
% Since we are not yet doing anything with the capture-based representation, we shouldn't
% discuss it as we won't be showing performance.
% We have two different versions of the map.
% Both versions 
maps all permutations of a set of antecedents to a key, whose value
contains the best ordering of those antecedents (\ie the prefix with the smallest lower bound).
Every antecedent has an antecedent id number, and we call the numerically
sorted order of a set of antecedents its canonical order, thus
by quering a set of antecedents by its canonical order, all
permutations map to the same key.
% Keys in one version of the map represent the set of rules (in canonical order) comprising a
% rule list prefix.
% Keys in the other version represent the set of captured data points.
% The set of captured entries is identical for a given set of rules, independent of ordering, so
% different permutations still map to the same key.
%
% Note that encodings of rule lists in canonical order tend to be
% significantly smaller than encodings of captured data points,
% especially for large datasets.
%
In general, the symmetry-aware map dominates memory usage during long runs.
Before inserting permutation $P_i$ into the symmetry-aware map, we check
if there exists a permutation $P_j$ of $P_i$ already in the map.
If the lower bound of $P_i$ is better than that of $P_j$,
we update the map and remove $P_j$ and its subtree from the trie.
Otherwise we do nothing (\ie we do not insert $P_i$ into the symmetry-aware map
or the trie).

We use a queue to store all of the leaves of the trie that still need to be explored.
We order entries in the queue to implement several different scheduling policies,
including BFS, DFS, and a priority queue, ordered by the lower bound, the objective,
or a custom priority metric.
We also have a stochastic exploration process that bypasses the use of a queue by
a random path from the root to a leaf.
We find that ordering by the lower bound and other priority metrics
often leads to a shorter runtime than using BFS.

Mapping our algorithm to our data structures produces the following execution strategy.
While the trie contains unexplored leaves, use a scheduling policy to select
the next prefix to extend.
Then, for every antecedent that is not already in this prefix, we calculate the lower bound,
objective, and other metrics for the rule list formed by appending the antecedent to the prefix.
If the lower bound of the new rule list is less than the current minimum objective, we insert that
rule list into the symmetry-aware map, trie, and queue, and, if relevant, update the
current minimum objective.
If the lower bound is greater than the minimum objective,
then extension of this rule list could possibly be optimal,
therefore we do not insert the new rule list into the tree or queue.

Each time we update the minimum objective, we garbage collect the trie, by walking it
from the root to the leaves, deleting all subtrees of any node with lower bound larger than the current
minimum objective. If we encounter a node with no children, we prune upwards--deleting that
node and recursively traversing the tree towards the root, deleting any childless nodes.
This garbage collection allows us to constrain the trie's memory consumption, though in our
experiments we observe the minimum objective to decrease only a small number of times.

%\section{Implementation architecture}
%
%We present an architecture for executing our branch-and-bound algorithm,
%consisting of a cache, a queue that is associated with a search policy,
%and, optionally, a symmetry-aware map.
%%
%First, we describe the cache, our primary data structure~(\S\ref{sec:cache});
%it is organized as a prefix tree and supports the incremental computations,
%detailed in~\S\ref{sec:incremental}, that are central to our approach.
%%
%Second, we describe the queue and search policy~(\S\ref{sec:queue}).
%%
%Like the queue in Algorithm~\ref{alg:branch-and-bound},
%our queue keeps track of which prefixes to evaluate during execution.
%%
%The policy for selecting a prefix from the queue to evaluate next,
%and thus also the natural queue data structure, depend on
%the search policy employed for exploring the space of rule lists
%%
%Next, we describe the symmetry-aware map~(\S\ref{sec:map}),
%which enables garbage collection of prefixes eliminated by the
%equivalent support bound in Theorem~\ref{sec:equivalent}.
%%
%While we present the map as an optional component of our architecture,
%our calculations in~\S\ref{sec:permutation-counting}
%and experiments in~\S\ref{sec:experiments} demonstrate that
%it is critical for efficient and practical algorithm performance.
%%
%Finally, we summarize an artifact we implemented~(\S\ref{sec:system}),
%which we evaluate in~\S\ref{sec:experiments}.
%
%\subsection{Prefix tree cache for incremental computation}
%\label{sec:cache}
%
%We maintain a cache to support incremental computation.
%%
%Our cache is organized as a prefix tree, which is also known as a trie.
%%
%
%\subsection{Queue and search policies}
%\label{sec:queue}
%
%Different search policies suggest different natural queue data structures.
%
%\begin{itemize}
%\item breadth-first
%\item depth-first
%\item something based on greedy
%\item (curiosity, lower bound, optimization) $\times$ (priority queue, something like Thompson sampling)
%\item optimistic
%\end{itemize}
%
%\subsection{Map data structure for symmetry-aware garbage collection}
%\label{sec:map}
%
%%\subsection{Large-scale optimization}
%
%\subsection{System}
%\label{sec:system}
