\begin{kdd}
\section{Implementation}
\label{sec:implementation}
\end{kdd}

\input{sections/incremental}

\begin{arxiv}
\section{Implementation}
\label{sec:implementation}
\end{arxiv}

We implement our algorithm using a collection of optimized data structures:
a trie (prefix tree), a symmetry-aware map, and a queue.
The trie acts like a cache, keeping track of rule lists we have already evaluated.
Each node in the trie contains metadata associated with that corresponding rule list;
the metadata consists of bookkeeping information such as what child rule lists are feasible and
the lower bound and accuracy for that rule list.
We also track the best observed minimum objective and its associated rule list.

The symmetry-aware map supports symmetry-aware pruning.
%
We implement this using the C++ STL unordered\_map,
% We have two different versions of the map.
to map all permutations of a set of antecedents to a key, whose value
contains the best ordering of those antecedents (\ie the prefix with the smallest lower bound).
%
Every antecedent is associated with an index, and we call the numerically
sorted order of a set of antecedents its canonical order.
%
Thus by querying a set of antecedents by its canonical order, all
permutations map to the same key.
% Keys in one version of the map represent the set of rules (in canonical order) comprising a
% rule list prefix.
% Keys in the other version represent the set of captured data points.
% The set of captured entries is identical for a given set of rules, independent of ordering, so
% different permutations still map to the same key.
%
% Note that encodings of rule lists in canonical order tend to be
% significantly smaller than encodings of captured data points,
% especially for large datasets.
%
\begin{kdd}
This map dominates memory usage for problems that explore longer prefixes.
\end{kdd}
\begin{arxiv}
The symmetry-aware map dominates memory usage for problems that explore longer prefixes.
\end{arxiv}
%
Before inserting permutation $P_i$ into the symmetry-aware map, we check
if there exists a permutation $P_j$ of $P_i$ already in the map.
If there is no permutation exists, then we insert $P_i$ in the map.
Otherwise, if a permutation $P_j$ exists and the lower bound of $P_i$ is better than 
that of $P_j$, we update the map and remove $P_j$ and its subtree from the trie.
Else, if $P_j$ exists and has a better lower bound than $P_i$, we do nothing 
(\ie we do not insert $P_i$ into the symmetry-aware map or the trie).

We use a queue to store all of the leaves of the trie that still need to be explored.
%
\begin{arxiv}
We order entries in the queue to implement several different policies.
%
A first-in-first-out (FIFO) queue implements breadth-first search (BFS),
and a priority queue implements best-first search.
%
Example priority queue policies include ordering
by the lower bound, the objective, or a custom metric
that maps prefixes to real values.
\end{arxiv}
\begin{kdd}
We order entries in the queue to implement several different policies,
including breadth-first search (BFS) and best-first search.
%
For best-first we use a priority queue, ordered by the lower bound, the objective,
or a custom priority metric.
\end{kdd}
%
We also support a stochastic exploration process that bypasses
the need for a queue by instead following random paths from the root to leaves.
%
We find that ordering by the lower bound and other priority metrics
often leads to a shorter runtime than using BFS.


Mapping our algorithm to our data structures produces the following execution strategy.
%
While the trie contains unexplored leaves, a scheduling policy selects the next prefix to extend.
%
Then, for every antecedent that is not already in this prefix, we calculate the lower bound,
objective, and other metrics for the rule list formed by appending the antecedent to the prefix.
%
If the lower bound of the new rule list is less than the current minimum objective, we insert that
rule list into the symmetry-aware map, trie, and queue, and, if relevant, update the
current minimum objective.
%
If the lower bound is greater than the minimum objective,
then no extension of this rule list could possibly be optimal,
thus we do not insert the new rule list into the tree or queue.
%
We also leverage our other bounds from~\S\ref{sec:framework}
to aggressively prune the search space.

During execution, we garbage collect the trie.
%
Each time we update the minimum objective,
we traverse the trie in a depth-first manner, deleting all subtrees
of any node with lower bound larger than the current minimum objective.
%
At other times, when we encounter a node with no children, we prune upwards--deleting that
node and recursively traversing the tree towards the root, deleting any childless nodes.
%
This garbage collection allows us to constrain the trie's memory consumption, though in our
experiments we observe the minimum objective to decrease only a
\begin{kdd}
few times. \\

Our code is at \textbf{\url{https://github.com/nlarusstone/corels}}.
\end{kdd}
\begin{arxiv}
small number of times. \\

Our implementation of CORELS is at \url{https://github.com/nlarusstone/corels}.
\end{arxiv}

\begin{kdd}
\vspace{-1mm}
\end{kdd}

\begin{arxiv}
\section{Curiosity}

We introduce a custom priority metric that we call \emph{curiosity}:
\begin{align}
\Curiosity(\Prefix, \x, \y) = \frac{1}{\NCap} \left(\sum_{n=1}^N \sum_{k=1}^K
  \Cap(x_n, p_k \given \Prefix) \wedge \one [ q_k \neq y_n ] \right)
  + \frac{\Reg K N}{\NCap} \,,
\end{align}
where~$\NCap$ is the number of datapoints captured by~$\Prefix$, \ie
${\NCap \equiv \sum_{n=1}^N \Cap(x_n, \Prefix)}$.
%
We can think of the curiosity as the expected objective value
of a rule list~${\RL' = }$ ${(\Prefix', \Labels', \Default', K')}$
generated from~$\Prefix$, for a simple model.
%
Assume that prefix~$\Prefix'$ starts with~$\Prefix$ and captures all the data,
such that each additional antecedent in~$\Prefix'$
both captures as many `new' datapoints and makes as many mistakes as
as each antecedent in~$\Prefix$, on average.
%
The expected objective value is then the sum of the expected prefix
misclassification error and the expected regularization penalty:
\begin{align}
\E[ \Obj(\RL', \x, \y) ] &= \E[\Loss_p(\Prefix, \Labels, \x, \y)] + \E[ \Reg K' ] \nn \\
&= \E[ K' ] \left(\frac{\Loss_p(\Prefix, \Labels, \x, \y)}{K}\right) + \Reg \E[ K' ] \nn \\
&=  \left(\frac{N}{\NCap / K}\right)
  \left(\frac{\Loss_p(\Prefix, \Labels, \x, \y)}{K}\right)
  + \Reg \left(\frac{N}{\NCap / K}\right) \nn \\
&= \left(\frac{N}{\NCap}\right) \left(\frac{1}{N} \sum_{n=1}^N \sum_{k=1}^K
  \Cap(x_n, p_k \given \Prefix) \wedge \one [ q_k \neq y_n ] \right)
  + \frac{\Reg K N}{\NCap} = \Curiosity(\Prefix, \x, \y).
\end{align}
\end{arxiv}
