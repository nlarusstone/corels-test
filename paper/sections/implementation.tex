\begin{kdd}
\section{Implementation}
\label{sec:implementation}
\end{kdd}

\input{sections/incremental}

\begin{arxiv}
\section{Implementation}
\label{sec:implementation}
\end{arxiv}

We implement our algorithm using a collection of optimized data structures:
a trie (prefix tree), a symmetry-aware map, and a queue.
The trie acts like a cache, keeping track of rule lists we have already evaluated.
Each node in the trie contains metadata associated with that corresponding rule list;
the metadata consists of bookkeeping information such as what child rule lists are feasible and
the lower bound and accuracy for that rule list.
We also track the best observed minimum objective and its associated rule list.

The symmetry-aware map supports symmetry-aware pruning.
%
We implement this using the C++ STL unordered\_map,
% Since we are not yet doing anything with the capture-based representation, we shouldn't
% discuss it as we won't be showing performance.
% We have two different versions of the map.
% Both versions 
to map all permutations of a set of antecedents to a key, whose value
contains the best ordering of those antecedents (\ie the prefix with the smallest lower bound).
%
Every antecedent is associated with an index, and we call the numerically
sorted order of a set of antecedents its canonical order.
%
Thus by querying a set of antecedents by its canonical order, all
permutations map to the same key.
% Keys in one version of the map represent the set of rules (in canonical order) comprising a
% rule list prefix.
% Keys in the other version represent the set of captured data points.
% The set of captured entries is identical for a given set of rules, independent of ordering, so
% different permutations still map to the same key.
%
% Note that encodings of rule lists in canonical order tend to be
% significantly smaller than encodings of captured data points,
% especially for large datasets.
%
The symmetry-aware map usually dominates memory usage during long runs.
Before inserting permutation $P_i$ into the symmetry-aware map, we check
if there exists a permutation $P_j$ of $P_i$ already in the map.
If the lower bound of $P_i$ is better than that of $P_j$,
we update the map and remove $P_j$ and its subtree from the trie.
Otherwise we do nothing (\ie we do not insert $P_i$ into the symmetry-aware map
or the trie).

We use a queue to store all of the leaves of the trie that still need to be explored.
%
\begin{arxiv}
We order entries in the queue to implement several different policies.
%
A first-in-first-out (FIFO) queue implements breadth-first search (BFS),
and a priority queue implements best-first search.
%
Example priority queue policies include ordering
by the lower bound, the objective, or a custom metric
that maps prefixes to real values.
\end{arxiv}
\begin{kdd}
We order entries in the queue to implement several different policies,
including breadth-first search (BFS) and best-first search.
%
For best-first we use a priority queue, ordered by the lower bound, the objective,
or a custom priority metric.
\end{kdd}
%
We also support a stochastic exploration process that bypasses
the need for a queue by instead following random paths from the root to leaves.
%
We find that ordering by the lower bound and other priority metrics
often leads to a shorter runtime than using BFS.


Mapping our algorithm to our data structures produces the following execution strategy.
%
While the trie contains unexplored leaves, a scheduling policy selects the next prefix to extend.
%
Then, for every antecedent that is not already in this prefix, we calculate the lower bound,
objective, and other metrics for the rule list formed by appending the antecedent to the prefix.
%
If the lower bound of the new rule list is less than the current minimum objective, we insert that
rule list into the symmetry-aware map, trie, and queue, and, if relevant, update the
current minimum objective.
%
If the lower bound is greater than the minimum objective,
then no extension of this rule list could possibly be optimal,
thus we do not insert the new rule list into the tree or queue.
%
We also leverage our other bounds from~\S\ref{sec:setup}
to aggressively prune the search space.

Each time we update the minimum objective, we garbage collect the trie, by walking it
from the root to the leaves, deleting all subtrees of any node with lower bound larger than the current
minimum objective. If we encounter a node with no children, we prune upwards--deleting that
node and recursively traversing the tree towards the root, deleting any childless nodes.
This garbage collection allows us to constrain the trie's memory consumption, though in our
experiments we observe the minimum objective to decrease only a small number of times.

\begin{kdd}
\vspace{-1mm}
\end{kdd}

\begin{arxiv}
\section{Curiosity}

We introduce a custom priority metric that we call \emph{curiosity}:
\begin{align}
\Curiosity(\Prefix, \x, \y) = \frac{1}{\NCap} \left(\sum_{n=1}^N \sum_{k=1}^K
  \Cap(x_n, p_k \given \Prefix) \wedge \one [ q_k \neq y_n ] \right)
  + \frac{\Reg K N}{\NCap} \,,
\end{align}
where~$\NCap$ is the number of datapoints captured by~$\Prefix$, \ie
\begin{align}
\NCap \equiv \sum_{n=1}^N \Cap(x_n, \Prefix).
\end{align}
%
We can think of the curiosity as the expected objective value
of a rule list~${\RL' = }$ ${(\Prefix', \Labels', \Default', K')}$
generated from~$\Prefix$, for a simple model.
%
Assume that prefix~$\Prefix'$ starts with~$\Prefix$ and captures all the data,
such that each additional antecedent in~$\Prefix'$
both captures as many `new' datapoints and makes as many mistakes as
as each antecedent in~$\Prefix$, on average.
%
The expected objective value is then the sum of the expected prefix
misclassification error and the expected regularization penalty:
\begin{align}
\E[ \Obj(\RL', \x, \y) ] &= \E[\Loss_p(\Prefix, \Labels, \x, \y)] + \E[ \Reg K' ] \nn \\
&= \E[ K' ] \left(\frac{\Loss_p(\Prefix, \Labels, \x, \y)}{K}\right) + \Reg \E[ K' ] \nn \\
&=  \left(\frac{N}{\NCap / K}\right)
  \left(\frac{\Loss_p(\Prefix, \Labels, \x, \y)}{K}\right)
  + \Reg \left(\frac{N}{\NCap / K}\right) \nn \\
&= \left(\frac{N}{\NCap}\right) \left(\frac{1}{N} \sum_{n=1}^N \sum_{k=1}^K
  \Cap(x_n, p_k \given \Prefix) \wedge \one [ q_k \neq y_n ] \right)
  + \frac{\Reg K N}{\NCap} = \Curiosity(\Prefix, \x, \y).
\end{align}
\end{arxiv}

%\section{Implementation architecture}
%
%We present an architecture for executing our branch-and-bound algorithm,
%consisting of a cache, a queue that is associated with a search policy,
%and, optionally, a symmetry-aware map.
%%
%First, we describe the cache, our primary data structure~(\S\ref{sec:cache});
%it is organized as a prefix tree and supports the incremental computations,
%detailed in~\S\ref{sec:incremental}, that are central to our approach.
%%
%Second, we describe the queue and search policy~(\S\ref{sec:queue}).
%%
%Like the queue in Algorithm~\ref{alg:branch-and-bound},
%our queue keeps track of which prefixes to evaluate during execution.
%%
%The policy for selecting a prefix from the queue to evaluate next,
%and thus also the natural queue data structure, depend on
%the search policy employed for exploring the space of rule lists
%%
%Next, we describe the symmetry-aware map~(\S\ref{sec:map}),
%which enables garbage collection of prefixes eliminated by the
%equivalent support bound in Theorem~\ref{sec:equivalent}.
%%
%While we present the map as an optional component of our architecture,
%our calculations in~\S\ref{sec:permutation-counting}
%and experiments in~\S\ref{sec:experiments} demonstrate that
%it is critical for efficient and practical algorithm performance.
%%
%Finally, we summarize an artifact we implemented~(\S\ref{sec:system}),
%which we evaluate in~\S\ref{sec:experiments}.
%
%\subsection{Prefix tree cache for incremental computation}
%\label{sec:cache}
%
%We maintain a cache to support incremental computation.
%%
%Our cache is organized as a prefix tree, which is also known as a trie.
%%
%
%\subsection{Queue and search policies}
%\label{sec:queue}
%
%Different search policies suggest different natural queue data structures.
%
%\begin{itemize}
%\item breadth-first
%\item depth-first
%\item something based on greedy
%\item (curiosity, lower bound, optimization) $\times$ (priority queue, something like Thompson sampling)
%\item optimistic
%\end{itemize}
%
%\subsection{Map data structure for symmetry-aware garbage collection}
%\label{sec:map}
%
%%\subsection{Large-scale optimization}
%
%\subsection{System}
%\label{sec:system}
